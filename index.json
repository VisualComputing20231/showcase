[{"content":"En esta página web se alojan los reportes académicos de los ejercicios desarrollados por el equipo para el curso (semestre 2023-I).\n→ Video correspondiente a la primera entrega ←\n→ Video correspondiente a la segunda entrega ←\n","date":"1 January 0001","permalink":"/showcase/","section":"","summary":"En esta página web se alojan los reportes académicos de los ejercicios desarrollados por el equipo para el curso (semestre 2023-I).","title":""},{"content":"Coloring\u003e Coloring # Introducción\u003e Introducción # En esta sección se abordará el problema de \"remappear\" o reasignar colores a una imagen para facilitar su visualizacion por parte de personas con visibilidad reducida. Color Blindness\u003e Color Blindness # Marco Teórico\u003e Marco Teórico # Cerca de 1 de cada 20 personas son daltónicas, lo cual significa que tienen problemas para distinguir algunos colores [1].\nEl ojo humano tiene dos tipos de receptores: conos y bastones. Los bastones detectan la luz y la oscuridad, mientras que los conos detectan los colores correspondientes a las longitudes de onda de la luz roja, verde y azul. El daltonismo está relacionado con el tipo de cono que tiene problemas [2]:\nProtanopia: los conos de onda larga (rojos) no funcionan o están ausentes. Protanomalía: los conos de onda larga funcionan parcialmente. Deuteranopia: los conos de onda media (verdes) no funcionan o están ausentes. Deuteranomalía: los conos de onda media funcionan parcialmente. Tritanopia: los conos de onda corta (azules) no funcionan o están ausentes. Tritanomalía: los conos de onda corta funcionan parcialmente. Acromatopsia: ningún cono o solo un tipo de cono funciona. Técnicamente, es posible que una persona tenga más de tres tipos de conos, lo que significa que todos somos daltónicos en cierto sentido.\nPara personas con daltonismo de tipo protanopia es difícil distinguir entre el rojo y el verde. Esto se debe a que el sistema visual de estas personas no puede distinguir entre estos colores. Podemos simular esto reemplazando colores en una imagen para crear líneas de confusión y, así, verla tal como la vería alguien con daltonismo.\nPara ello, se diseñó una aplicacion que permite modificar los colores de las imágenes de un video por medio de RGB para que, primeramente, se imite la dificultad para distinguir colores de una imagen y, posteriormente, esta pueda modificarse para facilitar el uso a personas con discapacidad visual.\nInstrucciones\u003e Instrucciones # Haga click o arrastre un video para que la aplicación lo cargue. Puede utilizar la página [3] para descargar un archivo de prueba: se recomienda emplear un mp4 con dimensiones 360 x 240, dado que esta configuración se ajusta mejor a las dimensiones del canvas. Si lo desea, puede acceder al ejemplo en dicho formato específico siguiendo este enlace.\nEl video iniciará automáticamente sin ningún filtro. Luego, podrá añadair hasta cuatro copias del video con diferentes filtros para comparar.\nManipule los canales con las teclas \u0026lsquo;r\u0026rsquo;, \u0026lsquo;g\u0026rsquo;, \u0026lsquo;b\u0026rsquo; para ver cómo se ven (aproximadamente) los colores para personas con daltonismo.\nMantenga presionado la tecla SHIFT y mueva el rodillo del ratón (o deslice dos dedos en su touchpad) para aumentar o disminuir el valor del canal seleccionado.\nOprima la tecla \u0026rsquo;s\u0026rsquo; para guardar un pantallazo de los filtros activos en formato jpg.\nCódigo y Resultados\u003e Código y Resultados # Este ejercicio se desarrolla haciendo uso de la librería p5.js, la cual permite la creación de aplicaciones web interactivas y de código abierto. Se utiliza, principalmente, la funcionalidad de tomar frame por frame de un video y manipularlos para crear una nueva imagen con el método onSeek que se ejeuta en cada frame y hace una pausa determinada.\nLa principal funcionalidad es manipular los píxeles de la imagen actual y reemplazarlos por los píxeles de la imagen duplicada. Para ello, se utiliza el método loadPixels() que carga los píxeles de la imagen actual y el método updatePixels() que actualiza los píxeles de la imagen actual con los píxeles cargados.\nSe puede volver a cargar un nuevo video arrastrándolo o haciendo click en la imagen. Además, está disponible el botón de reiniciar el video desde el principio.\nSe dispone de comandos por teclado para guardar un pantallazo de todos los filtros activos, así como las teclas para cambiar los valores de los canales RGB.\nCódigo completo let video; let time = 0; let loaded = false; let selected = false; let finished = false; let reseted = false; let filters = [false, false, false, false]; let button1, button2, button3, button4; let rgb = 0; let r1, g1, b1, r2, g2, b2, r3, g3, b3; function setup() { let c = createCanvas(400, 400); // Twice the width of the video c.drop(processFile); c.mouseClicked(openFile); button1 = createButton(\u0026#34;Copy original image here\u0026#34;); button1.style(\u0026#34;background-color\u0026#34;, \u0026#34;rgba(0,0,0,0)\u0026#34;); button1.style(\u0026#34;color\u0026#34;, \u0026#34;#006699\u0026#34;); button1.style(\u0026#34;border\u0026#34;, \u0026#34;none\u0026#34;); button1.style(\u0026#34;font-size\u0026#34;, \u0026#34;20px\u0026#34;); button1.style(\u0026#34;font-weight\u0026#34;, \u0026#34;bold\u0026#34;); button2 = createButton(\u0026#34;Copy original image here\u0026#34;); button2.style(\u0026#34;background-color\u0026#34;, \u0026#34;rgba(0,0,0,0)\u0026#34;); button2.style(\u0026#34;color\u0026#34;, \u0026#34;#006699\u0026#34;); button2.style(\u0026#34;border\u0026#34;, \u0026#34;none\u0026#34;); button2.style(\u0026#34;font-size\u0026#34;, \u0026#34;20px\u0026#34;); button2.style(\u0026#34;font-weight\u0026#34;, \u0026#34;bold\u0026#34;); button3 = createButton(\u0026#34;Copy original image here\u0026#34;); button3.style(\u0026#34;background-color\u0026#34;, \u0026#34;rgba(0,0,0,0)\u0026#34;); button3.style(\u0026#34;color\u0026#34;, \u0026#34;#006699\u0026#34;); button3.style(\u0026#34;border\u0026#34;, \u0026#34;none\u0026#34;); button3.style(\u0026#34;font-size\u0026#34;, \u0026#34;20px\u0026#34;); button3.style(\u0026#34;font-weight\u0026#34;, \u0026#34;bold\u0026#34;); button4 = createButton(\u0026#34;RESET\u0026#34;); button4.style(\u0026#34;background-color\u0026#34;, \u0026#34;rgba(0,0,0,0)\u0026#34;); button4.style(\u0026#34;color\u0026#34;, \u0026#34;#F00\u0026#34;); button4.style(\u0026#34;font-size\u0026#34;, \u0026#34;10px\u0026#34;); button4.style(\u0026#34;font-weight\u0026#34;, \u0026#34;bold\u0026#34;); button1.hide(); button2.hide(); button3.hide(); button4.hide(); (r1 = 1), (g1 = 1), (b1 = 1), (r2 = 1), (g2 = 1), (b2 = 1), (r3 = 1), (g3 = 1), (b3 = 1); } function processFile(file) { selected = true; noLoop(); // noCanvas(); if (file.type === \u0026#34;video\u0026#34;) { video = createVideo(file.data, () =\u0026gt; { loaded = true; video.volume(0); video.hide(); const drawNextFrame = () =\u0026gt; { // Only draw the image to the screen when the video // seek has completed const onSeek = () =\u0026gt; { draw(); // All the drawing code goes here video.elt.removeEventListener(\u0026#34;seeked\u0026#34;, onSeek); // Wait a 100 milisecond and draw the next frame setTimeout(drawNextFrame, 100); // TODO: Can be adjusted with keypress }; video.elt.addEventListener(\u0026#34;seeked\u0026#34;, onSeek); // Start seeking ahead video.time(time); // Seek ahead to the new time if (reseted) { reseted = false; time = 0; } time += 1 / 60; }; drawNextFrame(); video.onended(() =\u0026gt; { // When the video ends finished = true; }); }); } } function openFile() { let inputbtn = createFileInput(processFile); inputbtn.remove(); inputbtn.elt.click(); //Simulate Click } function printCurrentFrame() { fill(\u0026#34;#F00\u0026#34;); noStroke(); textAlign(LEFT, TOP); textSize(width / 40); text(\u0026#34;Frame: \u0026#34; + round(time * 60), width / 8, 5); } function copyImage(im, x, y, r, g, b) { let img = createImage(im.width, im.height); img.copy(im, 0, 0, im.width, im.height, 0, 0, im.width, im.height); img.loadPixels(); let d = pixelDensity(); let fullImage = 4 * (width * d) * (height * d); for (let i = 0; i \u0026lt; fullImage; i += 4) { img.pixels[i] = img.pixels[i] * r; img.pixels[i + 1] = img.pixels[i + 1] * g; img.pixels[i + 2] = img.pixels[i + 2] * b; img.pixels[i + 3] = img.pixels[i + 3]; } img.updatePixels(); image(img, im.width * x, im.height * y); } function manageButtons() { if (!filters[0]) { button1.show(); button1.position((width * 3) / 4 - (button1.width * 3) / 4, height / 4); button1.style(\u0026#34;font-size\u0026#34;, width / 30 + \u0026#34;px\u0026#34;); button1.style( \u0026#34;color\u0026#34;, random([\u0026#34;#00669933\u0026#34;, \u0026#34;#00669933\u0026#34;, \u0026#34;#00669933\u0026#34;, \u0026#34;#00669933\u0026#34;, \u0026#34;#00669930\u0026#34;]) ); button1.mousePressed(() =\u0026gt; { filters[0] = true; button1.hide(); }); } if (!filters[1]) { button2.show(); button2.position(width / 4 - (button2.width * 3) / 4, (height * 3) / 4); button1.style(\u0026#34;font-size\u0026#34;, width / 30 + \u0026#34;px\u0026#34;); button2.style( \u0026#34;color\u0026#34;, random([\u0026#34;#00669933\u0026#34;, \u0026#34;#00669933\u0026#34;, \u0026#34;#00669933\u0026#34;, \u0026#34;#00669933\u0026#34;, \u0026#34;#00669930\u0026#34;]) ); button2.mousePressed(() =\u0026gt; { filters[1] = true; button2.hide(); }); } if (!filters[2]) { button3.show(); button3.position( (width * 3) / 4 - (button3.width * 3) / 4, (height * 3) / 4 ); button1.style(\u0026#34;font-size\u0026#34;, width / 30 + \u0026#34;px\u0026#34;); button3.style( \u0026#34;color\u0026#34;, random([\u0026#34;#00669933\u0026#34;, \u0026#34;#00669933\u0026#34;, \u0026#34;#00669933\u0026#34;, \u0026#34;#00669933\u0026#34;, \u0026#34;#00669930\u0026#34;]) ); button3.mousePressed(() =\u0026gt; { filters[2] = true; button3.hide(); }); } button4.show(); button4.position(5, 5); button4.style(\u0026#34;font-size\u0026#34;, width / 50 + \u0026#34;px\u0026#34;); button4.style(\u0026#34;color\u0026#34;, random([\u0026#34;#F00\u0026#34;, \u0026#34;#FFF\u0026#34;])); button4.mousePressed(() =\u0026gt; { (r1 = 1), (g1 = 1), (b1 = 1), (r2 = 1), (g2 = 1), (b2 = 1), (r3 = 1), (g3 = 1), (b3 = 1); reseted = true; }); } function draw() { background(20); if (!selected) { let time = millis(); fill(0, 102, 153); textAlign(CENTER, CENTER); textStyle(BOLD); textSize((width / 500) * sin(time / 250) + width / 20); text(\u0026#34;SELECT or DROP video files HERE...\u0026#34;, width / 2, height / 2); } if (loaded \u0026amp;\u0026amp; !finished) { // Draw the original video to the screen resizeCanvas(video.width * 2, video.height * 2); image(video, 0, 0); printCurrentFrame(); manageButtons(); if (filters[0]) { copyImage(video, 1, 0, r1, g1, b1); } if (filters[1]) { copyImage(video, 0, 1, r2, g2, b2); } if (filters[2]) { copyImage(video, 1, 1, r3, g3, b3); } if (keyIsDown(82)) { fill(\u0026#34;#F00\u0026#34;); textAlign(CENTER, CENTER); textStyle(BOLD); text(\u0026#34;R\u0026#34;, video.width, 15); } if (keyIsDown(71)) { fill(\u0026#34;#0F0\u0026#34;); textAlign(CENTER, CENTER); textStyle(BOLD); text(\u0026#34;G\u0026#34;, video.width, 15); } if (keyIsDown(66)) { fill(\u0026#34;#00F\u0026#34;); textAlign(CENTER, CENTER); textStyle(BOLD); text(\u0026#34;B\u0026#34;, video.width, 15); } } } function keyPressed() { if (key == \u0026#34;s\u0026#34;) { saveCanvas(\u0026#34;myCanvas\u0026#34;, \u0026#34;jpg\u0026#34;); } if (key == \u0026#34;r\u0026#34;) { rgb = 0; } if (key == \u0026#34;g\u0026#34;) { rgb = 1; } if (key == \u0026#34;b\u0026#34;) { rgb = 2; } } function mouseWheel(event) { print(event.delta); textAlign(CENTER, CENTER); textStyle(BOLD); if (keyIsDown(SHIFT)) { if (mouseX \u0026gt; video.width \u0026amp;\u0026amp; mouseY \u0026lt; video.height \u0026amp;\u0026amp; filters[0]) { if (rgb == 0) { fill(\u0026#34;#F00\u0026#34;); r1 -= event.delta / 256; if (r1 \u0026lt; 0) { r1 = 0; } else if (r1 \u0026gt; 5) { r1 = 5; } text(\u0026#34;R: \u0026#34; + round(r1, 1), (video.width * 3) / 2, 15); } if (rgb == 1) { fill(\u0026#34;#0F0\u0026#34;); g1 -= event.delta / 256; if (g1 \u0026lt; 0) { g1 = 0; } else if (g1 \u0026gt; 5) { g1 = 5; } text(\u0026#34;G: \u0026#34; + round(g1, 1), (video.width * 3) / 2, 15); } if (rgb == 2) { fill(\u0026#34;#00F\u0026#34;); b1 -= event.delta / 100; if (b1 \u0026lt; 0) { b1 = 0; } else if (b1 \u0026gt; 5) { b1 = 5; } text(\u0026#34;B: \u0026#34; + round(b1, 1), (video.width * 3) / 2, 15); } } if (mouseX \u0026lt; video.width \u0026amp;\u0026amp; mouseY \u0026gt; video.height \u0026amp;\u0026amp; filters[1]) { if (rgb == 0) { fill(\u0026#34;#F00\u0026#34;); r2 -= event.delta / 256; if (r2 \u0026lt; 0) { r2 = 0; } else if (r2 \u0026gt; 5) { r2 = 5; } text(\u0026#34;R: \u0026#34; + round(r2, 1), video.width / 2, video.height + 15); } if (rgb == 1) { fill(\u0026#34;#0F0\u0026#34;); g2 -= event.delta / 256; if (g2 \u0026lt; 0) { g2 = 0; } else if (g2 \u0026gt; 5) { g2 = 5; } text(\u0026#34;G: \u0026#34; + round(g2, 1), video.width / 2, video.height + 15); } if (rgb == 2) { fill(\u0026#34;#00F\u0026#34;); b2 -= event.delta / 256; if (b2 \u0026lt; 0) { b2 = 0; } else if (b2 \u0026gt; 5) { b2 = 5; } text(\u0026#34;B: \u0026#34; + round(b2, 1), video.width / 2, video.height + 15); } } if (mouseX \u0026gt; video.width \u0026amp;\u0026amp; mouseY \u0026gt; video.height \u0026amp;\u0026amp; filters[2]) { if (rgb == 0) { fill(\u0026#34;#F00\u0026#34;); r3 -= event.delta / 256; if (r3 \u0026lt; 0) { r3 = 0; } else if (r3 \u0026gt; 5) { r3 = 5; } text(\u0026#34;R: \u0026#34; + round(r3, 1), (video.width * 3) / 2, video.height + 15); } if (rgb == 1) { fill(\u0026#34;#0F0\u0026#34;); g3 -= event.delta / 256; if (g3 \u0026lt; 0) { g3 = 0; } else if (g3 \u0026gt; 5) { g3 = 5; } text(\u0026#34;G: \u0026#34; + round(g3, 1), (video.width * 3) / 2, video.height + 15); } if (rgb == 2) { fill(\u0026#34;#00F\u0026#34;); b3 -= event.delta / 256; if (b3 \u0026lt; 0) { b3 = 0; } else if (b3 \u0026gt; 5) { b3 = 5; } text(\u0026#34;B: \u0026#34; + round(b3, 1), (video.width * 3) / 2, video.height + 15); } } } } Conclusiones y Trabajo futuro\u003e Conclusiones y Trabajo futuro # Se pueden utilizar otros espacios de color para la manipulación de colores, tales como el espacio de color XYZ o HSV y, adí, concluir cuál es más apropiado para esta tarea. Es posible modificar los valores de los filtros de manera más precisa, y también la velocidad o frames por segundo de la aplicacion. Como trabajo futuro, se podría mejorar la experiencia de usuario para que la aplicación sea más intuitiva y fácil de usar. Así mismo, se plantea reducir el tamaño de los videos para que se puedan visualizar mejor. Adicionalmente, se propone profundizar en funciones de color para la manipulacion de colores en cada pixel y, en lo posible, evitar cambios constantes. Esto implicaría acercarse a un enfoque polinomial o curvas. Es posible utilizar, a manera de filtro inverso, las imágenes que logren confundir a las personas con discapacidad visual en este programa para que sea aplicado a voluntad según se requiera. Finalmente, se plantea la opción de extender la aplicación para que pueda ser utilizada con otros tipos de entrada como imágenes, videos, etc. Referencias\u003e Referencias # [1] D. Nichols, \u0026ldquo;Coloring for Colorblindness\u0026rdquo; [Online]. Available: https://davidmathlogic.com/colorblind/#%23D81B60-%231E88E5-%23FFC107-%23004D40.\u003e [1] D. Nichols, \u0026ldquo;Coloring for Colorblindness\u0026rdquo; [Online]. Available: https://davidmathlogic.com/colorblind/#%23D81B60-%231E88E5-%23FFC107-%23004D40. # [2] ndesmic, \u0026ldquo;Exploring Color Math Through Color Blindness\u0026rdquo; [Online]. Available: https://dev.to/ndesmic/exploring-color-math-through-color-blindness-2m2h.\u003e [2] ndesmic, \u0026ldquo;Exploring Color Math Through Color Blindness\u0026rdquo; [Online]. Available: https://dev.to/ndesmic/exploring-color-math-through-color-blindness-2m2h. # [3] \u0026ldquo;Sample Videos\u0026rdquo; [Online]. Available: https://sample-videos.com.\u003e [3] \u0026ldquo;Sample Videos\u0026rdquo; [Online]. Available: https://sample-videos.com. # ","date":"1 January 0001","permalink":"/showcase/first/coloring/","section":"Firsts","summary":"Coloring\u003e Coloring # Introducción\u003e Introducción # En esta sección se abordará el problema de \"","title":""},{"content":"Depth Perception\u003e Depth Perception # Introducción\u003e Introducción # En esta sección se discute acerca de la percepción de la profundidad y su relación con las claves monoculares (\u0026quot;monocular cues\u0026quot; en inglés). Luego se procede a definir y detallar algunas de ellas.\nActo seguido, se desarrollan dos sketchs en 2D, los cuales, haciendo uso de las claves monoculares descritas en el marco teórico, permiten engañar al ojo para que se perciban como escenas en tres dimensiones.\nFinalmente, se discuten posibles aplicaciones de los conceptos aprendidos, así como el trabajo futuro que se puede llevar a cabo a partir del uso de las claves monoculares.\nMarco Teórico\u003e Marco Teórico # La percepción de la profundidad es la habilidad para percibir la distancia a los objetos en el mundo utilizando el sistema visual y la percepción visual. Es un factor importante en la percepción del mundo en tres dimensiones [1]. La percepción de la profundidad ocurre, principalmente, debido a la estereopsis, un fenómeno propio de la visión binocular. Sin embargo, también existen algunas pistas visuales que favorecen la percepción de la profundidad con un sólo ojo: estas se conocen como claves monoculares.\nA continuación se describen las claves monoculares que se aplicaron en el desarrollo del presente ejercicio:\nParalaje de movimiento: El paralaje es un fenómeno óptico en el que la posición aparente de un objeto parece cambiar cuando se observa desde diferentes ángulos o posiciones. Este efecto es causado por la diferencia en la ubicación de los ojos o la cámara utilizada para observar el objeto.\nEl paralaje es importante en muchas áreas: por ejemplo, en la astronomía, el paralaje se utiliza para medir la distancia de las estrellas y otros objetos celestes; en la fotografía, el paralaje se utiliza para ajustar la posición de la imagen en el visor de la cámara y para obtener una vista previa precisa de la imagen antes de tomarla; y en la medición, el paralaje se utiliza para determinar la profundidad y la distancia de los objetos [2].\nAhora bien, si además de la posición aparente se recibe información acerca del entorno, esto porque el observador está en movimiento, es aquí cuando se empieza a hablar de paralaje de movimiento: este efecto visual hace que los objetos cercanos parezcan moverse más rápido que los objetos más alejados, los cuales parecen estacionarios o, incluso, parecen moverse en la dirección opuesta [3].\nPerspectiva Aérea: La perspectiva aérea, también conocida como perspectiva atmosférica, se refiere al efecto visual en el que los objetos que se encuentran más lejos parecen más borrosos, descoloridos y con menos detalle que los objetos que están más cerca [4].\nEste efecto se produce debido a la interferencia de la atmósfera entre los objetos y el observador.\nPerspectiva: En este contexto, la perspectiva se refiere a la relación entre el tamaño de los objetos y su distancia percibida. Los objetos más alejados parecen más pequeños que los objetos más cercanos debido a la manera en que los rayos de luz se proyectan en el ojo del observador [5].\nEste efecto de los rayos de luz se traslada a líneas que convergen en un punto al infinito, lo cual permite que el observador reconstruya la distancia relativa entre los objetos. También existen otros tipos de perspectiva, como la mencionada anteriormente, la curvilínea y otras con más puntos de fuga. Sin embargo, para este ejercicio sólo se contempló la perspectiva lineal con un punto de fuga, pues, aunada con otras claves monoculares, ayudaba a reforzar la percepción de la profundidad.\nProfundidad a partir de la expansión óptica: La profundidad a partir del movimiento, también conocida como \u0026ldquo;depth from motion\u0026rdquo; en inglés, es una forma de percepción visual que se basa en la observación de la forma en que los objetos se mueven en relación con el observador. A partir de esta información, el cerebro puede determinar la velocidad, la dirección y la aceleración del movimiento de los objetos, lo que le permite calcular la profundidad y la distancia de los objetos en el mundo real [5].\nDentro de la profundidad a partir del movimiento, puede encontrarse una clave monocular aún más específica: la profunidad a partir de la expansión óptica. Esta pista visual es utilizada para inferir la profundidad y la distancia a partir de la información sobre cómo se expande un objeto en la retina, lo que le indica al cerebro que el objeto está más cerca en el espacio [6]. En consencuencia, este fenómeno otorga una sensación de movimiento en dirección al observador debido al cambio de tamaño.\nCódigo y Resultados\u003e Código y Resultados # De las claves monoculares que se detallaron anteriormente, se tomaron las dos primeras para desarrollar un sketch semejante a este gif, el cual es un ejemplo típico al hablar de claves monoculares. El resultado se puede ver a continuación:\nCódigo completo let val1 = []; let val2 = []; let val3 = [] function setup() { createCanvas(400, 400) frameRate(30) for(let i = 0; i \u0026lt; 8; i++){ append(val1, height - random(height/2, height)) append(val2, height - random(height/4, 4*height/5)) append(val3, height - random(20, 3*height/5)) } } function draw() { background(255) for(let i = -1; i \u0026lt;= width; i+=1){ stroke(100,150,255,85) rect(((frameCount + i) % (width+1)), val1[int(i/50)], 0, height) } for(let i = -1; i \u0026lt;= width; i++){ stroke(50,100,200,170) rect(((frameCount*4 + i) % (width+1)), val2[int(i/50)], 0, height) } for(let i = -1; i \u0026lt;= width; i++){ stroke(0,50,100,255) rect(((frameCount*8 + i) % (width+1)), val3[int(i/50)], 0, height) } } Como se puede evidenciar, en primer lugar se asignan unos valores aleatorios entre ciertos rangos para definir las alturas de los \u0026ldquo;edificios\u0026rdquo;. Luego, dentro del draw se definen 3 ciclos for, uno para los edificios delanteros, otro para los que van en medio y otro para los que van atrás. Por ejemplo, se tiene que el ciclo para el último caso mencionado es el siguiente:\nfor(let i = -1; i \u0026lt;= width; i+=1){ stroke(100,150,255,85) rect(((frameCount + i) % (width+1)), val1[int(i/50)], 0, height) } En primer lugar, se define el stroke correspondiente: en este caso es un tono de azul claro con un nivel alto de transparencia. Los edificios del medio tienen un color más oscuro y transparencia más baja, mientras que los edificios delanteros poseen el tono más opaco y sin transparencia. Esto se hace, justamente, para aplicar la clave monocular de perspectiva aérea.\nLuego, se dibujan rectángulos sin ancho (es decir, líneas, pero usa la función rect por practicidad) donde la posición en x está dada por la palabra reservada frameCount: esto se hace para dar la sensación de movimiento y, adicionalmente, se usa el operador módulo con el ancho del canvas para que se repita sucesivamente. Como altura se le da uno de los valores aleatorios asignados en el setup.\nAhora, para la otra clave monocular contenida en esta parte del código, se tiene que en el ciclo for de los edificios posicionados en medio, el valor de frameCount se cuadruplica, mientras que para los edificios posicionados al frente, este valor se multiplica por 8. De tal forma se consigue que, entre más cerca estén al observador, estos se muevan más rápido, lo cual es una aplicación directa del paralaje de movimiento.\nEn cuanto al segundo sketch, se buscó aplicar las otras dos claves monoculares descritas, es decir, la perspectiva y la profundidad a partir de la expansión óptica. Para ello, se tomó como punto de partida el siguiente código, el cual dibuja una carretera estática.\nAprovechando la perspectiva que presentaba, se buscó la manera de animarlo al agrandar el tamaño progresivamente, esto con el fin de dar la sensación de entrada a un túnel. El resultado se presenta a continuación:\nCódigo completo function setup() { createCanvas(600, 400); frameRate(100) rectMode(CENTER) } function draw() { background(80,180,80); road(frameCount % (2*height)) } function road(fc){ fill(140, 180, 240) for(let sky = -10; sky \u0026gt;= -200; sky-=10) { rect(width / 2, sky + fc / 4, 100 * fc, fc / 4) } stroke(\u0026#34;black\u0026#34;) strokeWeight(1) fill(220, 255) triangle(300, -10, 0, 400, 0, 350); fill(220, 255) triangle(300, -10, 600, 400, 600, 350); fill(220, 255) ellipse(300, -10 + fc/10, 63*fc/100); rect(width/2, -10 + fc/4, 63*fc/100, fc/4); fill(0, 255) ellipse(300, -10 + fc/10, 55*fc/100); rect(width/2, -10 + fc/4, 55*fc/100, fc/4); fill(70) triangle(300, -10, 600, 400, 0, 400); fill(155, 200) triangle(300, -10, 570, 400, 30, 400); stroke(255) fill(255) for(let i=15; i\u0026gt;0; i-=2){ if (i \u0026lt;= 1){ i+= 0.3 } quad(width/2 - fc/80, i*fc/5, width/2 + fc/80, i*fc/5, width/2 + fc/200, (i-1)*fc/5, width/2 - fc/200, (i-1)*fc/5) } noStroke() } En síntesis, se probaron varios valores por ensayo y error hasta que la animación fuese fluida. En este caso, el valor frameCount % (2*height) se seleccionó dado que permite ver la animación hasta un punto en el que la figura del túnel ocupa la mayor parte del espacio. Este mismo valor (que dentro la función se denomina como fc) es el que se emplea para ir variando el tamaño del ancho y el alto de los elementos. Por ejemplo, se tiene el siguiente ciclo:\nfor(let i=15; i\u0026gt;0; i-=2){ if (i \u0026lt;= 1){ i+= 0.3 } quad(width/2 - fc/80, i*fc/5, width/2 + fc/80, i*fc/5, width/2 + fc/200, (i-1)*fc/5, width/2 - fc/200, (i-1)*fc/5) } Esta porción de código es la que dibuja las líneas en la carretera, de forma que se ven más grandes entre más cerca están al borde inferior del canvas. En los puntos ubicados a la izquierda del paralelogramo, se resta una razón que lleva la partícula fc, mientras que en los puntos ubicados a la derecha se suma: con esto se logra que el tamaño se amplíe ya que los puntos se mueven en direcciones opuestas a la misma proporción.\nConclusiones y Trabajo futuro\u003e Conclusiones y Trabajo futuro # En síntesis, se puede evidenciar que el uso de claves monoculares puede ser muy útil a la hora de realizar animaciones en 2D, puesto que otorgan sensación de movimiento y profunidad de una manera sencilla. Es evidenciable que, cuando se aplican varias claves monoculares, el efecto de percepción de la profundidad es mayor. A veces, incluso, termina siendo inevitable que se empleen dos o más claves monoculares, tal como ocurre con el segundo sketch: para dar la sensación de que un elemento se acerca, este debe crecer proporcionalmente. Lo anterior conlleva una sensación de perspectiva, dado que si se lleva el trazo de las posiciones del objeto cuyo tamaño cambia, esto dará como resultado un par de líneas que tienden a un punto en el infinito. Como trabajo futuro, se podría hacer uso de otras claves monoculares que, al integrarlas con las presentadas, acrecenten la sensación de percepción de la profundidad. Así mismo, se podrían evaluar las diferencias de percepción entre el ojo izquierdo y el derecho en cuanto a las claves monoculares y, a partir de los resultados, buscar la mejor forma en que dichas claves pueden integrarse en un sketch 2D. Referencias\u003e Referencias # [1] J. P. Charalambos, \u0026ldquo;Depth perception\u0026rdquo;. Visual Computing, Feb. 2023. https://visualcomputing.github.io/docs/visual_illusions/depth_perception/\u003e [1] J. P. Charalambos, \u0026ldquo;Depth perception\u0026rdquo;. Visual Computing, Feb. 2023. https://visualcomputing.github.io/docs/visual_illusions/depth_perception/ # [2] A. E. Roy and D. Clarke, \u0026ldquo;Astronomy: Principles and Practice,\u0026rdquo; 4th ed., CRC Press, 2003.\u003e [2] A. E. Roy and D. Clarke, \u0026ldquo;Astronomy: Principles and Practice,\u0026rdquo; 4th ed., CRC Press, 2003. # [3] B. J. Rogers and M. E. Graham, \u0026ldquo;Motion Parallax as an Independent Cue for Depth Perception,\u0026rdquo; Perception, vol. 8, no. 2, pp. 125-34, 1979. doi: 10.1068/p080125.\u003e [3] B. J. Rogers and M. E. Graham, \u0026ldquo;Motion Parallax as an Independent Cue for Depth Perception,\u0026rdquo; Perception, vol. 8, no. 2, pp. 125-34, 1979. doi: 10.1068/p080125. # [4] J. Murray. \u0026ldquo;Some perspectives on visual depth perception,\u0026rdquo; ACM SIGGRAPH Computer Graphics, vol. 28, no. 2, pp. 155-157, 1994, doi: 10.1145/178951.178985.\u003e [4] J. Murray. \u0026ldquo;Some perspectives on visual depth perception,\u0026rdquo; ACM SIGGRAPH Computer Graphics, vol. 28, no. 2, pp. 155-157, 1994, doi: 10.1145/178951.178985. # [5] S. Schwartz, \u0026ldquo;Visual Perception: A Clinical Orientation (Fourth Edition),\u0026rdquo; McGraw-Hill Education, 1994.\u003e [5] S. Schwartz, \u0026ldquo;Visual Perception: A Clinical Orientation (Fourth Edition),\u0026rdquo; McGraw-Hill Education, 1994. # [6] M.T. Swanston and W.C. Gogel, \u0026ldquo;Perceived size and motion in depth from optical expansion,\u0026rdquo; Perception \u0026amp; Psychophysics, vol. 39, pp. 309-326, 1986. doi: 10.3758/BF03202998.\u003e [6] M.T. Swanston and W.C. Gogel, \u0026ldquo;Perceived size and motion in depth from optical expansion,\u0026rdquo; Perception \u0026amp; Psychophysics, vol. 39, pp. 309-326, 1986. doi: 10.3758/BF03202998. # ","date":"1 January 0001","permalink":"/showcase/first/depth/","section":"Firsts","summary":"Depth Perception\u003e Depth Perception # Introducción\u003e Introducción # En esta sección se discute acerca de la percepción de la profundidad y su relación con las claves monoculares (\u0026quot;monocular cues\u0026quot; en inglés).","title":""},{"content":"Visual illusions\u003e Visual illusions # Introducción\u003e Introducción # En esta sección se presentarán algunos fenómenos visuales y su respectiva implementación; además, se proporcionará una breve explicación junto con la fuente de donde se obtuvo la información. Motion Aftereffect (Waterfall Illusion)\u003e Motion Aftereffect (Waterfall Illusion) # Marco Teórico\u003e Marco Teórico # El fenómeno de la cascada es una ilusión óptica que se produce cuando se observa una cascada o un río en movimiento y luego se mira hacia una superficie estática. La superficie estática parece moverse en la dirección opuesta al flujo original. Este efecto se debe a la adaptación del sistema visual a un estímulo en movimiento que, al desaparecer, produce un efecto residual de movimiento en la dirección opuesta. Este efecto es conocido como \u0026ldquo;inducción de movimiento\u0026rdquo;.\nEste fenómeno ha sido observado desde la antigüedad. Lucrecio, poeta romano del siglo I a.C., describió la ilusión de que las piernas de un caballo estacionado en un río parecían moverse en la dirección opuesta al flujo del agua. El filósofo Aristóteles también hizo referencia a la percepción errónea del movimiento en el agua, aunque en su caso se tratara de la percepción de que objetos estáticos parecían moverse.\nInvestigaciones posteriores han demostrado que el fenómeno de la cascada está mediado por procesos neuronales complejos en el cerebro que procesan y adaptan la percepción visual del movimiento. Estudios modernos han identificado las áreas cerebrales que son responsables de esta adaptación y han ayudado a explicar los mecanismos subyacentes a la ilusión de la cascada.\nEn resumen, la ilusión de la cascada es un fenómeno óptico que se produce cuando se observa una superficie estática después de haber mirado un objeto en movimiento. Este efecto se debe a la adaptación del sistema visual a estímulos en movimiento que persisten durante unos momentos después de que el estímulo ha desaparecido. Este fenómeno ha sido estudiado desde la antigüedad y ha sido objeto de investigación científica moderna, lo que ha ayudado a comprender mejor los mecanismos neuronales que subyacen a la percepción visual del movimiento.\nInstrucciones\u003e Instrucciones # Fíjate en la cruz central durante el movimiento y mira el ciclo al menos tres veces. Observa el efecto de la imagen posterior al movimiento en la figura en reposo (el Buda de Kamakura). La \u0026ldquo;deformación\u0026rdquo; causada por el efecto de la imagen posterior al movimiento se aplica a cualquier cosa que observes. También puedes intentar cubrir un ojo, adaptarte durante aproximadamente tres ciclos y luego probar con el otro ojo.\nEsto se explica, a menudo, en términos de \u0026ldquo;fatiga\u0026rdquo; de la clase de neuronas que codifican una dirección de movimiento. Sin embargo, es más preciso interpretar esto en términos de adaptación o \u0026ldquo;control de ganancia\u0026rdquo;. Estos detectores de movimiento no se encuentran en la retina, sino en el cerebro [1]. Para obtener una explicación más detallada y una demostración interesante del \u0026ldquo;efecto cascada\u0026rdquo;, consulte la página de George Mather.\nWaterfall Illusion Código y Resultados\u003e Código y Resultados # En este ejercicio se desarrolla el efecto visual de la cascada. Para lograr la sensación de movimiento, se dibujan múltiples áreas a distintos radios, determinados por la cantidad de anillos (rings). Luego, se almacenan en un array ringpos[] en orden del más grande al más pequeño y, por último, se les asigna un color blanco o negro alternadamente. También hay que notar que la función está restringida por el módulo (%) de un radio establecido, por tanto, una vez que el radio de alguna área llega al límite (si se expande), este se devolverá al radio mínimo.\nfor (let i = 0; i \u0026lt; rings; i++) { cursize = (frameCount * speed + (size / rings) * i) % size; if (cursize == 0) { inverted = !inverted; exterior = !exterior; } if (reversed) cursize = size - cursize; ringpos[i] = cursize; } En este caso no se dibujan círculos completos, sino más bien arcos con un ángulo definido por la cantidad de ángulos angles y los radios previamente calculados. Luego, se rellenan las áreas de estos arcos cambiando su radio en cada frame. De esta manera, se logra el efecto de movimiento y el estilo cuadriculado de cada anillo.\nfor (let i = 0; i \u0026lt; rings; i++) { inverted = !inverted; for (let j = 0; j \u0026lt; angles; j++) { noStroke(); push(); if (j % 2 == 0) { if (inverted) fill(255); else fill(0); } else { if (inverted) fill(0); else fill(255); } arc( 200, 200, ringpos[i], ringpos[i], (2 * PI * j) / angles, (2 * PI * (j + 1)) / angles ); pop(); } } Según el frame rate, se actualiza el radio de cada arco presente, de manera que se expande o se colapsa según lo que determine la variable reversed, que cambiará su valor de verdad presionando el respectivo botón. Para lograr que el efecto se muestre correctamente, la duración del movimiento es mucho mayor a la duración de la imagen mostrada (cerca de 3 veces más largo).\nPor último, se dibuja la imagen original en el centro de la pantalla. Esta imagen se carga previamente en la función preload() y se almacena en la variable originalImage. Esta imagen se muestra o no dependiendo del valor de la variable showImage, que se cambia cada cierto tiempo dependiendo de la variable delayTime y el frame actual.\nif (frameCount % delayTime == 0) showImage = !showImage; if (showImage) { delayTime = 200; image(originalImage, 0, 0, 400, 400); } else { noFill(); stroke(0); erase(0,255); strokeWeight(100); circle(200,200, 500); noErase(); delayTime = 600; } El código completo se encuentra en el siguiente desplegable:\nCódigo completo let fr = 30; let img = null; let delayTime = 200; let size = 560; let angles = 12; let speed = 4; let anchor = 100; let rings = 14; let inverted = false; let exterior = false; let reversed = false; let showImage = false; let button; function preload() { originalImage = loadImage(\u0026#34;../assets/buddha.jpg\u0026#34;); }; function setup() { var canvas = createCanvas(400, 400); canvas.parent(\u0026#34;waterfall-illusion\u0026#34;); frameRate(fr); button = createButton(\u0026#39;Collapse\u0026#39;); button.parent(\u0026#34;waterfall-illusion\u0026#34;); button.position(0, 0, \u0026#39;sticky\u0026#39;); button.mousePressed(() =\u0026gt; { button.elt.innerText = reversed ? \u0026#39;Collapse\u0026#39; : \u0026#39;Expand\u0026#39;; reversed = !reversed} ); }; function draw() { background(128); var cursize; noStroke(); for (let j = 0; j \u0026lt; angles; j++) { push(); if (j % 2 == 0) { if (exterior) fill(255); else fill(0); } else { if (exterior) fill(0); else fill(255); } arc( 200, 200, size, size, (2 * PI * j) / angles, (2 * PI * (j + 1)) / angles ); pop(); } let ringpos = []; for (let i = 0; i \u0026lt; rings; i++) { cursize = (frameCount * speed + (size / rings) * i) % size; if (cursize == 0) { inverted = !inverted; exterior = !exterior; } if (reversed) cursize = size - cursize; ringpos[i] = cursize; } ringpos.sort(function (a, b) { return b - a; }); for (let i = 0; i \u0026lt; rings; i++) { inverted = !inverted; for (let j = 0; j \u0026lt; angles; j++) { noStroke(); push(); if (j % 2 == 0) { if (inverted) fill(255); else fill(0); } else { if (inverted) fill(0); else fill(255); } arc( 200, 200, ringpos[i], ringpos[i], (2 * PI * j) / angles, (2 * PI * (j + 1)) / angles ); pop(); } } fill(0, 0, 255); circle(200, 200, 30); fill(255, 0, 0); rect(199, 192, 2, 16); rect(192, 199, 16, 2); if (frameCount % delayTime == 0) showImage = !showImage; if (showImage) { delayTime = 200; image(originalImage, 0, 0, 400, 400); } else { noFill(); stroke(0); erase(0,255); strokeWeight(100); circle(200,200, 500); noErase(); delayTime = 600; } }; Conclusiones y Trabajo futuro\u003e Conclusiones y Trabajo futuro # Es posible mejorar el programa para que sea modificable en términos de velocidad, cantidad de anillos y/o ángulos, tamaño de la imagen, etc. Como trabajo futuro, se pueden investigar e implementar más fenómenos visuales. Referencias\u003e Referencias # [1] M. Bach, \u0026ldquo;Motion Aftereffect (Waterfall Illusion)\u0026rdquo; [Online]. Available: https://michaelbach.de/ot/mot-adapt/index.html.\u003e [1] M. Bach, \u0026ldquo;Motion Aftereffect (Waterfall Illusion)\u0026rdquo; [Online]. Available: https://michaelbach.de/ot/mot-adapt/index.html. # ","date":"1 January 0001","permalink":"/showcase/first/illusions/","section":"Firsts","summary":"Visual illusions\u003e Visual illusions # Introducción\u003e Introducción # En esta sección se presentarán algunos fenómenos visuales y su respectiva implementación; además, se proporcionará una breve explicación junto con la fuente de donde se obtuvo la información.","title":""},{"content":"Temporal Coherence\u003e Temporal Coherence # Introducción\u003e Introducción # En esta sección se discute, en primer lugar, acerca del fenómeno de coherencia temporal y la estrecha relación que guarda con las animaciones y, en particular, con el uso de fotogramas clave (keyframes).\nDespués, a manera de ejercicio práctico, se desarrolla una animación por keyframes empleando la librería nub en el lenguaje Processing. Para ello, se realiza un estudio detallado de esta, con el fin de comprender su funcionamiento y generar, así, un producto donde el posicionamiento de sus fotogramas guarde una intención determinada: es decir, donde este no se lleve a cabo de manera aleatoria.\nFinalmente, se discuten posibles aplicaciones de los conceptos aprendidos, así como el trabajo futuro que se puede llevar a cabo tomando el ejercicio desarrollado como punto de partida.\nMarco Teórico\u003e Marco Teórico # La coherencia temporal es un fenómeno visual presente en toda la naturaleza: de acuerdo con la teoría, un color percibido de un punto en una región de interés tiende a variar más según el tiempo transcurrido entre dos momentos dados [1].\nPor otra parte, en animación y gráficos por computadora, se tiene que los keyframes son un conjunto de fotogramas que representan los momentos clave de una animación. Los keyframes son los fotogramas en los que se definen los cambios en la posición, la rotación, la escala, la opacidad y otros atributos de los objetos en la animación. En general, suelen utilizarse algoritmos de interpolación para crear fotogramas intermedios que conecten los keyframes [2].\nCon base en lo anterior, los keyframes son esenciales para la coherencia temporal de la animación, pues se relacionan directamente con su capacidad para mantener una secuencia de fotogramas que sea suave y coherente a lo largo del tiempo. Si los keyframes están mal ubicados o son inconsistentes, la animación puede tener errores de coherencia temporal, como saltos o sacudidas entre fotogramas.\nUna vez que es claro el concepto, entra a colación el uso de la libería nub. Esta es de código abierto y fue cuenta con marcos de interacción, visualización, animación, y admite técnicas avanzadas de renderizado (tanto en como fuera de pantalla): por ejemplo, el recorte de frustum de visualización [3], que se refiere a la visualización de una escena desde diferentes ángulos empleando una cámara vortual, tal como ocurre en el presente ejercicio.\nA grandes rasgos, para desarrollar una animación empleando esta librería se debe instaciar la escena y los nodos. Luego de las configuraciones iniciales, el proceso consiste en ir variando la posición, la orientación, la escala y demás atributos entre cada keyframe: esto es lo que permite que los nodos se muevan dentro de la escena. Al ojo de la escena también se le pueden aplicar las mismas transformaciones, lo cual da la sensación de movimiento de la cámara al desplegar su animación por fotogramas clave.\nLa posición se establece mediante vectrores de tres dimensiones, los cuales dan las coordenadas para los ejes x, y y z en el plano.\nLa rotación se establece mediante cuaterniones: estos elementos matemáticos, que fueron descubiertos por William Rowan Hamilton en 1843, constan de cuatro componentes: una parte real y tres partes imaginarias [4]. Se pueden escribir como q = w + xi + yj + zk, donde w, x, y y z son números reales, e i, j y k son unidades imaginarias que satisfacen las siguientes relaciones:\ni² = j² = k² = ijk = -1\nSe tiene que, a diferencia de los números complejos, los cuaterniones poseen tres partes imaginarias Esto permite una representación más completa de las rotaciones tridimensionales que los números complejos no pueden proporcionar, ya que sólo poseen dos partes imaginarias.\nCódigo y Resultados\u003e Código y Resultados # A continuación, se describe el ejercicio desarrollado y se destacan las piezas de código más relevantes, las cuales demuestran los aportes efectuados sobre el ejemplo disponible. Para esta sección en particular, se adjunta un vídeo de demostración, dado que este ejercicio fue hecho en Processing y no en P5js.\nComo se puede evidenciar, el ejercicio desarrollado consiste en una animación que muestra un bosquejo de la rotación de la Tierra, al igual que tanto la rotación como la traslación de su satélite (la Luna). Haciendo uso del mouse, se pueden alterar los keyframes de la Luna lo que, en consecuencia, altera la rotación natural que trae por defecto; también se puede mover el ángulo de la cámara.\nPara la construcción de los objetos (Tierra y Luna), se crearon los nodos como esferas y se texturizaron a partir de imágenes en línea como se evidencia a continuación:\nearth = loadImage(\u0026#34;https://b3d.interplanety.org/wp-content/upload_content/2016/08/01-3.jpg\u0026#34;); moon = loadImage(\u0026#34;https://svs.gsfc.nasa.gov/vis/a000000/a004700/a004720/lroc_color_poles_1k.jpg\u0026#34;); (...) pshape = createShape(SPHERE, 80); pshape2 = createShape(SPHERE, 20); pshape.setTexture(earth); pshape2.setTexture(moon); Para definir los movimientos de rotación y traslación se definió el siguiente ciclo for:\nshape2.setPosition(120,0,0); int x = 40; int y = 80; for (int i = 0; i \u0026lt; 8; i++) { shape.addKeyFrame(Node.AXES | Node.SHAPE, 4000); shape.rotate(new Quaternion(0,1,0,0)); shape2.addKeyFrame(Node.AXES | Node.SHAPE | Node.HUD, 2000); shape2.translate(x*(i \u0026lt; 4 ? -1 : 1), 0, y*(i \u0026gt; 1 \u0026amp;\u0026amp; i \u0026lt; 6 ? -1 : 1)); shape2.rotate(new Quaternion(0,1,0,0)); if (i % 2 == 0){ int tmp = x; x = y; y = tmp; } } Aquí se evidencia cómo se instancia un cuaternión cuyo único valor distinto de cero es el segundo a la hora de rotar la Tierra: con ello se consigue que rote únicamente alrededor del eje y. En cuanto a la Luna, aparte de la rotación (que sigue la misma lógica), se lleva a cabo una traslación definida por condiciones.\nPara lograr lo anterior, se estudiaron, mediante ensayo y error, configuraciones matemáticas que dieran la sensación de movimiento circular (pues en realidad sólo se desplaza en línea recta). Entonces, se definieron 8 posiciones (adelante, atrás, izquierda, derecha y las 4 diagonales). La Luna arranca en la posición (120,0,0), es decir, a la derecha de la Tierra, y se desplaza en relación 80 y 40 en los ejes x y y para ocupar cada una de las posiciones subsecuentes.\nEs así que se inicializan dos variables con dichos valores y, cada vez que el contador del ciclo es par, estos valores se intercambian. La primera coordenada siempre será negativa cuando i \u0026lt; 4, porque será cuando la Luna se desplace hacia la izquierda; después de ello será positiva porque se desplazará a la derecha. El mismo razonamiento aplica cuando se desplaza hacia atrás y hacia adelante: el primer caso se dará desde la segunda iteración hasta la sexta y el otro se dará en los escenarios opuestos.\nPor otra parte, algunas teclas permiten alterar los keyframes del ojo de la escena. Dicha configuración está dada de la siguiente manera:\nVista frontal (FRONT): tecla \u0026ldquo;F\u0026rdquo;. Vista posterior (BACK): tecla \u0026ldquo;B\u0026rdquo;. Vista lateral izquierda (LEFT): tecla \u0026ldquo;L\u0026rdquo;. Vista lateral derecha (RIGHT): tecla \u0026ldquo;R\u0026rdquo;. Vista superior (UP): tecla \u0026ldquo;U\u0026rdquo;. Vista inferior (DOWN): tecla \u0026ldquo;D\u0026rdquo;. Esto se consigue a partir de la función keyPressed(), la cual se detalla a continuación:\nvoid keyPressed() { if (key == \u0026#39; \u0026#39;) { shape2.toggleHint(Node.KEYFRAMES); } else { scene.eye().removeKeyFrames(); scene.eye().addKeyFrame(Node.CAMERA | Node.BULLSEYE, 1000); if (key == \u0026#39;u\u0026#39;) { scene.eye().setPosition(0,-300,0); scene.eye().setOrientation(1,0,0,1); } if (key == \u0026#39;l\u0026#39;) { scene.eye().setPosition(-300,0,0); scene.eye().setOrientation(0,-1,0,1); } if (key == \u0026#39;r\u0026#39;) { scene.eye().setPosition(300,0,0); scene.eye().setOrientation(0,1,0,1); } if (key == \u0026#39;d\u0026#39;) { scene.eye().setPosition(0,300,0); scene.eye().setOrientation(-1,0,0,1); } if (key == \u0026#39;b\u0026#39;) { scene.eye().setPosition(0,0,-300); scene.eye().setOrientation(0,1,0,0); } if (key == \u0026#39;f\u0026#39;) { scene.eye().setPosition(0,0,300); scene.eye().setOrientation(0,0,0,0); } scene.eye().addKeyFrame(Node.CAMERA | Node.BULLSEYE, 1000); scene.eye().animate(); } } Se puede ver que la cámara toma una distancia de 300 respecto al centro para poder apreciar toda la animación: este valor se ubica en la posición correspondiente a la vista que se desea (por ejemplo, para ver desde arriba, se ubica su negativo en la posición y). Para la rotación se indica sobre qué eje se desea el movimiento de la cámara y los demás ejes se dejan en cero; en cuanto al cuatro valor, cuando este equivale a 1, el giro resulta ser de 90°, mientras que si equivale a 0, el giro resulta ser de 180°.\nEl código completo se encuentra en el siguiente desplegable y, de manera posteior, se encuentran dos botones en caso de que se desee descargar el programa como un archivo ejecutable, dependiendo del sistema operativo, para interactuar con este desde la máquina local.\nCódigo completo import nub.primitives.*; import nub.core.*; import nub.processing.*; Scene scene; Node shape; Node shape2; String renderer = P3D; float speed = 2; PImage night = loadImage(\u0026#34;https://images.hdqwalls.com/download/dark-starry-sky-stars-4k-9m-2560x1700.jpg\u0026#34;); void setup() { size(1000, 800, renderer); scene = new Scene(this, 150); PShape pshape; PShape pshape2; PImage earth; PImage moon; earth = loadImage(\u0026#34;https://b3d.interplanety.org/wp-content/upload_content/2016/08/01-3.jpg\u0026#34;); moon = loadImage(\u0026#34;https://svs.gsfc.nasa.gov/vis/a000000/a004700/a004720/lroc_color_poles_1k.jpg\u0026#34;); night.resize(1000,800); noStroke(); pshape = createShape(SPHERE, 80); pshape2 = createShape(SPHERE, 20); pshape.setTexture(earth); pshape2.setTexture(moon); shape = new Node(pshape); shape2 = new Node(pshape2); shape.setMinMaxScalingFilter(0.5, 1.0); shape2.setMinMaxScalingFilter(0.5, 1.0); scene.eye().disableHint(Node.KEYFRAMES); shape.disableHint(Node.KEYFRAMES); shape2.enableHint(Node.KEYFRAMES, Node.AXES, 1, color(255, 255, 255), 2); shape.setAnimationRecurrence(true); shape2.setAnimationRecurrence(true); shape2.setPosition(120,0,0); int x = 40; int y = 80; for (int i = 0; i \u0026lt; 8; i++) { shape.addKeyFrame(Node.AXES | Node.SHAPE, 4000); shape.rotate(new Quaternion(0,1,0,0)); shape2.addKeyFrame(Node.AXES | Node.SHAPE | Node.HUD, 2000); shape2.translate(x*(i \u0026lt; 4 ? -1 : 1), 0, y*(i \u0026gt; 1 \u0026amp;\u0026amp; i \u0026lt; 6 ? -1 : 1)); shape2.rotate(new Quaternion(0,1,0,0)); if (i % 2 == 0){ int tmp = x; x = y; y = tmp; } } shape2.addKeyFrame(Node.AXES | Node.SHAPE | Node.HUD, 2000); shape.resetScalingFilter(); shape2.resetScalingFilter(); shape.animate(); shape2.animate(); } void draw() { background(night); scene.render(); } void keyPressed() { if (key == \u0026#39; \u0026#39;) { shape2.toggleHint(Node.KEYFRAMES); } else { scene.eye().removeKeyFrames(); scene.eye().addKeyFrame(Node.CAMERA | Node.BULLSEYE, 1000); if (key == \u0026#39;u\u0026#39;) { scene.eye().setPosition(0,-300,0); scene.eye().setOrientation(1,0,0,1); } if (key == \u0026#39;l\u0026#39;) { scene.eye().setPosition(-300,0,0); scene.eye().setOrientation(0,-1,0,1); } if (key == \u0026#39;r\u0026#39;) { scene.eye().setPosition(300,0,0); scene.eye().setOrientation(0,1,0,1); } if (key == \u0026#39;d\u0026#39;) { scene.eye().setPosition(0,300,0); scene.eye().setOrientation(-1,0,0,1); } if (key == \u0026#39;b\u0026#39;) { scene.eye().setPosition(0,0,-300); scene.eye().setOrientation(0,1,0,0); } if (key == \u0026#39;f\u0026#39;) { scene.eye().setPosition(0,0,300); scene.eye().setOrientation(0,0,0,0); } scene.eye().addKeyFrame(Node.CAMERA | Node.BULLSEYE, 1000); scene.eye().animate(); } } void mouseDragged() { if (mouseButton == LEFT) scene.spin(); else if (mouseButton == RIGHT) scene.shift(); } void mouseMoved() { scene.updateTag(); } Conclusiones y Trabajo futuro\u003e Conclusiones y Trabajo futuro # El uso de la libería nub permite una comprensión más fácil y práctica del concepto de los cuaterniones, pues se logró evidenciar que las tres primeras posiciones indicaban sobre cuál de los 3 ejes (x, y o z) se hacía la rotación, mientras que el último valor permitía variar el ángulo. Aunque es posible generar una animación aleatorizando las posiciones, rotaciones y escalas de cada keyframe, se pudo comprobar que, con un estudio minucioso del manejo de vectores y cuaterniones, el programador puede definir con facilidad algunas transformaciones determinadas para que la animación tenga un propósito o cuente una historia. Dentro de las posibilidades de trabajo futuro, una de ellas consistiría en aprovechar los hallazgos encontrados para hacer modelos de interés científico, como lo pueden ser el sistema solar, los átomos, las sociedades artificiales, entre muchos otros. Sin embargo, para lograrlo habría que buscar una mejor manera de definir las trayectorias circulares o elípticas en reemplazo a las traslaciones en línea recta. Otra posible aplicación a futuro podría ser desarrollar una librería semejante para P5js, dado que el lenguaje Processing no cuenta con un editor web oficial (sólo se consiguen algunos que, incluso, advierten que ya no es tan utilizado y donde el manejo de liberías está lleno de dificultades). Esto permitiría aprovechar nociones matemáticas y de coherencia temporal para crear animaciones con un acabado profesional en javascript. Referencias\u003e Referencias # [1] J. P. Charalambos, \u0026ldquo;Temporal Coherence,\u0026rdquo; Visual Computing, Feb. 2023. https://visualcomputing.github.io/docs/visual_illusions/temporal_coherence/\u003e [1] J. P. Charalambos, \u0026ldquo;Temporal Coherence,\u0026rdquo; Visual Computing, Feb. 2023. https://visualcomputing.github.io/docs/visual_illusions/temporal_coherence/ # [2] R. Parent, \u0026ldquo;Computer Animation: Algorithms and Techniques,\u0026rdquo; 2nd ed. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc., 2012.\u003e [2] R. Parent, \u0026ldquo;Computer Animation: Algorithms and Techniques,\u0026rdquo; 2nd ed. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc., 2012. # [3] J. P. Charalambos, \u0026ldquo;nub: A library for processing large datasets in parallel,\u0026rdquo; Github, 2021. https://github.com/VisualComputing/nub\u003e [3] J. P. Charalambos, \u0026ldquo;nub: A library for processing large datasets in parallel,\u0026rdquo; Github, 2021. https://github.com/VisualComputing/nub # [4] K. Shoemake, \u0026ldquo;Animating rotation with quaternion curves,\u0026rdquo; in ACM SIGGRAPH Computer Graphics, vol. 19, no. 3, pp. 245-254, Jul. 1985, doi: 10.1145/325165.325242.\u003e [4] K. Shoemake, \u0026ldquo;Animating rotation with quaternion curves,\u0026rdquo; in ACM SIGGRAPH Computer Graphics, vol. 19, no. 3, pp. 245-254, Jul. 1985, doi: 10.1145/325165.325242. # ","date":"1 January 0001","permalink":"/showcase/first/keyframes/","section":"Firsts","summary":"Temporal Coherence\u003e Temporal Coherence # Introducción\u003e Introducción # En esta sección se discute, en primer lugar, acerca del fenómeno de coherencia temporal y la estrecha relación que guarda con las animaciones y, en particular, con el uso de fotogramas clave (keyframes).","title":""},{"content":"Visual Masking I\u003e Visual Masking I # Introducción\u003e Introducción # En esta sección se discute acerca del enmascaramiento visual, haciendo énfasis en los kinegramas y los patrones de moiré, que son fenómenos visuales estrechamente relacionados con esta acción.\nActo seguido, se desarrollan varios ejercicios relacionados con tales conceptos. En particular, se llevó a cabo un programa que genera un kinegrama a partir de un gif, así como un patrón de moiré en dos dimensiones y otro en tres dimensiones.\nPara terminar, se discuten posibles aplicaciones de los conceptos aprendidos, así como el trabajo futuro que se puede desarrollar mediante este tipo de patrones y animaciones.\nMarco Teórico\u003e Marco Teórico # El enmascaramiento visual es un fenómeno perceptual en el que la visibilidad de un estímulo se ve afectada por la presentación simultánea de otro estímulo: el que afecta la visibilidad del otro se conoce como enmascarador o máscara y puede ser presentado antes, después, o al mismo tiempo que el estímulo objetivo. Este fenómeno puede ocurrir tanto en la visión consciente como en la no consciente, y puede tener implicaciones en la percepción visual, la atención y la memoria [1].\nA continuación, se exponen los ejemplos de enmascaramiento visual que competen a esta sección:\nPatrones de moiré: Los patrones de moiré son un ejemplo de visual masking en el que se produce una interferencia entre dos patrones de líneas superpuestas con una ligera diferencia en su orientación o frecuencia espacial. El resultado es una percepción errónea de los patrones de líneas, con la aparición de un nuevo patrón que puede parecer más denso, más oscuro o con un patrón diferente al original. Este fenómeno puede tener aplicaciones en la detección de irregularidades en superficies o la verificación de seguridad en tarjetas de crédito o documentos de identidad [2].\nKinegramas: Los kinegramas son una técnica de impresión en la que se superponen patrones de líneas de diferentes tamaños y orientaciones para crear una imagen con efectos de movimiento y cambios de color. Se utilizan, comúnmente, como medida de seguridad en billetes, pasaportes y otros documentos oficiales, pues están hechos de patrones de líneas finas y detallados que, cuando se ven desde diferentes ángulos o se mueven, crean un efecto de profundidad y movimiento que es difícil de reproducir o falsificar [3].\nEn general, el proceso de creación de un kinegrama está compuesto por los siguientes pasos elementales [4]:\nSeleccionar un conjunto de imágenes que tengan el mismo tamaño y que representen una secuencia de movimiento suave o una transformación gradual, como una rotación o un cambio de forma. Superponer estas imágenes con un patrón de rejilla o de líneas finas. Ajustar la frecuencia y la velocidad del movimiento o la transformación para crear una animación fluida. Código y Resultados:\u003e Código y Resultados: # Inicialmente, se realizaron dos implementaciones de patrones de moiré: la primera de ellas se trata de dos conjuntos de círculos concentricos, donde el segundo grupo se desplaza horizontalmente. Es posible mover el slider para aumentar o disminuir la velocidad de tal desplazamiento.\nCódigo completo let direction = true; function setup() { createCanvas(650, 500); ellipseMode(RADIUS); slider = createSlider(10, 100, 50); slider.position(width/3, (9 * height) / 10); slider.style(\u0026#39;width\u0026#39;, \u0026#39;250px\u0026#39;); } function draw() { background(255); frameRate(slider.value()); if (frameCount % width == 0) direction = !direction; for (let i = 8; i \u0026lt;= height/2 - 40; i+=8){ noFill() strokeWeight(2) ellipse(width/2, height/2 - 40, i); if (direction) ellipse(frameCount % width, height/2 - 40, i); else ellipse(width - (frameCount % width), height/2 - 40, i); } } De este código se destaca la manera en que se trazan los círculos en movimiento:\nif (direction) ellipse(frameCount % width, height/2 - 40, i); else ellipse(width - (frameCount % width), height/2 - 40, i); Aquí, direction es un valor booleano que cambia cada vez que frameCount % width == 0: así se consigue que cambie el sentido de izquierda a derecha y viceversa, dado que cambia la coordenada en x mientras que lo demás permanece igual.\nUna vez se ha interiorizado el concepto mediante esta implementación sencilla, se procede con una más compleja, pues se trata de una animación en tres dimensiones que emplea color. De manera análoga, es posible cambiar la velocidad de rotación de cada patrón mediante el slider cuyo color coincide con las líneas (solo que, en este caso, deslizar hacia la izquierda incrementa la velocidad, mientras deslizar a la derecha hace que disminuya). Por otra parte, el ángulo de la cámara es mutable presionando las teclas de \u0026ldquo;flecha derecha\u0026rdquo; y \u0026ldquo;flecha izquierda\u0026rdquo; para tener la vista frontal o posterior, respectivamente.\nCódigo completo function setup() { createCanvas(700, 700, WEBGL); slider1 = createSlider(5000, 20000, 10000); slider1.position(width/3, 30); slider1.style(\u0026#39;width\u0026#39;, \u0026#39;250px\u0026#39;); slider1.style(\u0026#39;accent-color\u0026#39;, \u0026#39;red\u0026#39;); slider2 = createSlider(5000, 20000, 10000); slider2.position(width/12, height-50); slider2.style(\u0026#39;width\u0026#39;, \u0026#39;250px\u0026#39;); slider2.style(\u0026#39;accent-color\u0026#39;, \u0026#39;green\u0026#39;); slider3 = createSlider(5000, 20000, 10000); slider3.position(7*width/12, height-50); slider3.style(\u0026#39;width\u0026#39;, \u0026#39;250px\u0026#39;); slider3.style(\u0026#39;accent-color\u0026#39;, \u0026#39;blue\u0026#39;); cam = createCamera(); cam.setPosition(0, 0, 600); } function draw() { background(255); strokeWeight(3); rotateX(-millis()/slider1.value()); lines(\u0026#39;red\u0026#39;); rotateX(millis()/slider1.value()); rotateY(-millis()/slider2.value()); lines(\u0026#39;green\u0026#39;); rotateY(millis()/slider2.value()); rotateZ(-millis()/slider3.value()); lines(\u0026#39;blue\u0026#39;); if (keyIsDown(LEFT_ARROW)) { cam.setPosition(0, 0, -600); cam.lookAt(0,0,0); } if (keyIsDown(RIGHT_ARROW)) { cam.setPosition(0, 0, 600); cam.lookAt(0,0,0); } } function lines(color){ stroke(color) for (let i = 160 - width/2; i \u0026lt;= width/2 - 160; i += 5){ line(i, 160 - height/2, i, height/2 - 160) } } En este programa se destaca la manera en que se logran las rotaciones: rotateX(-millis()/slider1.value()); lines(\u0026#39;red\u0026#39;); rotateX(millis()/slider1.value()); rotateY(-millis()/slider2.value()); lines(\u0026#39;green\u0026#39;); rotateY(millis()/slider2.value()); rotateZ(-millis()/slider3.value()); lines(\u0026#39;blue\u0026#39;); Dado que las funciones rotateX, rotateY y rotateZ afectan a todos los elementos que se dibujen después de haberlas mencionado, fue necesario volver a aplicar rotateX con la misma magnitud pero el sentido opuesto para que el segundo patrón rotara únicamente en el eje y. Del mismo modo se volvió a aplicar rotateY al dibujar el tercer patrón para que este rotara tan solo en el eje z. Así se consiguió que cada conjunto de líneas girara sobre un eje distinto.\nPara el próximo ejercicio, partiendo de los pasos descritos en el marco teórico para crear un kiengrama, se implementó un programa que, tomando un archivo de tipo gif, extrajera sus primeros 20 fotogramas y construyera tal animación. Con el botón de la parte inferior es posible mostrar y ocultar la máscara, mientras que el slider permite controlar su velocidad.\nCódigo completo let keyframes = []; let kinegram; let overlay; let animate = false; let gif; let newGif; let gifLoading = false; let uploaded = false; function preload(){ gif = loadImage(\u0026#34;../assets/example.gif\u0026#34;) } function setup() { createCanvas(600, 600); imageMode(CENTER); textAlign(CENTER); if (!uploaded){ newGif = createFileInput(handleFile); newGif.position(width / 3, height / 8); slider = createSlider(5, 25, 5); slider.position((3 * width) / 5, (5 * height) / 6); slider.style(\u0026#39;width\u0026#39;, \u0026#39;100px\u0026#39;); } gif.resize(0,300) for (let i = 0; i \u0026lt; 20; i++) { gif.setFrame(i) keyframes[i] = createImage(gif.width, gif.height); gif.loadPixels(); keyframes[i].loadPixels(); for (let j = 0; j \u0026lt; gif.width; j++) { for (let k = 0; k \u0026lt; gif.height; k++) { keyframes[i].set(j, k, gif.get(j, k)); } } keyframes[i].updatePixels(); } kinegram = createImage(keyframes[0].width, keyframes[0].height); kinegram.loadPixels(); overlay = createImage(keyframes[0].width, keyframes[0].height); overlay.loadPixels(); for (let x = 0; x \u0026lt; overlay.width; x++) { for (let y = 0; y \u0026lt; overlay.height; y++) { let index = int(x/0.25) % keyframes.length; if (x \u0026lt; kinegram.width) { kinegram.set(x, y, keyframes[index].get(x, y)); } if (index \u0026gt; 0) { overlay.set(x, y, color(0, 255)); } else { overlay.set(x, y, color(0, 0)); } } } kinegram.updatePixels(); overlay.updatePixels(); button = createButton(\u0026#39;Toggle Overlay\u0026#39;); button.position(width / 4, (5 * height) / 6); button.mousePressed(() =\u0026gt; {animate = !animate}); } function draw() { background(255); if(!gifLoading){ image(kinegram, width / 2, height / 2); if (animate){ frameRate(slider.value()); image(overlay, (frameCount * 4) % width, height / 2); } } } function handleFile(file) { gifLoading = true; gif = createImg(file.data, \u0026#34;\u0026#34;); gif.hide(); let gifSrc = gif.attribute(\u0026#34;src\u0026#34;); gif = loadImage(gifSrc, gifLoaded); } function gifLoaded() { gifLoading = false; uploaded = true; setup(); } Por otra parte, con el botón de la parte superior es posible cargar cualquier gif desde la máquina local. Es importante tener en cuenta que su tamaño será reajustado a las proporciones del canvas. A continuación se deja una carpeta con archivos de prueba que se pueden cargar:\nAquí hay varias secciones del código que se destacan: en primera medida, se tiene el ciclo empleado para extraer los fotogramas del gif:\nfor (let i = 0; i \u0026lt; 20; i++) { gif.setFrame(i) keyframes[i] = createImage(gif.width, gif.height); gif.loadPixels(); keyframes[i].loadPixels(); for (let j = 0; j \u0026lt; gif.width; j++) { for (let k = 0; k \u0026lt; gif.height; k++) { keyframes[i].set(j, k, gif.get(j, k)); } } keyframes[i].updatePixels(); } Como se puede observar, se utiliza la función setFrame para obtener el fotograma y, luego, se cargan sus pixeles en otra imagen, la cual se almacena en un arreglo. Con los ciclos anidades se itera a través de los pixeles para copiarlos en la nueva imagen; finalmente, los pixeles se actualizan.\nEl kinegrama y la máscara se crean usando un ciclo parecido al anterior:\nfor (let x = 0; x \u0026lt; overlay.width; x++) { for (let y = 0; y \u0026lt; overlay.height; y++) { let index = int(x/0.25) % keyframes.length; if (x \u0026lt; kinegram.width) { kinegram.set(x, y, keyframes[index].get(x, y)); } if (index \u0026gt; 0) { overlay.set(x, y, color(0, 255)); } else { overlay.set(x, y, color(0, 0)); } } } La lógica es que se toma una columna de cada fotograma y sus pixeles se cargan en la nueva imagen (el kinegrama): esto se hace de manera iterativa, por ello se utiliza la función módulo respecto a la longitud del arreglo keyframes. Para la máscara, cada vez que este módulo sea igual a cero (cuando termina una \u0026ldquo;iteración\u0026rdquo;) se pinta de color negro y, en caso contrario, se deja transparente. La fracción respecto a x en el cálculo del índice fue un valor obtenido mediante ensayo y error que permitiese ver una animación fluida. Esta misma razón se tiene en cuenta al animar la máscara mediante el código image(overlay, (frameCount * 4) % width, height / 2), pues dividir entre 0.25 es equivalente a multiplicar por 4.\nPor último, se destacan las funciones para cargar gifs desde la máquina local:\nfunction handleFile(file) { gifLoading = true; gif = createImg(file.data, \u0026#34;\u0026#34;); gif.hide(); let gifSrc = gif.attribute(\u0026#34;src\u0026#34;); gif = loadImage(gifSrc, gifLoaded); } function gifLoaded() { gifLoading = false; uploaded = true; setup(); } Como se observa, cada vez que se carga un archivo, la bandera gifLoading se deja como verdadera, y se vuelve a poner como falsa en la función gifLoaded, es decir, cuando ya ha finalizado la carga. Se crea una nueva imagen en la variable gif (que luego debe ocultarse para evitar que aparezca bajo el canvas) y se carga la ruta del archivo con loadImage, cuyo callback es la función que indica que ya se cargó el archivo: dentro de esta última vuelve a llamarse el setup, pero se marca la bandera uploaded como verdadera para que se refresquen únicamente las imágenes (pero no los sliders, por ejemplo).\nConclusiones y Trabajo futuro\u003e Conclusiones y Trabajo futuro # Una particularidad muy interesante de los patrones de moiré que se observó en la segunda implementación son las deformaciones que se generan cuando el plano está en tres dimensiones: mientras que la superposición de patrones de líneas rectas en dos dimensiones genera efectos visuales rectilíneos, en este ejercicio la superposición de líneas rectas desemboca en la visualización de líneas curvas, dado que cada patrón gira sobre un eje distinto: x,y o z. Adicionalmente, estas líneas curvas se aprecian en tonalidades de amarillo, magenta y cian, puesto que las líneas rectas se trazaron en rojo, verde y azul (RGB). Mientras que elaborar un kinegrama manualmente puede ser una tarea compleja y que requiere conocimientos técnicos en programas de edición de imágenes, en p5js resulta convertirse en una tarea sencilla gracias a las múltiples funcionalidades que este lenguaje posee. Además, la propuesta realizada de convertir un gif a kinegrama permite comprender aún mejor su funcionamiento, dado que se ve la transición de animación a imagen estática y máscara, que vuelven a dar como resultado la misma animación cuando la máscara se pone en movimiento. No obstante, esta implementación también cuenta con algunos puntos negativos: por un lado, requiere ser muy precisa, porque calquier variación en el cálculo del índice podría estropear la armonía visual de la animación resultante (haciendo que no luzca ni remotamente parecido al gif original); por otra parte, consume muchos recursos tanto de tiempo, pues se aprecia que la página carga un poco más lento de lo usual, como de memoria, lo cual es evidente considerando toda la cantidad de imágenes y archivos que se están manipulando. Como trabajo futuro, se podría aplicar la implementación de patrones de moiré en tres dimensiones para construir, de manera análoga, kinegramas en tres dimensiones: esto podría conllevar un estudio a profunidad de cómo generar la máscara visual y, así mismo, de cómo debería ser su rotación para ofrecer una animación visualmente armoniosa. Referencias\u003e Referencias # [1] B. G. Breitmeyer, “Visual Masking: An Integrative Approach,” Oxford University Press, Oxford, 1984.\u003e [1] B. G. Breitmeyer, “Visual Masking: An Integrative Approach,” Oxford University Press, Oxford, 1984. # [2] L. Spillmann, \u0026ldquo;The Perception of Movement and Depth in Moiré Patterns,\u0026rdquo; Perception, vol. 22, no. 3, pp. 287-308, 1993, doi: 10.1068/p220287.\u003e [2] L. Spillmann, \u0026ldquo;The Perception of Movement and Depth in Moiré Patterns,\u0026rdquo; Perception, vol. 22, no. 3, pp. 287-308, 1993, doi: 10.1068/p220287. # [3] R. L. Van Renesse, \u0026ldquo;A review of holograms and other microstructures as security features,\u0026rdquo; Springer Series in Optical Sciences, vol. 78, 2007. https://www.dslreports.com/r0/download/2346751~41cd69e70ba7cfa509b37dddbba63faa/vanrenesse.pdf\u003e [3] R. L. Van Renesse, \u0026ldquo;A review of holograms and other microstructures as security features,\u0026rdquo; Springer Series in Optical Sciences, vol. 78, 2007. https://www.dslreports.com/r0/download/2346751~41cd69e70ba7cfa509b37dddbba63faa/vanrenesse.pdf # [4] O. Georgiou and M. Georgiou, \u0026ldquo;ZEBRA | COMPUTING MOIRE ANIMATIONS,\u0026rdquo; Sustainable Computational Workflows, pp. 49-56, 2018. http://papers.cumincad.org/data/works/att/ecaaderis2018_120.pdf\u003e [4] O. Georgiou and M. Georgiou, \u0026ldquo;ZEBRA | COMPUTING MOIRE ANIMATIONS,\u0026rdquo; Sustainable Computational Workflows, pp. 49-56, 2018. http://papers.cumincad.org/data/works/att/ecaaderis2018_120.pdf # ","date":"1 January 0001","permalink":"/showcase/first/masking1/","section":"Firsts","summary":"Visual Masking I\u003e Visual Masking I # Introducción\u003e Introducción # En esta sección se discute acerca del enmascaramiento visual, haciendo énfasis en los kinegramas y los patrones de moiré, que son fenómenos visuales estrechamente relacionados con esta acción.","title":""},{"content":"Visual Masking II\u003e Visual Masking II # Introducción\u003e Introducción # En esta sección se continúa con el enmascaramiento visual, pero esta vez el foco está en las máscaras de convolución (kernels de imágenes), así como en otras cualidades del procesamiento digital de imágenes: los histogramas y la luminosidad.\nActo seguido, se desarrolla una aplicación de procesamiento de imágenes que abarca algunos de los kernels más comunes: identity, edge detection, sharpen, emboss, gaussian blur, unsharp masking. Dentro del mismo programa se lleva a cabo la generación de un histograma con los valores de luminosidad del modo de color HSL: dicha luminosidad también es mutable en la imagen resultante de la convolución, alterando el histograma trazado en el proceso.\nPara terminar, se discuten posibles aplicaciones de los conceptos aprendidos, así como el trabajo futuro que se puede desarrollar mediante visual masking en general.\nMarco Teórico\u003e Marco Teórico # En la sección anterior se definía el enmascaramiento visual; por consiguiente, se pasa directamente a la conceptualización propia de esta sección.\nMáscaras de convolución: Para entender el concepto, en primer lugar es importante comprender qué es una convolución. De acuerdo con [1], esta se puede describir, intuitivamente, como una función que es la integral o suma de dos funciones componentes y que mide la cantidad de superposición a medida que una se desplaza sobre la otra. Formalmente, una convolución está definida de la siguiente manera [2]:\n\\[ g(x,y) = \\omega * f(x,y) = \\sum_{dx=-a}^{a}\\sum_{dy=-b}^{b} \\omega(dx,dy) f(x-dx,y-dy) \\] Ahora, una máscara de convolución es una matriz o kernel numérico utilizado en procesamiento de imágenes y visión por computadora para realizar operaciones de convolución en una imagen. Estas matrices, típicamente son de tamaño 3 x 3, aunque también se pueden emplear matrices de tamaño 2 x 2 y 5 x 5, entre otras. En líneas generales, en procedimiento consiste en que, a partir de una imagen, se van tomando bloques que posean el mismo tamaño de la matriz. Luego, aplicando la operación descrita (convolución) en función de los valores de los píxeles originales y los coeficientes de la máscara, se calcula el valor de cada píxel de la imagen resultante [1]. Esta tendrá ciertas características dependiendo del kernel que haya sido aplicado. Algunos de los más comunes (y los cuales fueron implementados en este ejercicio) se listan a continuación [2] y [3]:\nIdentity: \\[ \\begin{bmatrix} 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \\\\ \\end{bmatrix} \\] Edge detection: \\[ \\begin{bmatrix} -1 \u0026amp; -1 \u0026amp; -1 \\\\ -1 \u0026amp; 8 \u0026amp; -1 \\\\ -1 \u0026amp; -1 \u0026amp; -1 \\\\ \\end{bmatrix} \\] Sharpen: \\[ \\begin{bmatrix} 0 \u0026amp; -1 \u0026amp; 0 \\\\ -1 \u0026amp; 5 \u0026amp; -1 \\\\ 0 \u0026amp; -1 \u0026amp; 0 \\\\ \\end{bmatrix} \\] Emboss: \\[ \\begin{bmatrix} -2 \u0026amp; -1 \u0026amp; 0 \\\\ -1 \u0026amp; 1 \u0026amp; 1 \\\\ 0 \u0026amp; 1 \u0026amp; 2 \\\\ \\end{bmatrix} \\] Gaussian Blur (5x5): \\[ \\frac{1}{256} \\begin{bmatrix} 1 \u0026amp; 4 \u0026amp; 6 \u0026amp; 4 \u0026amp; 1 \\\\ 4 \u0026amp; 16 \u0026amp; 24 \u0026amp; 16 \u0026amp; 4 \\\\ 6 \u0026amp; 24 \u0026amp; 36 \u0026amp; 24 \u0026amp; 6 \\\\ 4 \u0026amp; 16 \u0026amp; 24 \u0026amp; 16 \u0026amp; 4 \\\\ 1 \u0026amp; 4 \u0026amp; 6 \u0026amp; 4 \u0026amp; 1 \\\\ \\end{bmatrix} \\] Unsharp Masking (5x5): \\[ \\frac{-1}{256} \\begin{bmatrix} 1 \u0026amp; 4 \u0026amp; 6 \u0026amp; 4 \u0026amp; 1 \\\\ 4 \u0026amp; 16 \u0026amp; 24 \u0026amp; 16 \u0026amp; 4 \\\\ 6 \u0026amp; 24 \u0026amp; -476 \u0026amp; 24 \u0026amp; 6 \\\\ 4 \u0026amp; 16 \u0026amp; 24 \u0026amp; 16 \u0026amp; 4 \\\\ 1 \u0026amp; 4 \u0026amp; 6 \u0026amp; 4 \u0026amp; 1 \\\\ \\end{bmatrix} \\] Luminosidad e histograma de imagen: En este ámbito, un histograma es una representación gráfica de la distribución de los valores de los píxeles en una imagen: el eje x representa los valores de intensidad de los píxeles, mientras que el eje y representa la cantidad de píxeles en la imagen que tienen cada valor de intensidad [4].\nLa intensidad es un valor presente en el modelo de color HSI, también conocido como HSL: sus siglas se refieren a Hue (matiz), que corresponde al tono que distingue a un color de otro; Saturation (saturación), que se relaciona con la pureza del color, es decir, la cantidad de gris en relación con el matiz; y Lightness (luminosidad), que es la cantidad de luz blanca que se mezcla con un color, dando una medida de la claridad u oscuridad del mismo con valores que van desde 0 (negro) hasta 100 (blanco) [4].\nCódigo y Resultados:\u003e Código y Resultados: # Código completo let file; let newFile let maskedFile; let lightFile let histogram; let fileLoading = false; let uploaded = false; function preload() { file = loadImage(\u0026#34;../../assets/shrek.png\u0026#34;); } function setup() { createCanvas(600, 1000); colorMode(RGB, 255); textSize(20); textAlign(CENTER); textStyle(BOLD); file.resize(250,250); maskedFile = createImage(file.width, file.height); lightFile = createImage(file.width, file.height); if (!uploaded){ newFile = createFileInput(handleFile); newFile.position(width/2 - 65, 25); slider = createSlider(-100, 100, 0); slider.position(325, 380); slider.style(\u0026#39;width\u0026#39;, \u0026#39;250px\u0026#39;); menu = createSelect(); menu.position(25, 380); menu.style(\u0026#39;width\u0026#39;, \u0026#39;250px\u0026#39;); menu.option(\u0026#34;Identity\u0026#34;); menu.option(\u0026#34;Edge Detection\u0026#34;); menu.option(\u0026#34;Sharpen\u0026#34;); menu.option(\u0026#34;Emboss\u0026#34;); menu.option(\u0026#34;Gaussian Blur 5x5\u0026#34;); menu.option(\u0026#34;Unsharp Masking 5x5\u0026#34;); resetHistogram(); file.loadPixels(); maskedFile.loadPixels(); for (let j = 0; j \u0026lt; file.width; j++) { for (let k = 0; k \u0026lt; file.height; k++) { let pixel = file.get(j, k); maskedFile.set(j, k, pixel); lightFile.set(j, k, pixel); histogram[int(lightness(pixel))]++; } } scaleHistogram(); maskedFile.updatePixels(); lightFile.updatePixels(); } else { masking(); } slider.input(changeLightness); menu.input(masking); } function draw() { background(0); noStroke(); fill(\u0026#34;white\u0026#34;) text(\u0026#34;Lightness:\u0026#34;, 3*width/4, 360); text(\u0026#34;Kernel:\u0026#34;, width/4, 360); rect(0, 420, width, height-420) fill(\u0026#34;black\u0026#34;) text(\u0026#34;Histograma de la imagen resultante:\u0026#34;, width/2, 450); if(!fileLoading){ image(file, 25, 70); image(lightFile, 325, 70); stroke(\u0026#34;purple\u0026#34;); for (let w = 0; w \u0026lt;= 100; w++) { let ab = int(map(w, 0, 100, 25, width - 25)); line(ab, height - histogram[w] - 30, ab, height - 30); } } } function changeLightness(){ resetHistogram(); for (let z = 0; z \u0026lt; maskedFile.width; z++) { for (let k = 0; k \u0026lt; maskedFile.height; k++) { let pixel = maskedFile.get(z, k); let l = constrain(int(lightness(pixel) + slider.value()), 0, 100) let newColor = \u0026#34;hsl(\u0026#34; + int(hue(pixel)) + \u0026#34;, \u0026#34; + saturation(pixel) + \u0026#34;%, \u0026#34; + l + \u0026#34;%)\u0026#34; lightFile.set(z, k, color(newColor)); histogram[l]++; } } lightFile.updatePixels(); scaleHistogram(); } function changeKernel(M){ resetHistogram(); let rangeX = int(M.length/2) let rangeY = int(M[0].length/2) for (let x = 0; x \u0026lt; maskedFile.width; x++) { for (let y = 0; y \u0026lt; maskedFile.height; y++) { let convolution = [0,0,0,255] for (let i = -rangeX; i \u0026lt;= rangeX; i++) { for (let j = -rangeY; j \u0026lt;= rangeY; j++) { let pixel = file.get(x + i, y + j); let val = M[i + rangeX][j + rangeY] convolution[0] += pixel[0] * val; convolution[1] += pixel[1] * val; convolution[2] += pixel[2] * val; } } histogram[int(lightness(convolution))]++; maskedFile.set(x, y, convolution) } } maskedFile.updatePixels(); scaleHistogram(); changeLightness(); } function masking() { if (menu.value() == \u0026#34;Identity\u0026#34;){ matrix = [ [ 0, 0, 0 ], [ 0, 1, 0 ], [ 0, 0, 0 ] ]; } else if (menu.value() == \u0026#34;Edge Detection\u0026#34;){ matrix = [ [ -1, -1, -1 ], [ -1, 8, -1 ], [ -1, -1, -1 ] ]; } else if (menu.value() == \u0026#34;Sharpen\u0026#34;){ matrix = [ [ 0, -1, 0 ], [ -1, 5, -1 ], [ 0, -1, 0 ] ]; } else if (menu.value() == \u0026#34;Emboss\u0026#34;){ matrix = [ [ -2, -1, 0 ], [ -1, 1, 1 ], [ 0, 1, 2 ] ]; } else if (menu.value() == \u0026#34;Gaussian Blur 5x5\u0026#34;){ matrix = [ [ 1/256, 4/256, 6/256, 4/256, 1/256 ], [ 4/256, 16/256, 24/256, 16/256, 4/256 ], [ 6/256, 24/256, 36/256, 24/256, 6/256 ], [ 4/256, 16/256, 24/256, 16/256, 4/256 ], [ 1/256, 4/256, 6/256, 4/256, 1/256 ] ]; } else if (menu.value() == \u0026#34;Unsharp Masking 5x5\u0026#34;){ matrix = [ [ -1/256, -4/256, -6/256, -4/256, -1/256 ], [ -4/256, -16/256, -24/256, -16/256, -4/256 ], [ -6/256, -24/256, 476/256, -24/256, -6/256 ], [ -4/256, -16/256, -24/256, -16/256, -4/256 ], [ -1/256, -4/256, -6/256, -4/256, -1/256 ] ]; } changeKernel(matrix); } function resetHistogram(){ histogram = []; for (let i = 0; i \u0026lt;= 100; i++){ append(histogram, 0); } } function scaleHistogram(){ for (let i = 0; i \u0026lt;= 100; i++){ histogram[i] = int(map(histogram[i], min(histogram), max(histogram), 0, 500, true)); } } function handleFile(img) { fileLoading = true; file = createImg(img.data, \u0026#34;\u0026#34;); file.hide(); let fileSrc = file.attribute(\u0026#34;src\u0026#34;); file = loadImage(fileSrc, fileLoaded); } function fileLoaded() { fileLoading = false; uploaded = true; setup(); } El programa desarrollado es extenso y posee varias funcionalidades: en primer lugar, el botón de la parte superior sirve para cargar imágenes desde la máquina local. Es importante que la imagen sea cuadrada desde un inicio para evitar deformaciones cuando se reajuste su tamaño. A continuación se deja una carpeta con archivos de prueba que se pueden cargar para probar el funcionamiento:\nEn segundo lugar, a la izquierda aparece la imagen original, mientras que a la derecha aparece la imagen transformada. Debajo de ellas hay un selector y un slider: con el primero es posible elegir el kernel a aplicar y con el segundo es posible cambiar la luminosidad.\nFinalmente, se tiene el histograma que genera la imagen resultante, es decir, después de sufrir la convolución y el cambio en la luminosidad: se hizo de esta forma porque así es posible ver las variaciones de este atributo para cada caso.\nEn cuanto al código, se tienen las mismas funciones para la carga de archivos que en la sección anterior sobre visual masking: allí están detalladas por si se desea conocer su funcionamiento. Como tal, en esta aplicación hay dos funciones que se destacan: la del cambio de luminosidad y la del cambio de kernel.\nfunction changeLightness(){ resetHistogram(); for (let z = 0; z \u0026lt; maskedFile.width; z++) { for (let k = 0; k \u0026lt; maskedFile.height; k++) { let pixel = maskedFile.get(z, k); let l = constrain(int(lightness(pixel) + slider.value()), 0, 100) let newColor = \u0026#34;hsl(\u0026#34; + int(hue(pixel)) + \u0026#34;, \u0026#34; + saturation(pixel) + \u0026#34;%, \u0026#34; + l + \u0026#34;%)\u0026#34; lightFile.set(z, k, color(newColor)); histogram[l]++; } } lightFile.updatePixels(); scaleHistogram(); } Esta función del cambio de luminosidad, primero que todo, resetea el histograma (pues está mostrando la distribución de la luminosidad y esta va a cambiar en cuanto se deslice el slider): esto equivale a poner todos sus valores en cero. Acto seguido, itera sobre cada pixel de la imagen emnascarada (es decir, la que ya pasó por la convolución) y obtiene su nuevo valor de luminosidad, limitándolo en el rango de cero a cien.\nPara actualizar la imagen, se crea un nuevo pixel utilizando el modo de color HSL, a diferencia del canvas, que utiliza RGB: aquí se recibe el matiz y la saturación sin variaciones, pero en la luminosidad sí se pasa el valor alterado. Por último, se incrementa el valor del histograma en la posición l, esto es porque el eje x representa los valores del cero al cien que puede tomar la luminosidad, dado que se va a mostrar la distribución que tiene en la imagen. Al final se actualizan los pixeles del archivo lightFile (el que finalmente se muestra al usuario) y se escala al histograma para que este se muestre de manera apropiada en el canvas.\nfunction changeKernel(M){ resetHistogram(); let rangeX = int(M.length/2) let rangeY = int(M[0].length/2) for (let x = 0; x \u0026lt; maskedFile.width; x++) { for (let y = 0; y \u0026lt; maskedFile.height; y++) { let convolution = [0,0,0,255] for (let i = -rangeX; i \u0026lt;= rangeX; i++) { for (let j = -rangeY; j \u0026lt;= rangeY; j++) { let pixel = file.get(x + i, y + j); let val = M[i + rangeX][j + rangeY] convolution[0] += pixel[0] * val; convolution[1] += pixel[1] * val; convolution[2] += pixel[2] * val; } } histogram[int(lightness(convolution))]++; maskedFile.set(x, y, convolution) } } maskedFile.updatePixels(); scaleHistogram(); changeLightness(); } Ahora, en la función de cambiar el kernel también se reinicia el histograma. Se calculan las variables rangeX y rangeY para saber qué tan atrás se debe ir en los pixeles que están al borde de la imagen. Luego están cuatro ciclos anidados: con los dos primeros se itera sobre los pixeles de la imagen original, mientras que con los dos internos se itera en los bloques del mismo tamaño de la matriz del kernel para realizar la operación de convolución. Este procedimiento se aplica sobre cada valor R, G y B del píxel, dando como resultado un pixel nuevo, llamado convolution, el cual se pone en el archivo maskedFile.\nPor su parte, para el histograma se calcula la luminosidad del pixel nuevo y se incrementa el valor en dicha posición del arreglo. Al final se actualizan los pixeles, se escala el histograma y se actualiza la luminosidad, pues es el archivo lightFile y no maskedFile el que se está renderizando junto a la imagen original. Esta función se llama dentro de la función masking, la cual contiene las matrices expresadas en el marco teórico y le pasa una de ellas como parámetro a la función changeKernel a partir de lo que se ingresó en el selector.\nConclusiones y Trabajo futuro\u003e Conclusiones y Trabajo futuro # Se puede evidenciar que el uso de máscaras de convolución es muy útil en el procesamiento de imágenes digitales, pues dependiendo del kernel aplicado es posible obtener información relevante como los bordes de la imagen o versiones más nítidas y/o borrrosas de la misma. Esto es algo que también se puede lograr mediante la aplicación de métodos numéricos, pero resulta ser mucho más sencillo (y práctico) con las técnicas desarrolladas en esta sección. El hecho de que el histograma de la imagen cambie en tiempo real permite reconocer la importancia de la luminosidad, pues su aumento o su decremento altera por completo la distribución del gráfico. Una posible aplicación, que podría considerarse también como trabajo futuro, consistiría en tener sliders que permitan variar las tonalidades R, G y B de manera individual, con el fin de evaluar qué impacto tienen en el cáculo y la distribución de la luminosidad en la imagen. Entre otros trabajos futuros a desarrollar, se considera la posibilidad de aplicar máscaras de convolución a animaciones y vídeos: esto podría ser una herramienta ampliamente útil, pero que podría consumir muchos recursos computacionales, puesto que, finalmente, se terminaría aplicando la operación en cada pixel de cada uno de los fotogramas. En síntesis, el enmascaramiento visual ofrece un abanico de posibilidades, tanto en lo que respecta a patrones y animaciones de moiré como a kernels de imágenes: los conceptos aprendidos pueden aplicarse para evaluar la atención visual de un producto multimedia, mejorar su accesibilidad y/o armonía, detectar ciertos patrones, entre muchos otros. Así mismo, gracias a esto también es posible comprender cómo funcionan los filtros que de edición de fotografías en los celulares y las aplicaciones móviles, e incluso algunas de las herramientas que pueden llegar a emplear las inteligencias artificiales relacionadas con el procesamiento de imágenes digitales. Referencias\u003e Referencias # [1] S. Kim and R. Casper, \u0026ldquo;Applications of convolution in image processing with MATLAB,\u0026rdquo; University of Washington, pp. 1-20, 2013. http://kiwi.bridgeport.edu/cpeg585/ConvolutionFiltersInMatlab.pdf\u003e [1] S. Kim and R. Casper, \u0026ldquo;Applications of convolution in image processing with MATLAB,\u0026rdquo; University of Washington, pp. 1-20, 2013. http://kiwi.bridgeport.edu/cpeg585/ConvolutionFiltersInMatlab.pdf # [2] J. P. Charalambos, \u0026ldquo;Visual masking\u0026rdquo;. Visual Computing, Feb. 2023. https://visualcomputing.github.io/docs/visual_illusions/masking/\u003e [2] J. P. Charalambos, \u0026ldquo;Visual masking\u0026rdquo;. Visual Computing, Feb. 2023. https://visualcomputing.github.io/docs/visual_illusions/masking/ # [3] S. Raveendran, P. J. Edavoor, N. K. Yernad Balachandra and V. Moodabettu Harishchandra, \u0026ldquo;Design and implementation of image kernels using reversible logic gates,\u0026rdquo; IET Image Processing, vol. 14, no 16, pp. 4110-4121, 2020, doi: 10.1049/iet-ipr.2019.1681.\u003e [3] S. Raveendran, P. J. Edavoor, N. K. Yernad Balachandra and V. Moodabettu Harishchandra, \u0026ldquo;Design and implementation of image kernels using reversible logic gates,\u0026rdquo; IET Image Processing, vol. 14, no 16, pp. 4110-4121, 2020, doi: 10.1049/iet-ipr.2019.1681. # [4] R. C. Gonzalez and R. E. Woods, \u0026ldquo;Digital Image Processing,\u0026rdquo; 4th ed., Pearson, 2018.\u003e [4] R. C. Gonzalez and R. E. Woods, \u0026ldquo;Digital Image Processing,\u0026rdquo; 4th ed., Pearson, 2018. # ","date":"1 January 0001","permalink":"/showcase/first/masking2/","section":"Firsts","summary":"Visual Masking II\u003e Visual Masking II # Introducción\u003e Introducción # En esta sección se continúa con el enmascaramiento visual, pero esta vez el foco está en las máscaras de convolución (kernels de imágenes), así como en otras cualidades del procesamiento digital de imágenes: los histogramas y la luminosidad.","title":""},{"content":"Spatial Coherence\u003e Spatial Coherence # Introducción\u003e Introducción # Para este ejercicio se puso en práctica el fenómeno visual de \u0026ldquo;Spatial Coherence\u0026rdquo; con el objetivo de desarrollar un código capaz de pixelar un video.\nDurante este ejercicio se buscó ir un poco más allá de lo que se pedía como tarea inicial: por consiguiente, se llevó a cabo el pixelado de un video en vivo y se agregaron diversas funciones para que el usuario pudiese interectuar con el programa. Todo esto, por supuesto, hecho a través de la implementación de un código en JavaScript, haciendo uso de la libreria P5js.\nFinalmente, se discuten posibles aplicaciones de los conceptos aprendidos, así como el trabajo futuro que se puede llevar a cabo tomando el ejercicio desarrollado como punto de partida.\nMarco Teórico\u003e Marco Teórico # La coherencia espacial es un término que se utiliza en la óptica para describir la capacidad de una onda electromagnética para mantener una relación de fase estable en diferentes puntos del espacio. La coherencia espacial se refiere a la propiedad de la luz que hace que se comporte como una onda: esto significa que tiene la capacidad de interferir constructiva o destructivamente consigo misma en diferentes puntos del espacio [1].\nLa coherencia espacial se utiliza en muchas aplicaciones ópticas, como la holografía, la tomografía óptica de coherencia y la microscopía de campo cercano [7]. En estas aplicaciones, la coherencia espacial es esencial para la formación de imágenes y para obtener información detallada sobre la estructura de los objetos que se están observando.\nLa coherencia espacial se puede cuantificar utilizando varias medidas, tales como la función de correlación espacial, el ancho de banda de coherencia y la longitud de coherencia: la función de correlación espacial es una medida de la correlación entre dos puntos de la onda en diferentes posiciones, el ancho de banda de coherencia es una medida de la gama de longitudes de onda que contribuyen a la coherencia de la onda y la longitud de coherencia es una medida de la distancia sobre la cual la onda mantiene una relación de fase estable [1].\nEn resumen, la coherencia espacial es una propiedad fundamental de las ondas electromagnéticas que permite que se comporten como ondas y que puedan interferir constructiva o destructivamente en diferentes puntos del espacio. La coherencia espacial se utiliza en muchas aplicaciones ópticas para la formación de imágenes y para obtener información detallada sobre la estructura de los objetos que se están observando.\nCódigo y Resultados\u003e Código y Resultados # A continuación, se describe el ejercicio desarrollado y se destacan las piezas de código más relevantes, las cuales demuestran los aportes efectuados sobre el ejemplo disponible. Cabe aclarar que para el correcto funcionamiento del código se debe autorizar a la página para que esta use la webcam.\nComo se puede evidenciar, el ejercicio desarrollado consiste en capturar la imagen que proviene de la cámara del computador y modificar los pixeles provinientes de cada frame: es posible variar su tamaño al desplazar el mouse horizontalemnte, así como su forma y su color mediante los botones de la sección inferior.\nPara lograrlo, se toma cada fotograma como una matriz compuesta de pequeñas submatrices, las cuales se manipulan para lograr cada efecto visual propuesto. Primero, se emplea una función interna de P5 llamada LoadPixels [9], la cual facilita el trabajo y permite usar cada frame como una matriz. Después, se crea una variable que permite manejar el valor de cada pixel a partir del tamaño de la pantalla, la posición del mouse sobre el eje x y otros valores predeterminados. Una vez definido el tamaño de los pixeles, estos se agrupan en pequeñas submatrices de 4 x 4: es aquí cuando se empieza a jugar con el tamaño de los pixeles. Por un lado, en caso de querer disminuir el número de pixeles en la transmisión, a cada pequeña submatriz se le asigna un color siguiendo el principio de la coherencia espacial [10]. Una vez que todos los pixeles de esa matriz tienen el mismo color, esta pasa a ser considerada como un único pixel, el cual ahora forma parte de una matriz más grande. Este proceso se puede repetir hasta llegar al mínimo de pixeles permitidos por frame.\nCabe aclarar que cada pixel debe ser escalado para que se ajuste al tamaño de la pantalla. Cuando se modifica la forma de los pixeles, dicha función se ejecuta, precisamente, sobre aquellos que están escalados. Entonces, al igual como se manipulan las submatrices para modificar el tamaño de los pixeles de la imagen (y su color por spatial coherence), tanto la forma como la paleta de colores se alteran siguiendo la misma lógica y empleando más funciones especiales de P5js [2] [3] [4] [5] [6] [7].\nEn la siguiente porción de código se resalta cómo se generan los valores para cada pixel y, además, cómo se van manipulando el tamaño, la forma o el color dependiendo de la posición del mouse y la opción que escoja el usuario.\nvideo.loadPixels(); if (isProcessingEnabled) { pixelSize = int(map(mouseX, 0, windowWidth, minSize, maxSize)); for (let x = 0; x \u0026lt; video.width; x += pixelSize) { for (let y = 0; y \u0026lt; video.height; y += pixelSize) { let index = (y * video.width + x) * 4; let r = video.pixels[index + 0]; let g = video.pixels[index + 1]; let b = video.pixels[index + 2]; let newColor = getColor(r, g, b); // Scale to fit windowWidth: let scaledX = floor(x * windowVideoRatio); let scaledY = floor(y * windowVideoRatio); let scaledPixelSize = ceil(pixelSize * windowVideoRatio); fill(newColor); if(pixelMode == 0){ stroke(\u0026#34;black\u0026#34;); rect(scaledX, scaledY, scaledPixelSize, scaledPixelSize); }else if(pixelMode == 1){ ellipse(scaledX, scaledY, scaledPixelSize, scaledPixelSize); }else if(pixelMode == 2){ triangle(scaledX, scaledY, scaledX + (pixelSize / 2), scaledY + pixelSize, scaledX + pixelSize, scaledY) } else if(pixelMode == 3){ noStroke() rect(scaledX, scaledY, scaledPixelSize, scaledPixelSize); } } } } else { image(video, 0,0, videoXResolution * windowVideoRatio, videoYResolution * windowVideoRatio); } El código completo se encuentra en el siguiente desplegable:\nCódigo completo const videoXResolution = 640; const videoYResolution = 480; let video; let pixelSize = 10; const minSize = 5; const maxSize = 50; let windowVideoRatio; let colorButton; let videoModeButton; let isProcessingEnabled = true; let colorModeIndex = 0; let lastColorModeIndex = 4; let pixelMode = 0 let lastPixelMode = 3 function setup() { createCanvas(800, 550); video = createCapture(VIDEO); video.size(videoXResolution, videoYResolution); video.hide(); windowVideoRatio = windowWidth / video.width; colorButton = createButton(\u0026#34;Toggle color modes\u0026#34;); colorButton.size(200, 50); colorButton.position(230, windowHeight - 60); colorButton.mousePressed(changeColorMode); colorButton = createButton(\u0026#34;Toggle processing\u0026#34;); colorButton.size(200, 50); colorButton.position(10, windowHeight - 60); colorButton.mousePressed(changeProcessing); colorButton = createButton(\u0026#34;Change pixels\u0026#34;); colorButton.size(200, 50); colorButton.position(450, windowHeight - 60); colorButton.mousePressed(changePixels); } function draw() { background(0); video.loadPixels(); if (isProcessingEnabled) { pixelSize = int(map(mouseX, 0, windowWidth, minSize, maxSize)); for (let x = 0; x \u0026lt; video.width; x += pixelSize) { for (let y = 0; y \u0026lt; video.height; y += pixelSize) { let index = (y * video.width + x) * 4; let r = video.pixels[index + 0]; let g = video.pixels[index + 1]; let b = video.pixels[index + 2]; let newColor = getColor(r, g, b); // Scale to fit windowWidth: let scaledX = floor(x * windowVideoRatio); let scaledY = floor(y * windowVideoRatio); let scaledPixelSize = ceil(pixelSize * windowVideoRatio); fill(newColor); if(pixelMode == 0){ stroke(\u0026#34;black\u0026#34;); rect(scaledX, scaledY, scaledPixelSize, scaledPixelSize); }else if(pixelMode == 1){ ellipse(scaledX, scaledY, scaledPixelSize, scaledPixelSize); }else if(pixelMode == 2){ triangle(scaledX, scaledY, scaledX + (pixelSize / 2), scaledY + pixelSize, scaledX + pixelSize, scaledY) } else if(pixelMode == 3){ noStroke() rect(scaledX, scaledY, scaledPixelSize, scaledPixelSize); } } } } else { image(video, 0,0, videoXResolution * windowVideoRatio, videoYResolution * windowVideoRatio); } textSize(16); fill(255); text(`Pixel size: ${pixelSize} - Frame rate: ${int(frameRate())}`, 10, windowHeight - 80); } function changeColorMode() { if (colorModeIndex \u0026lt; lastColorModeIndex) { colorModeIndex++; } else { colorModeIndex = 0; } } function changeProcessing() { isProcessingEnabled = !isProcessingEnabled; } function changePixels() { if (pixelMode \u0026lt; lastPixelMode) { pixelMode++; } else { pixelMode = 0; } } function getColor(r, g, b) { let newColor; switch (colorModeIndex) { case 0: newColor = color(r, g, b); break; case 1: newColor = getGrayScaleColor(r, g, b); break; case 2: newColor = getGameboyColor(r, g, b); break; case 3: newColor = getFunkyFutureColor(r, g, b); break; case 4: newColor = getFairyDustColor(r, g, b); break; } return newColor; } function getGrayScaleColor(r, g, b) { return 0.2126 * r + 0.7152 * g + 0.0722 * b; } function getGameboyColor(r, g, b) { let grayScale = getGrayScaleColor(r, g, b); let colorPalette = [\u0026#34;#332c50\u0026#34;, \u0026#34;#46878f\u0026#34;, \u0026#34;#94e344\u0026#34;, \u0026#34;#e2f3e4\u0026#34;]; let index = floor(map(grayScale, 0, 255, 0, 4)); return colorPalette[index]; } function getFunkyFutureColor(r, g, b) { let colorPalette = [ \u0026#34;#2b0f54\u0026#34;, \u0026#34;#ab1f65\u0026#34;, \u0026#34;#ff4f69\u0026#34;, \u0026#34;#fff7f8\u0026#34;, \u0026#34;#ff8142\u0026#34;, \u0026#34;#ffda45\u0026#34;, \u0026#34;#3368dc\u0026#34;, \u0026#34;#49e7ec\u0026#34;, ]; return getNearestColorInPalette(colorPalette, r, g, b); } function getFairyDustColor(r, g, b) { let colorPalette = [ \u0026#34;#f0dab1\u0026#34;, \u0026#34;#e39aac\u0026#34;, \u0026#34;#c45d9f\u0026#34;, \u0026#34;#634b7d\u0026#34;, \u0026#34;#6461c2\u0026#34;, \u0026#34;#2ba9b4\u0026#34;, \u0026#34;#93d4b5\u0026#34;, \u0026#34;#f0f6e8\u0026#34;, ]; return getNearestColorInPalette(colorPalette, r, g, b); } function getNearestColorInPalette(colorPalette, r, g, b) { let nearestColorIndex = 0; let nearestColorDistance = 255; for (let c = 0; c \u0026lt; colorPalette.length; c++) { let indexColor = color(colorPalette[c]); let indexRed = red(indexColor); let indexGreen = green(indexColor); let indexBlue = blue(indexColor); let colorDist = dist(r, g, b, indexRed, indexGreen, indexBlue); if (colorDist \u0026lt; nearestColorDistance) { nearestColorDistance = colorDist; nearestColorIndex = c; } } return colorPalette[nearestColorIndex]; } Conclusiones y Trabajo futuro\u003e Conclusiones y Trabajo futuro # El uso de la libería P5js otorga muchas facilidades a la hora de realizar el ejercicio de pixelar una imagen (y, en consencuencia, los frames de un video). Mediante las diversas funciones internas que brinda es posible solventar muchas dificultades que se presentan, lo que permite ahorrar tiempo y esfuerzo. Como el programa está hecho para modificar cada pixel de cada frame de un video en vivo, hay muchos valores que se deben calcular por segundo. En consecuencia, se debe llevar a cabo un gran procesamiento interno y se puede evidenciar que, a medida que aumenta el número de pixeles en el video, este va disminuyendo los frames por segundo que pude procesar. Sí a esto se le suman todas las funciones extra que el usuario puede realizar, esto se termina convirtiendo en una ardua tarea para la máquina, así que es razonable que los frames procesados por segundo caigan tanto en los escenarios más complejos. Para un trabajo futuro, se podría implementar una función que permita una mejora en la tasa de procesamiento para cada pixel y en la tasa de refresco de frames en pantalla. Con esto, se podría lograr una mayor fluidez en la transmisión cuando el número de pixeles aumente. Otro trabajo futuro consistiría en implementar diversas formas de manipular las submatrices de pixeles (por ejemplo utilizando promedio de color en lugar de coherencia espacial) y, de tal forma, comparar cuál código tiene un mejor desempeño a la hora de generar frames por segundo, así como una mejor calidad visual. Referencias\u003e Referencias # [1] J. P. Charalambos. \u0026ldquo;Spatial Coherence\u0026rdquo;. Visual Computing. 2023. https://visualcomputing.github.io/docs/visual_illusions/spatial_coherence/\u003e [1] J. P. Charalambos. \u0026ldquo;Spatial Coherence\u0026rdquo;. Visual Computing. 2023. https://visualcomputing.github.io/docs/visual_illusions/spatial_coherence/ # [2] \u0026ldquo;Kirokaze (Game Boy)\u0026rdquo;, Lospec, [En línea]. Available: https://lospec.com/palette-list/kirokaze-gameboy.\u003e [2] \u0026ldquo;Kirokaze (Game Boy)\u0026rdquo;, Lospec, [En línea]. Available: https://lospec.com/palette-list/kirokaze-gameboy. # [3] \u0026ldquo;Funky Future 8\u0026rdquo;, Lospec, [En línea]. Available: https://lospec.com/palette-list/funkyfuture-8.\u003e [3] \u0026ldquo;Funky Future 8\u0026rdquo;, Lospec, [En línea]. Available: https://lospec.com/palette-list/funkyfuture-8. # [4] \u0026ldquo;Fairydust 8\u0026rdquo;, Lospec, [En línea]. Available: https://lospec.com/palette-list/fairydust-8.\u003e [4] \u0026ldquo;Fairydust 8\u0026rdquo;, Lospec, [En línea]. Available: https://lospec.com/palette-list/fairydust-8. # [5] E. Glytsis, \u0026ldquo;Spatial Coherence,\u0026rdquo; [En línea]. Available: http://users.ntua.gr/eglytsis/OptEng/Coherence_p.pdf.\u003e [5] E. Glytsis, \u0026ldquo;Spatial Coherence,\u0026rdquo; [En línea]. Available: http://users.ntua.gr/eglytsis/OptEng/Coherence_p.pdf. # [6] S. Johnson, \u0026ldquo;Stephen Johnson on Digital Photography,\u0026rdquo; O\u0026rsquo;Reilly, 2006, ISBN 0-596-52370-X.\u003e [6] S. Johnson, \u0026ldquo;Stephen Johnson on Digital Photography,\u0026rdquo; O\u0026rsquo;Reilly, 2006, ISBN 0-596-52370-X. # [7] C. Poynton, \u0026ldquo;Digital Video and HD: Algorithms and Interfaces,\u0026rdquo; 2nd ed., Morgan Kaufmann, 2012, pp. 31-35, 65-68, 333, 337, ISBN 978-0-12-391926-7. [Online]. Available: https://www.sciencedirect.com/book/9780123919267/digital-video-and-hd. [Accessed: Mar. 31, 2022].\u003e [7] C. Poynton, \u0026ldquo;Digital Video and HD: Algorithms and Interfaces,\u0026rdquo; 2nd ed., Morgan Kaufmann, 2012, pp. 31-35, 65-68, 333, 337, ISBN 978-0-12-391926-7. [Online]. Available: https://www.sciencedirect.com/book/9780123919267/digital-video-and-hd. [Accessed: Mar. 31, 2022]. # [8] Daniel Shiffman, \u0026ldquo;The Nature of Code - Simulating Natural Systems with Processing,\u0026rdquo; YouTube, 2012. [En línea]. Available: https://www.youtube.com/watch?v=KfLqRuFjK5g. [Accedido: Mar. 31, 2022].\u003e [8] Daniel Shiffman, \u0026ldquo;The Nature of Code - Simulating Natural Systems with Processing,\u0026rdquo; YouTube, 2012. [En línea]. Available: https://www.youtube.com/watch?v=KfLqRuFjK5g. [Accedido: Mar. 31, 2022]. # [9] The Coding Train, \u0026ldquo;What is p5.js?,\u0026rdquo; YouTube, 2017. [En línea]. Available: https://www.youtube.com/watch?v=M3wTNVICUTg\u0026t=2s. [Accedido: Mar. 31, 2022].\u003e [9] The Coding Train, \u0026ldquo;What is p5.js?,\u0026rdquo; YouTube, 2017. [En línea]. Available: https://www.youtube.com/watch?v=M3wTNVICUTg\u0026t=2s. [Accedido: Mar. 31, 2022]. # [10] The Coding Train, \u0026ldquo;Introduction to p5.js - Variables and Drawing - p5.js Tutorial,\u0026rdquo; YouTube, 2015. [En línea]. Available: https://www.youtube.com/watch?v=VYg-YdGpW1o. [Accedido: Mar. 31, 2022].\u003e [10] The Coding Train, \u0026ldquo;Introduction to p5.js - Variables and Drawing - p5.js Tutorial,\u0026rdquo; YouTube, 2015. [En línea]. Available: https://www.youtube.com/watch?v=VYg-YdGpW1o. [Accedido: Mar. 31, 2022]. # ","date":"1 January 0001","permalink":"/showcase/first/pixelator/","section":"Firsts","summary":"Spatial Coherence\u003e Spatial Coherence # Introducción\u003e Introducción # Para este ejercicio se puso en práctica el fenómeno visual de \u0026ldquo;Spatial Coherence\u0026rdquo; con el objetivo de desarrollar un código capaz de pixelar un video.","title":""},{"content":"Mach Bands\u003e Mach Bands # Introducción\u003e Introducción # Para este ejercicio se definen los efectos visuales de \u0026ldquo;Mach bands\u0026rdquo; y Perlin noise, así como la relación que guardan al momento de generar un terreno automatizado y continuo.\nPosteriormente, como ejercicio práctico, se realizó una animación a través de la implementación de un código en JavaScript, haciendo uso de la libreria P5.js: esta consiste en generar terreno de manera aleatoria por medio de los dos efectos visuales mencionados en el párrafo anterior.\nFinalmente, se discuten posibles aplicaciones de los conceptos aprendidos, así como el trabajo futuro que se puede llevar a cabo tomando el ejercicio desarrollado como punto de partida.\nMarco Teórico\u003e Marco Teórico # Las bandas de Mach (Mach bands, en inglés) son un fenómeno óptico que se produce cuando hay un cambio gradual en la intensidad de la luz en una imagen. El ojo humano tiende a percibir estas transiciones como bordes o líneas más definidas de lo que realmente son [1]; esto se debe a la forma en que el cerebro procesa la información visual.\nPor otro lado, la generación de terreno con Perlin noise es un método para crear terrenos y superficies naturales de forma procedural. Este método utiliza una función de ruido desarrollada por Ken Perlin, en la década de 1980, que genera valores de ruido aleatorios pero coherentes y suaves [6].\nLa relación entre los Mach bands y la generación de terreno con Perlin noise es que se pueden utilizar las bandas de Mach para mejorar la apariencia visual de los terrenos generados con este método. Al aplicar una función de suavizado a los valores de ruido generados por Perlin noise, se pueden crear transiciones suaves entre las diferentes alturas del terreno [4]. Sin embargo, estas transiciones suaves pueden producir una apariencia visual demasiado uniforme o artificial. Por consiguiente, al aplicar una función que aumente los valores de intensidad cerca de las transiciones, se pueden crear efectos visuales similares a los de las bandas de Mach, que ayudan a definir mejor los bordes y las transiciones del terreno [5].\nEn resumen, las bandas de Mach se utilizan en la generación de terreno con Perlin noise como una técnica para mejorar la apariencia visual de los terrenos, creando efectos visuales que ayudan a definir mejor los bordes y las transiciones.\nCódigo y Resultados\u003e Código y Resultados # A continuación, se describe el ejercicio desarrollado y se destacan las piezas de código más relevantes, las cuales demuestran los aportes efectuados sobre el ejemplo disponible.\nComo se puede evidenciar, el ejercicio desarrollado consiste en una animación que genera de manera aleatoria y continua un terreno, al cual se le pueden modificar los valores de altitud máxima que puede llegar a tomar cada punto. Entrando un poco más en detalle sobre cómo se realizó el ejercicio, es importante aclarar que para generar el terreno se utilizó una grilla en 3 dimensiones (x, y, z) con valores predeterminados para los puntos x y y.\nEn primer lugar, se realiza una rotación sobre el eje x y se le asigna un valor aleatorio a la coordenada z entre un rango preestablecido. Acá es donde entra el método de Perlin Noise: de manera simplificada, con este método es poisble asegurar que cada valor generado \u0026ldquo;aleatoriamente\u0026rdquo; para la coordenada z, de cada punto de la grilla, únicamnete diverja dentro de un rango preestablecido y, a su vez, que este rango sea dependiente de los valores en z que se encuntren alrededor del punto que se está generando [7].\nEn la siguiente porción de codigo se resalta cómo se generan los valores en z para cada par de coordenadas (x, y) de la grilla y, también, cómo se asegura que estos valores sean acordes al método de Perlin noise.\nfunction draw() { fly -= 0.1 var yoff = fly for(var j=0; j\u0026lt;rows; j++){ var xoff = 0 for(var i=0; i\u0026lt;cols; i++){ terrein[i][j] = map(noise(xoff,yoff), 0, 1, -100, slider.value()) xoff += 0.1 } yoff += 0.1 } background(10); stroke(225) translate(0, 0) rotateX(PI/3) translate(-w/2, -h/2) for(var j=0; j\u0026lt;rows-1; j++){ beginShape(TRIANGLE_STRIP) for(var i=0; i\u0026lt;cols; i++){ vertex(i*scl, j*scl, terrein[i][j]) vertex(i*scl, (j+1)*scl, terrein[i][j+1]) colors = colorChange(terrein[i][j]) fill(colors) } endShape() } } El código completo se encuentra en el siguiente desplegable:\nCódigo completo let cols, rows; let scl = 20 let w = 1200 let h = 900 let terrein var fly = 0 let colors function setup() { createCanvas(800, 650, WEBGL); cols = w/scl rows = h/scl terrein = Array(cols) for(var x = 0; x\u0026lt;terrein.length; x++){ terrein[x] = new Array(rows) } slider = createSlider(0, 300, 100); slider.position(width/3, 30); slider.style(\u0026#39;width\u0026#39;, \u0026#39;300px\u0026#39;); } function draw() { fly -= 0.1 var yoff = fly for(var j=0; j\u0026lt;rows; j++){ var xoff = 0 for(var i=0; i\u0026lt;cols; i++){ terrein[i][j] = map(noise(xoff,yoff), 0, 1, -100, slider.value()) xoff += 0.1 } yoff += 0.1 } background(10); stroke(225) translate(0, 0) rotateX(PI/3) translate(-w/2, -h/2) for(var j=0; j\u0026lt;rows-1; j++){ beginShape(TRIANGLE_STRIP) for(var i=0; i\u0026lt;cols; i++){ vertex(i*scl, j*scl, terrein[i][j]) vertex(i*scl, (j+1)*scl, terrein[i][j+1]) colors = colorChange(terrein[i][j]) fill(colors) } endShape() } } function colorChange(num){ if(num\u0026gt;=120){ return(255) }else if(num\u0026gt;=100){ return(200) }else if(num\u0026gt;=40){ return([128,64 ,0]) } else if(num\u0026gt;=10){ return([0,150,0]) } else{ return([0,0,150]) } } Conclusiones y Trabajo futuro\u003e Conclusiones y Trabajo futuro # El uso de la libería P5js permite una aplicación más fácil y práctica de los conceptos de Periln Noise y Mach bands, pues estos ya están integrados dentro de la libreria, además de brindar una amplia documentación y guía de cómo usarlos. Así mismo, el uso de la librería P5js facilita considerablemente el trabajo y manejo de objetos en tres dimensiones: brinda una amplia variedad de herramientas para la manipulación de dichos elementos y, además, resulta ser precisa para el desarrollo de esta actividad, pues permite realizar distintas acciones para cada caso, como la manipulacion de los ejes x, y o z, entre otras. Para un trabajo futuro, se podría implementar una función que le permita al usuario girar la vista en modo 360° o que le permita jugar con los ejes de la grilla, de modo que pueda visualizar la generación automática del terreno desde diferentes perspectivas. Así mismo, se podria integrar como trabajo fúturo la incorporación de una función que le permita al usuario jugar con los valores preestablecidos para el método de Pearling Noise en tiempo de ejecución, es decir, aquellos valores que se generan \u0026ldquo;aleatoriamente\u0026rdquo; en z para cada par de coordenadas x, y. Referencias\u003e Referencias # [1] F. Ratliff, \u0026ldquo;Mach bands: quantitative studies on neural networks in the retina\u0026rdquo;, Holden-Day, 1965, ISBN 9780816270453.\u003e [1] F. Ratliff, \u0026ldquo;Mach bands: quantitative studies on neural networks in the retina\u0026rdquo;, Holden-Day, 1965, ISBN 9780816270453. # [2] G. von Békésy, \u0026ldquo;Mach Band Type Lateral Inhibition in Different Sense Organs\u0026rdquo;, PDF, 1967.\u003e [2] G. von Békésy, \u0026ldquo;Mach Band Type Lateral Inhibition in Different Sense Organs\u0026rdquo;, PDF, 1967. # [3] F.A.A. Kingdom, \u0026ldquo;Mach bands explained by response normalization\u0026rdquo;, Frontiers in Human Neuroscience, vol. 8, pp. 843, Nov. 2014, doi: 10.3389/fnhum.2014.00843, ISSN 1662-5161, PMC 4219435, PMID 25408643.\u003e [3] F.A.A. Kingdom, \u0026ldquo;Mach bands explained by response normalization\u0026rdquo;, Frontiers in Human Neuroscience, vol. 8, pp. 843, Nov. 2014, doi: 10.3389/fnhum.2014.00843, ISSN 1662-5161, PMC 4219435, PMID 25408643. # [4] P. Ambalathankandy et al, \u0026ldquo;Radiography Contrast Enhancement: Smoothed LHE Filter a Practical Solution for Digital X-Rays with Mach Band\u0026rdquo;, Digital Image Computing: Techniques and Applications, 2019.\u003e [4] P. Ambalathankandy et al, \u0026ldquo;Radiography Contrast Enhancement: Smoothed LHE Filter a Practical Solution for Digital X-Rays with Mach Band\u0026rdquo;, Digital Image Computing: Techniques and Applications, 2019. # [5] C.J. Nielsen, \u0026ldquo;Effect of Scenario and Experience on Interpretation of Mach Bands\u0026rdquo;, Journal of Endodontics, vol. 27, no. 11, pp. 687-691.\u003e [5] C.J. Nielsen, \u0026ldquo;Effect of Scenario and Experience on Interpretation of Mach Bands\u0026rdquo;, Journal of Endodontics, vol. 27, no. 11, pp. 687-691. # [6] R. Touti, \u0026ldquo;Perlin Noise Algorithm\u0026rdquo;, [Online]. Available: https://rtouti.github.io/graphics/perlin-noise-algorithm.\u003e [6] R. Touti, \u0026ldquo;Perlin Noise Algorithm\u0026rdquo;, [Online]. Available: https://rtouti.github.io/graphics/perlin-noise-algorithm. # [7] D. Shiffman, \u0026ldquo;What is a Pixel?\u0026rdquo; [Online]. Available: https://www.youtube.com/watch?v=IKB1hWWedMk\u0026t=1s. [Accessed: Mar. 31, 2022].\u003e [7] D. Shiffman, \u0026ldquo;What is a Pixel?\u0026rdquo; [Online]. Available: https://www.youtube.com/watch?v=IKB1hWWedMk\u0026t=1s. [Accessed: Mar. 31, 2022]. # ","date":"1 January 0001","permalink":"/showcase/first/terrain/","section":"Firsts","summary":"Mach Bands\u003e Mach Bands # Introducción\u003e Introducción # Para este ejercicio se definen los efectos visuales de \u0026ldquo;Mach bands\u0026rdquo; y Perlin noise, así como la relación que guardan al momento de generar un terreno automatizado y continuo.","title":""},{"content":"Coloring\u003e Coloring # Introducción\u003e Introducción # En esta sección se abordará el problema de color blending, también conocido como mezcla de colores, que se refiere al proceso de combinar diferentes colores para crear nuevos tonos y matices.\nMarco Teórico\u003e Marco Teórico # En el contexto del arte, el color blending se puede realizar mezclando pinturas en el lienzo o utilizando técnicas de mezcla digital en programas de diseño. Al combinar colores primarios o secundarios en diferentes proporciones, se pueden obtener una amplia variedad de colores intermedios. Los artistas suelen utilizar técnicas de mezcla, como el difuminado o el degradado, para suavizar las transiciones entre colores y lograr efectos de sombreado o iluminación.\nEn el diseño gráfico y la fotografía, el color blending se realiza a menudo mediante el uso de herramientas y software específicos. Los programas de edición de imágenes permiten mezclar colores de forma precisa y controlada, lo que resulta útil para retocar fotografías, crear ilustraciones o generar efectos especiales.\nAlgunos modos de mezcla, los cuales se implementan más adelante, son los siguientes:\nMultiply (Multiplicar): Este modo de mezcla multiplica los valores de los píxeles de la capa superior con los valores de los píxeles de la capa inferior. El resultado es un color más oscuro y con mayor saturación. Es útil para oscurecer y dar profundidad a las imágenes, especialmente cuando se superponen capas con colores más oscuros.\nAdd (Sumar): En este modo de mezcla, los valores de los píxeles de la capa superior se suman a los valores de los píxeles de la capa inferior. El resultado es una imagen más clara y con mayor brillo. Se utiliza para crear efectos de luz y resplandor, ya que los colores más claros se suman entre sí para generar áreas más brillantes.\nDarkest (Más oscuro): En este modo de mezcla, se selecciona el color más oscuro entre los píxeles de la capa superior y la capa inferior para formar la imagen resultante. Es útil para resaltar los detalles más oscuros de una imagen y crear efectos de contraste.\nLightest (Más claro): En contraste con el modo \u0026ldquo;Darkest\u0026rdquo;, el modo \u0026ldquo;Lightest\u0026rdquo; selecciona el color más claro entre los píxeles de la capa superior y la capa inferior. Esto resulta en una imagen en la que se destacan los elementos más claros y se puede utilizar para realzar áreas iluminadas o generar efectos de luz intensa.\nDifference (Diferencia): En este modo de mezcla, se calcula la diferencia entre los valores de los píxeles de la capa superior y la capa inferior. El resultado es una imagen en la que los colores similares entre las capas se anulan, mientras que los colores diferentes se acentúan. Se utiliza para crear efectos de contraste y resaltar las áreas donde las capas difieren significativamente en color.\nScreen (Pantalla): En este modo de mezcla, los valores de los píxeles de la capa superior se invierten y luego se multiplican con los valores de los píxeles de la capa inferior. El resultado es una imagen más clara y con mayor iluminación. Se utiliza para añadir brillo y suavizar las transiciones entre capas.\nCódigo y Resultados\u003e Código y Resultados # A continuación se presenta el ejercicio desarrollado, en el cual se realiza la mezcla entre dos colores y también se aplica una modulación multiplicando el color resultante por un valor de brillo (brightness).\nPuede dar clic sobre los selectores de la parte superior para cambiar los colores a mezclar. Puede elegir, entre la lista desplegable, cuál de los modos enunciados con anterioridad desea aplicar. Por último, puede deslizar la barra para elegir el nivel de brillo que desea aplicar al nuevo color. El resultado se muestra en la parte inferior.\nA continuación se muestra el shader implementado para esta aplicación:\ncolorBlending.frag precision mediump float; uniform float brightness; uniform vec4 uMaterial1; uniform vec4 uMaterial2; uniform int blendMode; uniform vec4 identity; void main() { if (blendMode == 0){ vec4 material = uMaterial1 * uMaterial2; gl_FragColor = vec4(brightness * material.rgb, material.a); } else if (blendMode == 1){ vec4 material = uMaterial1 + uMaterial2; gl_FragColor = vec4(brightness * material.rgb, material.a); } else if (blendMode == 2){ vec4 material = min(uMaterial1, uMaterial2); gl_FragColor = vec4(brightness * material.rgb, material.a); } else if (blendMode == 3){ vec4 material = max(uMaterial1, uMaterial2); gl_FragColor = vec4(brightness * material.rgb, material.a); } else if (blendMode == 4){ vec4 material = max(uMaterial1, uMaterial2) - min(uMaterial1, uMaterial2); gl_FragColor = vec4(brightness * material.rgb, material.a); } else if (blendMode == 5){ vec4 material = identity - ((identity - uMaterial1) * (identity - uMaterial2)); gl_FragColor = vec4(brightness * material.rgb, material.a); } } Se evidencia que para cada modo de mezcla se aplica una operación diferente. Por ejemplo, mientras que el modo multiply (el primero) corresponde a mutiplicar ab, el modo screen (el último) corresponde a una operación más compleja: 1 - (1-a)(1-b). El shader recibe en uMaterial1 y en uMaterial2 los colores a mezclar, en blendMode un entero asociado al tipo de mezcla de la lista de selección, y en identity un vector que representa el valor \u0026ldquo;1\u0026rdquo; para la operación del modo screen que se enunció hace un momento.\nEn el código de JavaScript, primero se crean los elementos html y, luego, para dibujar los colores originales es importante tener en cuenta que estos pasan por el mismo shader que el color resultante de la mezcla. Por ello, en la parte inferior se define una función que, además de asociar el modo de mezcla elegido con su entero análogo en el shader, también retorna un vector, el cual, al ser operado con un color, lo deja invariable en el modo correspondiente. Por ejemplo, en multiply es [1.0, 1.0, 1.0, 1.0] porque al multiplicar los pixeles por 1, el resultado no cambia. Por su parte, en add es [0.0, 0.0, 0.0, 0.0] porque al sumar los pixeles con 0, el resultado no varía. Se sigue el mismo análisis para los modos restantes y, finalmente, se hace uso de la función vertex para pintar todos los colores, tanto los originales como el resultante. Para ello, se jugó con las coordenadas hasta dar con una posición que resultase cómoda. El código completo se deja a continuación:\nCódigo completo let blendingShader; let brightness; let color1, color2, picker1, picker2; let blendingSelect; let mode = 0, identity = [1.0, 1.0, 1.0, 1.0]; function preload() { blendingShader = readShader(\u0026#39;/showcase/sketches/frags/colorBlending.frag\u0026#39;); } function setup() { createCanvas(550, 550, WEBGL); colorMode(RGB, 1); noStroke(); picker1 = createColorPicker(color(\u0026#39;#cc804d\u0026#39;)); picker1.position(30, 30); picker2 = createColorPicker(color(\u0026#39;#e61a66\u0026#39;)); picker2.position(width / 2 + 30, 30); brightness = createSlider(0, 1, 1, 0.01); brightness.position(width / 2 + 40, height / 2); brightness.style(\u0026#39;width\u0026#39;, \u0026#39;100px\u0026#39;); shader(blendingShader); blendingSelect = createSelect(); blendingSelect.position(150, height / 2); blendingSelect.option(\u0026#39;MULTIPLY\u0026#39;); blendingSelect.option(\u0026#39;ADD\u0026#39;); blendingSelect.option(\u0026#39;DARKEST\u0026#39;); blendingSelect.option(\u0026#39;LIGHTEST\u0026#39;); blendingSelect.option(\u0026#39;DIFFERENCE\u0026#39;); blendingSelect.option(\u0026#39;SCREEN\u0026#39;); blendingSelect.changed(() =\u0026gt; { [mode, identity] = blendingMode(blendingSelect.value()); blendingShader.setUniform(\u0026#34;blendMode\u0026#34;, mode) }); } function draw() { color1 = picker1.color(); color2 = picker2.color(); background(0); blendingShader.setUniform(\u0026#39;uMaterial2\u0026#39;, identity); blendingShader.setUniform(\u0026#39;uMaterial1\u0026#39;, [red(color1), green(color1), blue(color1), 1.0]); blendingShader.setUniform(\u0026#39;brightness\u0026#39;, 1.0); beginShape(); vertex(-0.85, 0.15, 0); vertex(-0.15, 0.15, 0); vertex(-0.15, 0.85, 0); vertex(-0.85, 0.85, 0); endShape(); blendingShader.setUniform(\u0026#39;uMaterial1\u0026#39;, identity); blendingShader.setUniform(\u0026#39;uMaterial2\u0026#39;, [red(color2), green(color2), blue(color2), 1.0]); blendingShader.setUniform(\u0026#39;brightness\u0026#39;, 1.0); beginShape(); vertex(0.15, 0.15, 0); vertex(0.85, 0.15, 0); vertex(0.85, 0.85, 0); vertex(0.15, 0.85, 0); endShape(); blendingShader.setUniform(\u0026#39;uMaterial1\u0026#39;, [red(color1), green(color1), blue(color1), 1.0]); blendingShader.setUniform(\u0026#39;uMaterial2\u0026#39;, [red(color2), green(color2), blue(color2), 1.0]); blendingShader.setUniform(\u0026#39;brightness\u0026#39;, brightness.value()); blendingShader.setUniform(\u0026#39;identity\u0026#39;, [1.0, 1.0, 1.0, 1.0]); beginShape(); vertex(-0.35, -0.85, 0); vertex(0.35, -0.85, 0); vertex(0.35, -0.15, 0); vertex(-0.35, -0.15, 0); endShape(); } function blendingMode(mode){ if (mode === \u0026#39;MULTIPLY\u0026#39;){ return [0, [1.0, 1.0, 1.0, 1.0]] } else if (mode === \u0026#39;ADD\u0026#39;){ return [1, [0.0, 0.0, 0.0, 0.0]] } else if (mode === \u0026#39;DARKEST\u0026#39;){ return [2, [1.0, 1.0, 1.0, 1.0]] } else if (mode === \u0026#39;LIGHTEST\u0026#39;){ return [3, [0.0, 0.0, 0.0, 0.0]] } else if (mode === \u0026#39;DIFFERENCE\u0026#39;){ return [4, [0.0, 0.0, 0.0, 0.0]] } else if (mode === \u0026#39;SCREEN\u0026#39;){ return [5, [0.0, 0.0, 0.0, 0.0]] } } Conclusiones y Trabajo futuro\u003e Conclusiones y Trabajo futuro # En síntesis, se puede evidenciar que el uso de shaders permite mezclar colores de manera ágil, pues solo basta con realizar una operación entre los pixeles para obtener el tono resultante, el cual también puede variar a partir de un cierto brillo. Como trabajo futuro, se podrían implementar muchos más modos de mezcla de colores, incluyendo aquellos que tienen un mayor nivel de complejidad, tales como hard_light o soft_light. Así mismo, se podrían evaluar otros valores que sean variables además de brightness, tales como la luminosidad o la saturación. Referencias\u003e Referencias # [1] J. P. Charalambos, \u0026ldquo;Shaders: Coloring\u0026rdquo;. Visual Computing, Apr. 2023. https://visualcomputing.github.io/docs/shaders/coloring/\u003e [1] J. P. Charalambos, \u0026ldquo;Shaders: Coloring\u0026rdquo;. Visual Computing, Apr. 2023. https://visualcomputing.github.io/docs/shaders/coloring/ # [2] MA, Yunpeng. The mathematic magic of Photoshop blend modes for image processing. En 2011 International Conference on Multimedia Technology. IEEE, 2011. p. 5159-5161.\u003e [2] MA, Yunpeng. The mathematic magic of Photoshop blend modes for image processing. En 2011 International Conference on Multimedia Technology. IEEE, 2011. p. 5159-5161. # [3] CHUANG, Johnson; WEISKOPF, Daniel; MOLLER, Torsten. Hue-preserving color blending. IEEE Transactions on Visualization and Computer Graphics, 2009, vol. 15, no 6, p. 1275-1282.\u003e [3] CHUANG, Johnson; WEISKOPF, Daniel; MOLLER, Torsten. Hue-preserving color blending. IEEE Transactions on Visualization and Computer Graphics, 2009, vol. 15, no 6, p. 1275-1282. # [4] VALENTINE, Scott. The hidden power of blend modes in Adobe Photoshop. Adobe Press, 2012.\u003e [4] VALENTINE, Scott. The hidden power of blend modes in Adobe Photoshop. Adobe Press, 2012. # ","date":"1 January 0001","permalink":"/showcase/second/coloring/","section":"Seconds","summary":"Coloring\u003e Coloring # Introducción\u003e Introducción # En esta sección se abordará el problema de color blending, también conocido como mezcla de colores, que se refiere al proceso de combinar diferentes colores para crear nuevos tonos y matices.","title":""},{"content":"Image Processing \u0026amp; Post Effects\u003e Image Processing \u0026amp; Post Effects # Introducción\u003e Introducción # En esta sección se aborda el procesamiento de imágenes digitales, referido a las las máscaras de convolución. Acto seguido, se desarrolla una aplicación que abarca algunos de los kernels más comunes: identity, edge detection, sharpen, emboss, sobel, gaussian blur, unsharp masking. Adicionalmente, se lleva a cabo la implementación de una región de interés, lupa y herramientas de coloring brightness mediante postefectos.\nPara terminar, se discuten posibles aplicaciones de los conceptos aprendidos, así como el trabajo futuro que se puede desarrollar en este ámbito.\nMarco Teórico\u003e Marco Teórico # Máscaras de convolución: Para entender el concepto, en primer lugar es importante comprender qué es una convolución. De acuerdo con [1], esta se puede describir, intuitivamente, como una función que es la integral o suma de dos funciones componentes y que mide la cantidad de superposición a medida que una se desplaza sobre la otra. Formalmente, una convolución está definida de la siguiente manera [2]:\n\\[ g(x,y) = \\omega * f(x,y) = \\sum_{dx=-a}^{a}\\sum_{dy=-b}^{b} \\omega(dx,dy) f(x-dx,y-dy) \\] Ahora, una máscara de convolución es una matriz o kernel numérico utilizado en procesamiento de imágenes y visión por computadora para realizar operaciones de convolución en una imagen. Estas matrices, típicamente son de tamaño 3 x 3, aunque también se pueden emplear matrices de tamaño 2 x 2 y 5 x 5, entre otras. En líneas generales, en procedimiento consiste en que, a partir de una imagen, se van tomando bloques que posean el mismo tamaño de la matriz. Luego, aplicando la operación descrita (convolución) en función de los valores de los píxeles originales y los coeficientes de la máscara, se calcula el valor de cada píxel de la imagen resultante [1]. Esta tendrá ciertas características dependiendo del kernel que haya sido aplicado. Algunos de los más comunes (y los cuales fueron implementados en este ejercicio) se listan a continuación [2] y [3]:\nIdentity: \\[ \\begin{bmatrix} 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \\\\ \\end{bmatrix} \\] Edge detection: \\[ \\begin{bmatrix} -1 \u0026amp; -1 \u0026amp; -1 \\\\ -1 \u0026amp; 8 \u0026amp; -1 \\\\ -1 \u0026amp; -1 \u0026amp; -1 \\\\ \\end{bmatrix} \\] Sharpen: \\[ \\begin{bmatrix} 0 \u0026amp; -1 \u0026amp; 0 \\\\ -1 \u0026amp; 5 \u0026amp; -1 \\\\ 0 \u0026amp; -1 \u0026amp; 0 \\\\ \\end{bmatrix} \\] Emboss: \\[ \\begin{bmatrix} -2 \u0026amp; -1 \u0026amp; 0 \\\\ -1 \u0026amp; 1 \u0026amp; 1 \\\\ 0 \u0026amp; 1 \u0026amp; 2 \\\\ \\end{bmatrix} \\] Sobel: \\[ \\begin{bmatrix} 1 \u0026amp; 2 \u0026amp; 1 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \\\\ -1 \u0026amp; -2 \u0026amp; -1 \\\\ \\end{bmatrix} \\] Gaussian Blur (5x5): \\[ \\frac{1}{256} \\begin{bmatrix} 1 \u0026amp; 4 \u0026amp; 6 \u0026amp; 4 \u0026amp; 1 \\\\ 4 \u0026amp; 16 \u0026amp; 24 \u0026amp; 16 \u0026amp; 4 \\\\ 6 \u0026amp; 24 \u0026amp; 36 \u0026amp; 24 \u0026amp; 6 \\\\ 4 \u0026amp; 16 \u0026amp; 24 \u0026amp; 16 \u0026amp; 4 \\\\ 1 \u0026amp; 4 \u0026amp; 6 \u0026amp; 4 \u0026amp; 1 \\\\ \\end{bmatrix} \\] Unsharp Masking (5x5): \\[ \\frac{-1}{256} \\begin{bmatrix} 1 \u0026amp; 4 \u0026amp; 6 \u0026amp; 4 \u0026amp; 1 \\\\ 4 \u0026amp; 16 \u0026amp; 24 \u0026amp; 16 \u0026amp; 4 \\\\ 6 \u0026amp; 24 \u0026amp; -476 \u0026amp; 24 \u0026amp; 6 \\\\ 4 \u0026amp; 16 \u0026amp; 24 \u0026amp; 16 \u0026amp; 4 \\\\ 1 \u0026amp; 4 \u0026amp; 6 \u0026amp; 4 \u0026amp; 1 \\\\ \\end{bmatrix} \\] Región de interés: En el procesamiento de imágenes digitales, una región de interés (ROI, por sus siglas en inglés) se refiere a una parte específica de una imagen que es seleccionada para ser analizada, procesada o resaltada en función de un objetivo particular. Una ROI es una subregión dentro de una imagen más grande que se considera relevante o de interés para el análisis o aplicación en cuestión.\nLa selección de una región de interés puede ser realizada de forma manual, mediante la interacción del usuario que delimita el área de interés utilizando herramientas como rectángulos de selección, trazado a mano alzada, o mediante técnicas de segmentación automática en función de propiedades específicas de la imagen, como el color, la textura, el contraste, la forma, entre otros.\nUna vez definida la región de interés, se pueden aplicar diversas técnicas de procesamiento de imágenes para extraer características, medir propiedades, aplicar filtros o realizar análisis específicos en esa región en particular. Por ejemplo, se puede realizar un seguimiento de movimiento en una ROI, calcular estadísticas sobre esa región, realizar un recorte para obtener solo esa parte de la imagen, aplicar técnicas de reconocimiento de objetos, entre otros.\nLas regiones de interés son especialmente útiles cuando se trabaja con imágenes grandes o complejas, ya que permiten focalizar el procesamiento o el análisis en áreas específicas de interés, evitando el procesamiento innecesario de la imagen completa y mejorando la eficiencia computacional.\nEn resumen, una región de interés (ROI) en el procesamiento de imágenes digitales es una parte seleccionada o delimitada de una imagen que se considera relevante para un análisis, procesamiento o aplicación específica. Permite enfocar la atención y los cálculos en áreas de interés específicas en lugar de procesar la imagen completa.\nMagnificador: Un magnificador es una herramienta o dispositivo que se utiliza para aumentar el tamaño o la visualización de objetos pequeños o detalles finos. Su función principal es proporcionar una ampliación óptica para facilitar la observación y el estudio de elementos que no son fácilmente visibles a simple vista.\nExisten diferentes tipos de magnificadores, cada uno con sus características y usos específicos; en este caso se implementa una lupa, la cual es el tipo más común de magnificador y generalmente consiste en una lente convexa montada en un mango o marco. Las lupas pueden tener diferentes grados de aumento y se utilizan para examinar detalles pequeños, como texto, imágenes o componentes electrónicos.\nPost Effects: Los postefectos (posteffects en inglés), también conocidos como efectos posteriores o efectos de posprocesamiento, son técnicas utilizadas en gráficos por computadora para aplicar efectos visuales a una imagen o una escena después de que han sido renderizadas.\nEstos efectos se aplican en la etapa final del proceso de renderizado y se utilizan para mejorar la calidad visual, añadir realismo, crear estilos artísticos o modificar la apariencia estética de una imagen o una escena.\nLos postefectos se aplican mediante algoritmos y técnicas de procesamiento de imágenes y pueden incluir diferentes tipos de efectos, como los siguientes:\nEfectos de color y tonalidad: Se utilizan para ajustar el color, el contraste, la saturación o el balance de blancos de una imagen. También pueden incluir efectos de tonemapping para ajustar la representación tonal de una escena.\nEfectos de desenfoque y enfoque: Estos efectos permiten simular la profundidad de campo, el desenfoque de movimiento, el desenfoque gaussiano u otros tipos de desenfoque para agregar realismo o resaltar elementos específicos de una imagen.\nEfectos de iluminación y sombreado: Se utilizan para simular la iluminación global, las sombras suaves, los reflejos o la refracción de la luz. Estos efectos pueden mejorar la apariencia y el realismo de una escena.\nEfectos de partículas y partículas de postprocesamiento: Estos efectos permiten agregar elementos como humo, fuego, niebla o chispas a una imagen o una escena. También se pueden utilizar para aplicar efectos de movimiento o de distorsión a la imagen.\nEfectos de profundidad y efectos de contorno: Estos efectos permiten resaltar los contornos de los objetos, aplicar sombreado de borde o agregar efectos de profundidad basados en la distancia o la posición del objeto en la escena.\nLos postefectos se aplican generalmente mediante el uso de shaders o filtros de imagen que se aplican a la imagen final o a los píxeles de la escena renderizada. En este caso, se utilizan para implementar una región de interés, un magnificador y herramientas de coloring brigthness. Estas últimas fueron cubiertas a completitud en una sección anterior, así que no se volverá a ahondar en la teoría.\nCódigo y Resultados:\u003e Código y Resultados: # A continuación se encuentra la aplicación construida. Para desarrollarla se llevó un estudio a profundidad, pues el resultado tiene un alto nivel de complejidad. Cabe aclarar que para el correcto funcionamiento del código se debe autorizar a la página para que esta use la webcam\nEn primer lugar, se carga la imagen sin ningún tipo de alteración. Hay tres selectores: el primero (el de la izquierda) se usa para seleccionar la máscara de convolución que se desea aplicar. El segundo (el del medio) se emplea para definir si el usuario desea manejar un magnificador (o lupa), una región de interés respecto a la máscara de convolución, una región de interés respecto a las herramientas de coloring brightness, o ninguna. El tercero (el de la derecha) se emplea para elegir cuál herramienta desea aplicar entre luma, HSV, HSL, Component Average o ninguna.\nEs decir, puede seleccionar el kernel de detección de bordes, la opción luma y el magnificador: en ese caso podrá mirar de cerca los bordes de cerca, gracias a la lupa, con el filtro correspondiente. También podrá seleccionar, otro ejemplo, el kernel \u0026ldquo;sharpen\u0026rdquo;, la opción región de interés respecto al coloring brightness y HSV. En ese caso, el kernel se aplicará a toda la imagen, perp el HSV irá solo dentro de la región de interés. Si eligiera la región de interés respecto al coloring brightness, pasaría lo contrario: HSV se aplica a toda la imagen, pero la convolución solamente ocurriría en la zona circular.\nAhora, también hay dos sliders: el primero (el de la izquierda), sirve para hacer zoom a la lupa; el segundo, por su parte, sirve para ampliar el radio de la región de interés. Finalmente, a la derecha hay un checkbox que permite alternar entre una imagen o la cámara web de su computadora.\nA continuación se dejan los shaders implementados: mask.frag precision mediump float; uniform sampler2D texture; // see the emitTexOffset() treegl macro // https://github.com/VisualComputing/p5.treegl#macros uniform vec2 texOffset; // holds the 3x3 kernel uniform float mask[100]; uniform float customLen; // we need our interpolated tex coord varying vec2 texcoords2; uniform bool roi; uniform vec2 iResolution; uniform vec2 iMouse; uniform float lens_radius; void main() { // Sample texel neighbours within the rgba array vec4 rgba[100]; for (int i = 0; i \u0026lt; 100; i++) { if (float(i) == customLen){ break; } rgba[i] = texture2D(texture, texcoords2 + vec2(-texOffset.s*floor(sqrt(customLen)/2.0) + texOffset.s*mod(float(i),sqrt(customLen)), - texOffset.t*floor(sqrt(customLen)/2.0) + texOffset.t*floor(float(i)/sqrt(customLen)))); } // Apply convolution kernel vec4 convolution; for (int i = 0; i \u0026lt; 100; i++) { if (float(i) == customLen){ break; } convolution += rgba[i]*mask[i]; } if (roi){ vec2 uv = gl_FragCoord.xy / iResolution.y; // At the beginning of the sketch, center the magnifying glass. vec2 mouse = iMouse.xy; if (mouse == vec2(0.0)) mouse = iResolution.xy / 2.0; // UV coordinates of mouse vec2 mouse_uv = mouse / iResolution.y; // Distance to mouse float mouse_dist = distance(uv, mouse_uv); gl_FragColor = texture2D(texture, texcoords2); // Draw the outline of the glass if (mouse_dist \u0026lt; lens_radius + 0.01) gl_FragColor = vec4(1., 1., 1., 1.); // Draw a processed version of the texture if (mouse_dist \u0026lt; lens_radius) gl_FragColor = vec4(convolution.rgb, 1.0); } else { // Set color from convolution gl_FragColor = vec4(convolution.rgb, 1.0); } } Se observa que este shader hace uso de la macro texOffSet para lograr el desplazamiento a lo largo de los pixeles que resulta necesario para calcular la convolución. Por una parte, se tiene que glsl no recibe arreglos dinámicos, así que se instancia un array con una longitud suficiente (100) y se detiene el ciclo cuando se llega al valor enviado desde JavaScript (9 para el caso de las matrices 3x3, y 25 para las matrices 5x5).\nAdicionalmente a la aplicación de la convolución, se destaca el código para la región de interés, el cual fue inspirado en los ejemplos ofrecidos en shadertoy para implementar un magnificador. En esencia, se hace un mapeo uv sobre las coordenadas del mouse; inicialmente se traza la textura sin alteraciones y, luego, se aplica la convolución hasta una cierta distancia: ello logrará que se tomen todos los puntos equidistantes al mouse y, en consecuencia, la región sea circular. Se hace algo semejante, antes de trazar esa textura, para conseguir que la región tenga un borde blanco delgado.\nUna lógica semejante se emplea para el siguiente shader:\nlens.frag precision mediump float; uniform sampler2D texture; varying vec2 texcoords2; uniform vec2 iResolution; uniform vec2 iMouse; uniform float lens_radius; uniform float magnification; uniform bool roi; void main() { vec2 uv = gl_FragCoord.xy / iResolution.y; // At the beginning of the sketch, center the magnifying glass. vec2 mouse = iMouse.xy; if (mouse == vec2(0.0)) mouse = iResolution.xy / 2.0; // UV coordinates of mouse vec2 mouse_uv = mouse / iResolution.y; // Distance to mouse float mouse_dist = distance(uv, mouse_uv); // Draw the texture gl_FragColor = texture2D(texture, uv); if (!roi){ // Draw the outline of the glass if (mouse_dist \u0026lt; lens_radius + 0.01) gl_FragColor = vec4(1., 1., 1., 1.); // Draw a zoomed-in version of the texture if (mouse_dist \u0026lt; lens_radius) gl_FragColor = texture2D(texture, mouse_uv - (mouse_uv - texcoords2) / magnification); } } En este caso, aquí se lleva a cabo el magnificador: por ello, la textura aumentada se dibuja solo cuando el booleano roi es falso, pues quiere decir que no se está en ninguna de las dos opciones relacionadas con regiones de interés.\nOtra diferencia con respecto al anterior shader es el trazado de la textura dentro del lente: en el caso anterior se aplicaba la convolución, aquí se lleva a cabo una operación matemática que recalcula las coordenadas a partir del nivel de zoom que se recibe desde el slider (magnification).\nEste es el último shader: se puede ver que se aplica la misma lógica que los anteriores para establecer la región de interés y, por su parte, se aplica la misma lógica del shader de la sección \u0026ldquo;Texturing\u0026rdquo; para aplicar luma, HSV, HSL y Average.\nluma.frag precision mediump float; uniform int coloringBrightness; uniform sampler2D texture; varying vec2 texcoords2; uniform bool roi; uniform vec2 iResolution; uniform vec2 iMouse; uniform float lens_radius; float luma(vec3 texel) { return 0.299 * texel.r + 0.587 * texel.g + 0.114 * texel.b; } float hsv(vec3 texel) { return max(max(texel.r, texel.b), max(texel.r,texel.g)); } float hsl(vec3 texel){ float maxColor = max(max(texel.r, texel.g), texel.b); float minColor = min(min(texel.r, texel.g), texel.b); return (maxColor + minColor)/2.0; } float average(vec3 texel) { return (texel.r + texel.g + texel.b)/3.0; } void limited(vec4 texel){ if (roi){ vec2 uv = gl_FragCoord.xy / iResolution.y; // At the beginning of the sketch, center the magnifying glass. vec2 mouse = iMouse.xy; if (mouse == vec2(0.0)) mouse = iResolution.xy / 2.0; // UV coordinates of mouse vec2 mouse_uv = mouse / iResolution.y; // Distance to mouse float mouse_dist = distance(uv, mouse_uv); gl_FragColor = texture2D(texture, texcoords2); // Draw the outline of the glass if (mouse_dist \u0026lt; lens_radius + 0.01) gl_FragColor = vec4(1., 1., 1., 1.); // Draw a zoomed-in version of the texture if (mouse_dist \u0026lt; lens_radius) gl_FragColor = texel; } else { gl_FragColor = texel; } } void main() { vec4 texel = texture2D(texture, texcoords2); if (coloringBrightness == 1) { limited(vec4((vec3(luma(texel.rgb))), 1.0)); } else if (coloringBrightness == 2) { limited(vec4((vec3(hsv(texel.rgb))), 1.0)); } else if (coloringBrightness == 3) { limited(vec4((vec3(hsl(texel.rgb))), 1.0)); } else if (coloringBrightness == 4) { limited(vec4((vec3(average(texel.rgb))), 1.0)); } else { limited(texel); } } Con los shaders claros, es posible explicar la implementación en JavaScript. Dentro del código puede destacarse la siguiente sección, en la cual se bloquean los sliders dependiendo de la opción seleccionada: es decir, sólo se habilita el ajuste del nivel de zoom cuando se está empleando el magnificador, pero en cualquier otro caso se bloquea. El tamaño del radio del lente sólo se deshabilita si no está seleccionada ninguna de las herramientas.\nroi.input(() =\u0026gt; { if (roi.value() == \u0026#34;Magnifier\u0026#34;){ magnification.removeAttribute(\u0026#39;disabled\u0026#39;); } else { magnification.attribute(\u0026#39;disabled\u0026#39;, \u0026#39;\u0026#39;); } if (roi.value() == \u0026#34;None\u0026#34;){ lens_radius.attribute(\u0026#39;disabled\u0026#39;, \u0026#39;\u0026#39;); } else { lens_radius.removeAttribute(\u0026#39;disabled\u0026#39;); } }) Ahora, el uso de post effects para aplicar los múltiples shaders se evidencia en la función draw. Desde el inicio se crean las variables mask_pg, lens_pg y luma_pg. Se puede observar que se aplica el primer shader sobre la fuente (ya sea la imagen o la captura de video en vivo); luego, el segundo shader se aplica sobre la textura obtenida (es decir, sobre pg) y, finalmente, el tercer shader se aplica tras el segundo renderizado. Las coordenadas del cuadrilátero se modifican en este último para evitar que la imagen aparezca invertida, tal como se aprecia a continuación:\nfunction draw() { maskShader.setUniform(\u0026#39;texOffset\u0026#39;, [1 / src.width, 1 / src.height]) maskShader.setUniform(\u0026#39;mask\u0026#39;, masking()); maskShader.setUniform(\u0026#39;customLen\u0026#39;, masking().length); mask_pg.emitResolution(maskShader, \u0026#39;iResolution\u0026#39;); mask_pg.emitPointerPosition(maskShader, mouseX, height - mouseY, \u0026#39;iMouse\u0026#39;); maskShader.setUniform(\u0026#39;roi\u0026#39;, roi.value() == \u0026#34;R.O.I: Convolution\u0026#34;); maskShader.setUniform(\u0026#39;texture\u0026#39;, src); pg = mask_pg; pg.quad(-1, -1, 1, -1, 1, 1, -1, 1) lens_pg.emitResolution(lensShader, \u0026#39;iResolution\u0026#39;); lens_pg.emitPointerPosition(lensShader, mouseX, mouseY, \u0026#39;iMouse\u0026#39;); lensShader.setUniform(\u0026#39;roi\u0026#39;, roi.value() != \u0026#34;Magnifier\u0026#34;); lensShader.setUniform(\u0026#39;texture\u0026#39;, pg) pg = lens_pg; pg.quad(-1, -1, 1, -1, 1, 1, -1, 1) luma_pg.emitResolution(lumaShader, \u0026#39;iResolution\u0026#39;); luma_pg.emitPointerPosition(lumaShader, mouseX, mouseY, \u0026#39;iMouse\u0026#39;); lumaShader.setUniform(\u0026#39;roi\u0026#39;, roi.value() == \u0026#34;R.O.I: Color Brightness\u0026#34;); lumaShader.setUniform(\u0026#39;texture\u0026#39;, pg); pg = luma_pg; pg.quad(-1, 1, 1, 1, 1, -1, -1, -1) image(pg, 0, 0) } En el próximo desplegable se anexa el código completo en JavaScript para su consulta.\nCódigo completo let maskShader, mask_pg, pg; let lensShader, lens_radius, magnification, lens_pg; let lumaShader, luma_pg; let img, vid, video_on, src; let roi; let menu, coloringBrightness; function preload() { maskShader = readShader(\u0026#39;/showcase/sketches/frags/mask.frag\u0026#39;, { varyings: Tree.texcoords2 }); lensShader = readShader(\u0026#39;/showcase/sketches/frags/lens.frag\u0026#39;, { varyings: Tree.texcoords2 }); lumaShader = readShader(\u0026#39;/showcase/sketches/frags/luma.frag\u0026#39;, { varyings: Tree.texcoords2 }); img = loadImage(\u0026#39;../../assets/shrek.png\u0026#39;); vid = createCapture(VIDEO); vid.hide(); src = img; } function setup() { createCanvas(550, 550); video_on = createCheckbox(\u0026#39;camera\u0026#39;, false); video_on.style(\u0026#39;color\u0026#39;, \u0026#39;white\u0026#39;); video_on.changed(() =\u0026gt; { src = video_on.checked() ? vid : img; }); video_on.position(480, 40); mask_pg = createGraphics(width, height, WEBGL); mask_pg.colorMode(RGB, 1); mask_pg.textureMode(NORMAL); mask_pg.shader(maskShader); luma_pg = createGraphics(width, height, WEBGL); luma_pg.colorMode(RGB, 1); luma_pg.textureMode(NORMAL); luma_pg.shader(lumaShader); lens_pg = createGraphics(width, height, WEBGL); lens_pg.colorMode(RGB, 1); lens_pg.textureMode(NORMAL); lens_pg.shader(lensShader); menu = createSelect(); menu.position(10, 10); menu.style(\u0026#39;width\u0026#39;, \u0026#39;160px\u0026#39;); menu.option(\u0026#34;Identity\u0026#34;); menu.option(\u0026#34;Edge Detection\u0026#34;); menu.option(\u0026#34;Sharpen\u0026#34;); menu.option(\u0026#34;Emboss\u0026#34;); menu.option(\u0026#34;Sobel\u0026#34;); menu.option(\u0026#34;Gaussian Blur 5x5\u0026#34;); menu.option(\u0026#34;Unsharp Masking 5x5\u0026#34;); lens_radius = createSlider(0.1, 0.3, 0.15, 0.01); lens_radius.position(110, 40); lens_radius.style(\u0026#39;width\u0026#39;, \u0026#39;80px\u0026#39;); lens_radius.input(() =\u0026gt; { maskShader.setUniform(\u0026#39;lens_radius\u0026#39;, lens_radius.value()) lumaShader.setUniform(\u0026#39;lens_radius\u0026#39;, lens_radius.value()) lensShader.setUniform(\u0026#39;lens_radius\u0026#39;, lens_radius.value()) }); maskShader.setUniform(\u0026#39;lens_radius\u0026#39;, lens_radius.value()); lumaShader.setUniform(\u0026#39;lens_radius\u0026#39;, lens_radius.value()); lensShader.setUniform(\u0026#39;lens_radius\u0026#39;, lens_radius.value()) roi = createSelect(); roi.position(200, 10); roi.style(\u0026#39;width\u0026#39;, \u0026#39;160px\u0026#39;); roi.option(\u0026#34;None\u0026#34;); roi.option(\u0026#34;Magnifier\u0026#34;); roi.option(\u0026#34;R.O.I: Convolution\u0026#34;); roi.option(\u0026#34;R.O.I: Color Brightness\u0026#34;); magnification = createSlider(1, 8, 2, 0); magnification.position(10, 40); magnification.style(\u0026#39;width\u0026#39;, \u0026#39;80px\u0026#39;); magnification.input(() =\u0026gt; { lensShader.setUniform(\u0026#39;magnification\u0026#39;, magnification.value()) }); lensShader.setUniform(\u0026#39;magnification\u0026#39;, magnification.value()); magnification.attribute(\u0026#39;disabled\u0026#39;, \u0026#39;\u0026#39;); lens_radius.attribute(\u0026#39;disabled\u0026#39;, \u0026#39;\u0026#39;); roi.input(() =\u0026gt; { if (roi.value() == \u0026#34;Magnifier\u0026#34;){ magnification.removeAttribute(\u0026#39;disabled\u0026#39;); } else { magnification.attribute(\u0026#39;disabled\u0026#39;, \u0026#39;\u0026#39;); } if (roi.value() == \u0026#34;None\u0026#34;){ lens_radius.attribute(\u0026#39;disabled\u0026#39;, \u0026#39;\u0026#39;); } else { lens_radius.removeAttribute(\u0026#39;disabled\u0026#39;); } }) coloringBrightness = createSelect(); coloringBrightness.position(390, 10); coloringBrightness.style(\u0026#39;width\u0026#39;, \u0026#39;160px\u0026#39;); coloringBrightness.option(\u0026#39;None\u0026#39;); coloringBrightness.option(\u0026#39;Luma\u0026#39;); coloringBrightness.option(\u0026#39;HSV\u0026#39;); coloringBrightness.option(\u0026#39;HSL\u0026#39;); coloringBrightness.option(\u0026#39;Average\u0026#39;); coloringBrightness.changed(() =\u0026gt; { let val = coloringBrightness.value(); if (val === \u0026#39;Luma\u0026#39;) { lumaShader.setUniform(\u0026#39;coloringBrightness\u0026#39;, 1); } else if (val === \u0026#39;HSV\u0026#39;) { lumaShader.setUniform(\u0026#39;coloringBrightness\u0026#39;, 2); } else if (val === \u0026#39;HSL\u0026#39;) { lumaShader.setUniform(\u0026#39;coloringBrightness\u0026#39;, 3); } else if (val === \u0026#39;Average\u0026#39;) { lumaShader.setUniform(\u0026#39;coloringBrightness\u0026#39;, 4); } else { lumaShader.setUniform(\u0026#39;coloringBrightness\u0026#39;, 0); } }); lumaShader.setUniform(\u0026#39;coloringBrightness\u0026#39;, 0); } function draw() { maskShader.setUniform(\u0026#39;texOffset\u0026#39;, [1 / src.width, 1 / src.height]) maskShader.setUniform(\u0026#39;mask\u0026#39;, masking()); maskShader.setUniform(\u0026#39;customLen\u0026#39;, masking().length); mask_pg.emitResolution(maskShader, \u0026#39;iResolution\u0026#39;); mask_pg.emitPointerPosition(maskShader, mouseX, height - mouseY, \u0026#39;iMouse\u0026#39;); maskShader.setUniform(\u0026#39;roi\u0026#39;, roi.value() == \u0026#34;R.O.I: Convolution\u0026#34;); maskShader.setUniform(\u0026#39;texture\u0026#39;, src); pg = mask_pg; pg.quad(-1, -1, 1, -1, 1, 1, -1, 1) lens_pg.emitResolution(lensShader, \u0026#39;iResolution\u0026#39;); lens_pg.emitPointerPosition(lensShader, mouseX, mouseY, \u0026#39;iMouse\u0026#39;); lensShader.setUniform(\u0026#39;roi\u0026#39;, roi.value() != \u0026#34;Magnifier\u0026#34;); lensShader.setUniform(\u0026#39;texture\u0026#39;, pg) pg = lens_pg; pg.quad(-1, -1, 1, -1, 1, 1, -1, 1) luma_pg.emitResolution(lumaShader, \u0026#39;iResolution\u0026#39;); luma_pg.emitPointerPosition(lumaShader, mouseX, mouseY, \u0026#39;iMouse\u0026#39;); lumaShader.setUniform(\u0026#39;roi\u0026#39;, roi.value() == \u0026#34;R.O.I: Color Brightness\u0026#34;); lumaShader.setUniform(\u0026#39;texture\u0026#39;, pg); pg = luma_pg; pg.quad(-1, 1, 1, 1, 1, -1, -1, -1) image(pg, 0, 0) } function masking() { if (menu.value() == \u0026#34;Identity\u0026#34;){ return [ 0, 0, 0, 0, 1, 0, 0, 0, 0 ]; } else if (menu.value() == \u0026#34;Edge Detection\u0026#34;){ return [ -1, -1, -1, -1, 8, -1, -1, -1, -1 ]; } else if (menu.value() == \u0026#34;Sharpen\u0026#34;){ return [ 0, -1, 0, -1, 5, -1, 0, -1, 0 ]; } else if (menu.value() == \u0026#34;Emboss\u0026#34;){ return [ -2, -1, 0, -1, 1, 1, 0, 1, 2 ]; } else if (menu.value() == \u0026#34;Sobel\u0026#34;){ return [ 1, 2, 1, 0, 0, 0, -1, -2, -1 ]; } else if (menu.value() == \u0026#34;Gaussian Blur 5x5\u0026#34;){ return [ 1/256, 4/256, 6/256, 4/256, 1/256, 4/256, 16/256, 24/256, 16/256, 4/256, 6/256, 24/256, 36/256, 24/256, 6/256, 4/256, 16/256, 24/256, 16/256, 4/256, 1/256, 4/256, 6/256, 4/256, 1/256 ] } else if (menu.value() == \u0026#34;Unsharp Masking 5x5\u0026#34;){ return [ -1/256, -4/256, -6/256, -4/256, -1/256, -4/256, -16/256, -24/256, -16/256, -4/256, -6/256, -24/256, 476/256, -24/256, -6/256, -4/256, -16/256, -24/256, -16/256, -4/256, -1/256, -4/256, -6/256, -4/256, -1/256 ] } } Conclusiones y Trabajo futuro\u003e Conclusiones y Trabajo futuro # Se logra evidenciar que, mediante el uso de shaders, se reduce enormemente el consumo de recursos computacionales en las tareas relacionadas con el procesamiento de imagenes digitales respecto a la aplicación desarrollada en la entrega anterior. Aquí, incluso, se pueden implementar postefectos más allá de la mera convolución y no hay sobrecarga, como sí pasaba antes. De hecho, toda la ejecución se puede llevar a cabo en un video en vivo sin mayor complicación. Como trabajo futuro, se podría implementar esta aplicación en el procesamiento de imágenes dentro del ámbito de la medicina, por dar un ejemplo. En ese caso, se daría la opción de cargar imágenes o videos. También podrían mirarse más postefectos para su aplicación dentro y fuera de la región de interés, así como la integración de dos o más de ellos (por ejemplo, que el lente funcione como magnificador y detector de bordes al mismo tiempo). Referencias\u003e Referencias # [1] S. Kim and R. Casper, \u0026ldquo;Applications of convolution in image processing with MATLAB,\u0026rdquo; University of Washington, pp. 1-20, 2013. http://kiwi.bridgeport.edu/cpeg585/ConvolutionFiltersInMatlab.pdf\u003e [1] S. Kim and R. Casper, \u0026ldquo;Applications of convolution in image processing with MATLAB,\u0026rdquo; University of Washington, pp. 1-20, 2013. http://kiwi.bridgeport.edu/cpeg585/ConvolutionFiltersInMatlab.pdf # [2] J. P. Charalambos, \u0026ldquo;Visual masking\u0026rdquo;. Visual Computing, Feb. 2023. https://visualcomputing.github.io/docs/visual_illusions/masking/\u003e [2] J. P. Charalambos, \u0026ldquo;Visual masking\u0026rdquo;. Visual Computing, Feb. 2023. https://visualcomputing.github.io/docs/visual_illusions/masking/ # [3] S. Raveendran, P. J. Edavoor, N. K. Yernad Balachandra and V. Moodabettu Harishchandra, \u0026ldquo;Design and implementation of image kernels using reversible logic gates,\u0026rdquo; IET Image Processing, vol. 14, no 16, pp. 4110-4121, 2020, doi: 10.1049/iet-ipr.2019.1681.\u003e [3] S. Raveendran, P. J. Edavoor, N. K. Yernad Balachandra and V. Moodabettu Harishchandra, \u0026ldquo;Design and implementation of image kernels using reversible logic gates,\u0026rdquo; IET Image Processing, vol. 14, no 16, pp. 4110-4121, 2020, doi: 10.1049/iet-ipr.2019.1681. # [4] R. C. Gonzalez and R. E. Woods, \u0026ldquo;Digital Image Processing,\u0026rdquo; 4th ed., Pearson, 2018.\u003e [4] R. C. Gonzalez and R. E. Woods, \u0026ldquo;Digital Image Processing,\u0026rdquo; 4th ed., Pearson, 2018. # [5] J. P. Charalambos, \u0026ldquo;Image Processing\u0026rdquo;. Visual Computing, Apr. 2023. https://visualcomputing.github.io/docs/shaders/image_processing/\u003e [5] J. P. Charalambos, \u0026ldquo;Image Processing\u0026rdquo;. Visual Computing, Apr. 2023. https://visualcomputing.github.io/docs/shaders/image_processing/ # [6] J. P. Charalambos, \u0026ldquo;Posteffects\u0026rdquo;. Visual Computing, Apr. 2023. https://visualcomputing.github.io/docs/shaders/post_effects/\u003e [6] J. P. Charalambos, \u0026ldquo;Posteffects\u0026rdquo;. Visual Computing, Apr. 2023. https://visualcomputing.github.io/docs/shaders/post_effects/ # [7] DOPPIOSLASH, Claudia. Post-Processing Effects. Physically Based Shader Development for Unity 2017: Develop Custom Lighting Systems, 2018, p. 121-135.\u003e [7] DOPPIOSLASH, Claudia. Post-Processing Effects. Physically Based Shader Development for Unity 2017: Develop Custom Lighting Systems, 2018, p. 121-135. # [8] Shadertoy. Magnifier. https://www.shadertoy.com/results?query=magnifier\u003e [8] Shadertoy. Magnifier. https://www.shadertoy.com/results?query=magnifier # ","date":"1 January 0001","permalink":"/showcase/second/imageprocessing/","section":"Seconds","summary":"Image Processing \u0026amp; Post Effects\u003e Image Processing \u0026amp; Post Effects # Introducción\u003e Introducción # En esta sección se aborda el procesamiento de imágenes digitales, referido a las las máscaras de convolución.","title":""},{"content":"Spatial Coherence \u0026amp; Photomosaic\u003e Spatial Coherence \u0026amp; Photomosaic # Introducción\u003e Introducción # Para este ejercicio se puso en práctica el fenómeno visual de \u0026ldquo;Spatial Coherence\u0026rdquo; con el objetivo de desarrollar un código capaz de pixelar una imagen y, a partir de este, desarrollar una aplicación que consiste en un fotomosaico.\nMarco Teórico\u003e Marco Teórico # La coherencia espacial es un término que se utiliza en la óptica para describir la capacidad de una onda electromagnética para mantener una relación de fase estable en diferentes puntos del espacio. La coherencia espacial se refiere a la propiedad de la luz que hace que se comporte como una onda: esto significa que tiene la capacidad de interferir constructiva o destructivamente consigo misma en diferentes puntos del espacio [1].\nLa coherencia espacial se utiliza en muchas aplicaciones ópticas, como la holografía, la tomografía óptica de coherencia y la microscopía de campo cercano [3]. En estas aplicaciones, la coherencia espacial es esencial para la formación de imágenes y para obtener información detallada sobre la estructura de los objetos que se están observando.\nLa coherencia espacial se puede cuantificar utilizando varias medidas, tales como la función de correlación espacial, el ancho de banda de coherencia y la longitud de coherencia: la función de correlación espacial es una medida de la correlación entre dos puntos de la onda en diferentes posiciones, el ancho de banda de coherencia es una medida de la gama de longitudes de onda que contribuyen a la coherencia de la onda y la longitud de coherencia es una medida de la distancia sobre la cual la onda mantiene una relación de fase estable [1].\nEn resumen, la coherencia espacial es una propiedad fundamental de las ondas electromagnéticas que permite que se comporten como ondas y que puedan interferir constructiva o destructivamente en diferentes puntos del espacio. La coherencia espacial se utiliza en muchas aplicaciones ópticas para la formación de imágenes y para obtener información detallada sobre la estructura de los objetos que se están observando.\nAhora, un fotomosaico es una imagen o composición visual que se crea al combinar múltiples fotografías pequeñas, llamadas teselas, para formar una imagen más grande. Cada tesela individual es una fotografía completa en sí misma y puede ser una imagen independiente o una parte recortada de una imagen más grande.\nEl proceso de creación de un fotomosaico generalmente implica los siguientes pasos:\nRecopilación de imágenes: Se selecciona un conjunto de imágenes que se utilizarán como teselas para construir el fotomosaico. Estas imágenes pueden ser fotografías relacionadas con un tema específico o pueden abarcar una amplia variedad de temas.\nAnálisis y división: La imagen objetivo o imagen base que se desea recrear con el fotomosaico se divide en pequeñas secciones o celdas.\nAsignación de teselas: Cada celda de la imagen base se compara con las teselas disponibles y se selecciona la imagen más adecuada en función de la similitud de color, tono o características visuales.\nComposición: Las teselas seleccionadas se colocan en las celdas correspondientes de la imagen base, formando así el fotomosaico completo.\nEl resultado final es una imagen que, a distancia o vista en su totalidad, representa la imagen base, pero cuando se observa de cerca o se examina con atención, se revelan las múltiples imágenes que componen el mosaico.\nLos fotomosaicos pueden ser utilizados como una forma artística de representar imágenes o como una técnica de organización visual para presentar grandes conjuntos de imágenes de manera más compacta y creativa.\nCódigo y Resultados\u003e Código y Resultados # A continuación se describe el ejercicio desarrollado: se implementó un dataset consistente de 56 imágenes relacionadas con películas. El fotomosaico toma como fuente una de esas imágenes y la reconstruye a partir de las demás.\nEn la parte inferior se encuentra, a la izquierda, un checkbox que permite habilitar la visualización uv. En el centro hay otro checkbox que permite alternar entre el fotomosaico o el pixelador. Por último, a la derecha hay un slider que permite realizar el downsampling, es decir, aumentar el tamaño de las teselas (y, en consecuencia, disminuir el número de estas). El valor que se deja por defecto en el slider es el sugerido para visualizar los fotomosaicos con mejor calidad respecto a las demás resoluciones.\nPuede cambiar de una fotografía a otra presionando la tecla d para seguir y la tecla s para retroceder.\nPara esta aplicación se implementó un fragment shader, el cual se detalla a continuación:\nphotomosaic.frag // heavily influence by some discussions with Sebastian Chaparro // https://github.com/sechaparroc precision mediump float; // palette is sent by the sketch and comprises the video uniform sampler2D source; // palette is sent by the sketch and comprises the video uniform sampler2D palette; // target horizontal \u0026amp; vertical resolution uniform float resolution; // uv visualization uniform bool uv; // pixelator uniform bool pixelator; // target horizontal \u0026amp; vertical resolution uniform float pg_size; // texture space normalized interpolated texture coordinates // should have same name and type as in vertex shader varying vec2 texcoords2; // (defined in [0..1] ∈ R) float luma(vec3 texel) { return 0.299 * texel.r + 0.587 * texel.g + 0.114 * texel.b; } void main() { // i. define symbolCoord as a texcoords2 // remapping in [0.0, resolution] ∈ R vec2 symbolCoord = texcoords2 * resolution; // ii. define stepCoord as a symbolCoord // remapping in [0.0, resolution] ∈ Z vec2 stepCoord = floor(symbolCoord); // iii. remap symbolCoord to [0.0, 1.0] ∈ R symbolCoord = symbolCoord - stepCoord; stepCoord = stepCoord / vec2(resolution); vec4 texel = texture2D(source, stepCoord); vec2 tile = vec2((floor(luma(texel.rgb) * pg_size) + symbolCoord.x) / pg_size, symbolCoord.y); // display uv or sample palette using symbolCoord gl_FragColor = pixelator ? uv ? vec4(stepCoord.st, 0.0, 1.0): texture2D(source, stepCoord) : uv ? vec4(tile.st, 0.0, 1.0) : texture2D(palette, tile); } Se evidencia que el shader recibe tanto source como palette, donde la primera corresponde a la imagen que se va a reproducir y la segunda a la paleta de imágenes que se interpolarán. Entonces, se cuenta con la función luma (ya mencionada en secciones anteriores), puesto que se usa como criterio de ordenamiento. En el main se remapean las coordenadas según la resolución, y luego se normalizan a un rango entre 0 y 1. Se crea el texel como si se tratase del pixelador y, con este, se determinan las coordenadas de la tesela correspondiente: para ello se calcula el \u0026ldquo;luma\u0026rdquo; del texel, multiplicado por el tamaño del buffer. Eso se le suma a la coordenada en x, para luego dividirse de nuevo entre el tamaño. De esa manera se consigue que la tesela (es decir, la imagen seleccionada del buffer) corresponda con el nivel de luminancia del texel.\nLuego, para hacer el sampleo, se mira si está habilitado o no la opción de pixelador: en caso que sí, se aplica la textura de la imagen original en las coordenadas correspondientes tras la aplicación de la coherencia espacial; en caso que no, se aplica la textura de la paleta en las coordenadas calculadas para la tesela. En ambos casos se cuenta con la opción de que se habilite o no la visualización uv.\nPor su parte, dentro del código de javascript se destaca que, en el preload se cargan todas las imágenes al arreglo paintings. Después se hace uso de la librería p5.Quadrille para organizarlas según el criterio \u0026ldquo;luma\u0026rdquo; y, tras ello, ponerlas en un objeto buffer en celdas de 100x100 y sin bordes. Luego está una función para que se cambien las imágenes al oprimir las teclas indicadas más arriba. Por último, en el draw se pasan tanto el buffer como la imagen a replicar al shader y se traza la textura obtenida. El archivo completo se deja a continuación:\nCódigo completo let mosaic; let paintings, imageCells, pg; let symbol; // ui let resolution; let uv, pixelator; let movie = 0, num_movies = 56; function preload() { mosaic = readShader(\u0026#39;/showcase/sketches/frags/photomosaic.frag\u0026#39;, { varyings: Tree.texcoords2 }); paintings = []; for (let i = 1; i \u0026lt;= num_movies; i++) { paintings.push(loadImage(`../../assets/photomosaic/${i}.jpg`)); } } function setup() { createCanvas(600, 600, WEBGL); textureMode(NORMAL); noStroke(); shader(mosaic); imageCells = createQuadrille(paintings); resolution = createSlider(10, 600, 450, 10); resolution.position(width - 120, 620); resolution.style(\u0026#39;width\u0026#39;, \u0026#39;80px\u0026#39;); resolution.input(() =\u0026gt; { mosaic.setUniform(\u0026#39;resolution\u0026#39;, resolution.value()) }); mosaic.setUniform(\u0026#39;resolution\u0026#39;, resolution.value()); symbol = paintings[movie]; mosaic.setUniform(\u0026#39;uv\u0026#39;, false); uv = createCheckbox(\u0026#39;uv visualization\u0026#39;, false); uv.style(\u0026#39;color\u0026#39;, \u0026#39;white\u0026#39;); uv.changed(() =\u0026gt; mosaic.setUniform(\u0026#39;uv\u0026#39;, uv.checked())); uv.position(30, 620); pixelator = createCheckbox(\u0026#39;pixelator\u0026#39;, false); pixelator.style(\u0026#39;color\u0026#39;, \u0026#39;white\u0026#39;); pixelator.changed(() =\u0026gt; mosaic.setUniform(\u0026#39;pixelator\u0026#39;, pixelator.checked())); pixelator.position(width/2 - 40, 620); pg = createGraphics(100 * imageCells.width, 100); mosaic.setUniform(\u0026#39;pg_size\u0026#39;, imageCells.width); imageCells.sort(); drawQuadrille(imageCells, { graphics: pg, cellLength: 100, outlineWeight: 0, }); } function keyPressed() { if (key === \u0026#39;d\u0026#39;) { movie += 1; if (movie \u0026gt;= num_movies){ movie = 0; } symbol = paintings[movie]; } else if (key === \u0026#39;s\u0026#39;) { movie -= 1; if (movie \u0026lt; 0){ movie = num_movies - 1; } symbol = paintings[movie]; } } function draw() { mosaic.setUniform(\u0026#39;palette\u0026#39;, pg); mosaic.setUniform(\u0026#39;source\u0026#39;, symbol); beginShape(); vertex(-1, -1, 0, 0, 1); vertex(1, -1, 0, 1, 1); vertex(1, 1, 0, 1, 0); vertex(-1, 1, 0, 0, 0); endShape(); } Conclusiones y Trabajo futuro\u003e Conclusiones y Trabajo futuro # Esta aplicación demuestra que el uso de shaders reduce bastante el consumo de recursos computacionales con respecto a la sección de coherencia espacial de la entrega anterior, pues facilita las operaciones que requieren hacerse pixel a pixel. Puede verse, en algunas de las imágenes, que su reproducción en el fotomosaico resulta un poco distorsionada: esto es evidente, más que todo, en las carátulas de películas que tienen juegos de luces y sombras, así que esas distorsiones tienen sentido ya que el criterio de ordenamiento empleado para la paleta fue luma. Como trabajo futuro se podría seguir esta misma aplicación sobre videos (por ejemplo, escenas de películas), donde las teselas también sean archivos de video. Sin embargo eso requeriría definir otros criterios, dado que luma puede no ser constante a lo largo de todos los fotogramas. Referencias\u003e Referencias # [1] J. P. Charalambos, \u0026ldquo;Spatial Coherence\u0026rdquo;. Visual Computing, Apr. 2023. https://visualcomputing.github.io/docs/shaders/spatial_coherence/\u003e [1] J. P. Charalambos, \u0026ldquo;Spatial Coherence\u0026rdquo;. Visual Computing, Apr. 2023. https://visualcomputing.github.io/docs/shaders/spatial_coherence/ # [2] J. P. Charalambos, \u0026ldquo;Photomosaic\u0026rdquo;. Visual Computing, Apr. 2023. https://visualcomputing.github.io/docs/shaders/photomosaic/\u003e [2] J. P. Charalambos, \u0026ldquo;Photomosaic\u0026rdquo;. Visual Computing, Apr. 2023. https://visualcomputing.github.io/docs/shaders/photomosaic/ # [3] C. Poynton, \u0026ldquo;Digital Video and HD: Algorithms and Interfaces,\u0026rdquo; 2nd ed., Morgan Kaufmann, 2012, pp. 31-35, 65-68, 333, 337, ISBN 978-0-12-391926-7. [Online]. Available: https://www.sciencedirect.com/book/9780123919267/digital-video-and-hd\u003e [3] C. Poynton, \u0026ldquo;Digital Video and HD: Algorithms and Interfaces,\u0026rdquo; 2nd ed., Morgan Kaufmann, 2012, pp. 31-35, 65-68, 333, 337, ISBN 978-0-12-391926-7. [Online]. Available: https://www.sciencedirect.com/book/9780123919267/digital-video-and-hd # ","date":"1 January 0001","permalink":"/showcase/second/photomosaic/","section":"Seconds","summary":"Spatial Coherence \u0026amp; Photomosaic\u003e Spatial Coherence \u0026amp; Photomosaic # Introducción\u003e Introducción # Para este ejercicio se puso en práctica el fenómeno visual de \u0026ldquo;Spatial Coherence\u0026rdquo; con el objetivo de desarrollar un código capaz de pixelar una imagen y, a partir de este, desarrollar una aplicación que consiste en un fotomosaico.","title":""},{"content":"Procedural Texturing\u003e Procedural Texturing # Introducción\u003e Introducción # En esta sección se aborda el problema de procedural texturing, que consiste en una técnica para generar texturas de forma automatizada mediante algoritmos y reglas matemáticas, en lugar de utilizar imágenes o fotografías preexistentes como base.\nMarco Teórico\u003e Marco Teórico # En lugar de depender de imágenes de texturas reales, la texturización procedural se basa en algoritmos y fórmulas matemáticas para crear patrones y detalles visuales que se aplican a las superficies de los objetos virtuales en un entorno 3D. Esto permite generar texturas de forma dinámica y controlada, ofreciendo una mayor flexibilidad y eficiencia en comparación con el uso de imágenes de texturas estáticas.\nLos procedimientos utilizados en la texturización procedural pueden incluir funciones matemáticas, ruido fractal, algoritmos de generación procedural, entre otros. Estos procedimientos permiten crear texturas complejas, variadas y detalladas que se adaptan a las características y propiedades de los objetos virtuales.\nAlgunos ejemplos de texturización procedural incluyen la creación de patrones naturales como terrenos, nubes, follaje, así como texturas más artificiales como ladrillos, madera, metal, entre otros. La texturización procedural también puede ser utilizada para simular efectos visuales como desgaste, corrosión, deformación o efectos de iluminación.\nLas ventajas de la texturización procedural incluyen la capacidad de generar texturas infinitamente escalables, menor necesidad de almacenamiento, mayor control y manipulación de las texturas, y la posibilidad de generar variaciones aleatorias de texturas para aumentar la diversidad y realismo de las escenas.\nA continuación se detallan algunos elementos relevantes:\nPatrones: En los shaders el número de cálculos se mantiene constante. Por ende, son comúnmente empleados para la generación de patrones, donde se realiza una normalización del espacio, de forma que las coordenadas queden entre 1 y 0, las cuales se pueden dividir fácilmente para generar una cuadrícula. Las cuadrículas son especialmente útiles a la hora de generar patrones y se han utilizado desde la antigüedad; por ejemplo, en los mosaicos de los baños romanos.\nEn glsl, para realizar este proceso se toman las coordenadas de textura y se dividen en la resolución de la pantalla. Estas coordenadas también se pueden escalar para ocupar el espacio de una cuadrícula. Dentro de estas cuadrículas ya es posible patrones, tales como el patrón offset o patrón de desplazamiento.\nRuido: Ya teniendo las bases para generar formas definidas, se hace necesario introducir la aleatoriedad para generar formas más realistas. El primer acercamiento que se hace frente a este tema utiliza funciones sinusoidales: estas se multiplican por números muy grandes y, luego, se extrae la parte fraccionaria de cada número, generando números pseudoaleatorios. El resultado de funciones como tal es un ruido como el que se generaba en los televisores antiguos cuando no había señal. En los años 80 Ken Perlin se enfrentó al reto de generar texturas más realistas para el cine y, por consiguiente, debía lograr un ruido más \u0026ldquo;natural\u0026rdquo;. Entonces, ideó un algoritmo llamado “Value Noise”, el cual utiliza una interpolación de la parte entera y la parte fraccionaria del número de entrada para realizar la generación, manteniendo tanto una correlación con la parte entera como un elemento que aporta aleatoriedad.\nAhora bien, aunque ese tipo de fórmulas aportan aleatoriedad, no resultan aplicables a la realidad, pues, en la naturaleza, la mayoría de los patrones guardan memoria del estado anterior.\nNo obstante, esta función no fue lo suficientemente buena para Perlin, quien ideó otra implementación de este algoritmo en 1985, la cual llamó “Gradient Noise”. En esta, Perlin plantea la interpolación de gradientes aleatorios, en lugar de valores. Estos gradientes son el resultado de una función aleatoria en 2D que retorna direcciones. Finalmente, en 2001 presenta el algoritmo “simplex noise”, el cual tenía menor complejidad computacional y menos multiplicaciones, y un ruido que escala a dimensiones más altas con menos coste computacional, sin artefactos direccionales, con gradientes bien definidos y continuos que puedan calcularse de forma bastante económica.\nLos tipos de ruido mencionados se implementan y detallan más adelante.\nIluminación: Para que un objeto tenga una apariencia texturizada, es necesario utilizar luces que se reflejarán según la forma y el material del objeto, creando efectos de brillo y profundidad. Dentro del modelo de iluminación más comúnmente utilizado se hallan diferentes tipos de luces:\nLuz difusa: Es la luz reflejada por un objeto en todas las direcciones.\nLuz ambiente: Se utiliza para simular la iluminación rebotada. Rellena las áreas donde no hay luz directa, evitando que esas zonas se vuelvan demasiado oscuras. Por lo general, el valor de luz ambiente es proporcional al color difuso.\nLuz especular: Es la luz que se refleja con mayor intensidad en una dirección específica, generalmente alrededor del vector de reflexión de la luz y la normal de la superficie. Este color no está relacionado con el color difuso del objeto.\nLuz emisiva: Es cuando el propio objeto emite luz por sí mismo.\nLuz puntual: Implica que hay un punto específico en la escena desde donde la luz se emite e ilumina los objetos.\nCódigo y Resultados:\u003e Código y Resultados: # Para este ejercicio se llevó a cabo la generación de una textura de una pared de ladrillos desde el shader. Acto seguido, se agregaron las opciones de ruido descritas en el marco teórico para emular la rugosidad de los ladrillos y, posteriormente, se le dio movimiento y velocidad a la figura, además de una luz difusa.\nAl interactuar con la aplicación encontrará dos sliders: el primero (el de la izquierda) permite incrementar el número de ladrillos de la textura. El segundo (el del centro) permite aumentar la velocidad de movimiento de la figura texturizada.\nPor otra parte, se cuenta con dos selectores: el primero (en la esquina superior izquierda) permite alternar entre los algoritmos de Perlin Noise; el segundo (en la parte inferior central), permite alternar entre varias formas sobre las cuales se desea aplicar la textura del shader.\nPara esta aplicación se implementó tanto un vertex shader como un fragment shader, los cuales se dejan a continuación:\nprocedural.vert precision mediump float; attribute vec3 aPosition; attribute vec2 aTexCoord; attribute vec3 aNormal; uniform mat4 uModelViewMatrix; uniform mat3 uNormalMatrix; varying vec2 texcoords2; varying vec3 normal3; varying vec3 position3; varying vec3 light_dir; varying vec3 eye; uniform vec4 light_pos; void main() { texcoords2 = aTexCoord; vec3 pos = vec3(uModelViewMatrix * vec4(aPosition, 1.0)); normal3 = vec3(normalize(uNormalMatrix * aNormal)); light_dir = vec3(light_pos) - pos; eye = -pos; position3 = aPosition; gl_Position = vec4(aPosition, 1.0); } Este vertex shader se crea con el propósito de declarar variables adicionales a las que tiene el shader que se genera automáticamente con treegl. Ahora, el fragment shader:\nprocedural.frag precision mediump float; uniform vec2 u_resolution; uniform float frameCount; varying vec2 texcoords2; varying vec3 light_dir; varying vec3 eye; varying vec3 normal3; uniform float brick_num; uniform float speedFactor; uniform bool valueNoise; uniform bool gradientNoise; uniform bool simplexNoise; float rand(vec2 n) { return fract(sin(dot(n, vec2(12.9898, 4.1414))) * 43758.5453); } float noise(vec2 p){ vec2 ip = floor(p); vec2 u = fract(p); u = u*u*(3.0-2.0*u); float res = mix( mix(rand(ip),rand(ip+vec2(1.0,0.0)),u.x), mix(rand(ip+vec2(0.0,1.0)),rand(ip+vec2(1.0,1.0)),u.x),u.y); return res*res; } vec2 random2(vec2 st){ st = vec2( dot(st,vec2(127.1,311.7)), dot(st,vec2(269.5,183.3)) ); return -1.0 + 2.0*fract(sin(st)*43758.5453123); } // Gradient Noise by Inigo Quilez - iq/2013 // https://www.shadertoy.com/view/XdXGW8 float noise2(vec2 st) { vec2 i = floor(st); vec2 f = fract(st); vec2 u = f*f*(3.0-2.0*f); return mix( mix( dot( random2(i + vec2(0.0,0.0) ), f - vec2(0.0,0.0) ), dot( random2(i + vec2(1.0,0.0) ), f - vec2(1.0,0.0) ), u.x), mix( dot( random2(i + vec2(0.0,1.0) ), f - vec2(0.0,1.0) ), dot( random2(i + vec2(1.0,1.0) ), f - vec2(1.0,1.0) ), u.x), u.y); } vec3 mod289(vec3 x) { return x - floor(x * (1.0 / 289.0)) * 289.0; } vec2 mod289(vec2 x) { return x - floor(x * (1.0 / 289.0)) * 289.0; } vec3 permute(vec3 x) { return mod289(((x*34.0)+1.0)*x); } // // Description : GLSL 2D simplex noise function // Author : Ian McEwan, Ashima Arts // Maintainer : ijm // Lastmod : 20110822 (ijm) // License : // Copyright (C) 2011 Ashima Arts. All rights reserved. // Distributed under the MIT License. See LICENSE file. // https://github.com/ashima/webgl-noise // float snoise(vec2 v) { // Precompute values for skewed triangular grid const vec4 C = vec4(0.211324865405187, // (3.0-sqrt(3.0))/6.0 0.366025403784439, // 0.5*(sqrt(3.0)-1.0) -0.577350269189626, // -1.0 + 2.0 * C.x 0.024390243902439); // 1.0 / 41.0 // First corner (x0) vec2 i = floor(v + dot(v, C.yy)); vec2 x0 = v - i + dot(i, C.xx); // Other two corners (x1, x2) vec2 i1 = vec2(0.0); i1 = (x0.x \u0026gt; x0.y)? vec2(1.0, 0.0):vec2(0.0, 1.0); vec2 x1 = x0.xy + C.xx - i1; vec2 x2 = x0.xy + C.zz; // Do some permutations to avoid // truncation effects in permutation i = mod289(i); vec3 p = permute( permute( i.y + vec3(0.0, i1.y, 1.0)) + i.x + vec3(0.0, i1.x, 1.0 )); vec3 m = max(0.5 - vec3( dot(x0,x0), dot(x1,x1), dot(x2,x2) ), 0.0); m = m*m ; m = m*m ; // Gradients: // 41 pts uniformly over a line, mapped onto a diamond // The ring size 17*17 = 289 is close to a multiple // of 41 (41*7 = 287) vec3 x = 2.0 * fract(p * C.www) - 1.0; vec3 h = abs(x) - 0.5; vec3 ox = floor(x + 0.5); vec3 a0 = x - ox; // Normalise gradients implicitly by scaling m // Approximation of: m *= inversesqrt(a0*a0 + h*h); m *= 1.79284291400159 - 0.85373472095314 * (a0*a0+h*h); // Compute final noise value at P vec3 g = vec3(0.0); g.x = a0.x * x0.x + h.x * x0.y; g.yz = a0.yz * vec2(x1.x,x2.x) + h.yz * vec2(x1.y,x2.y); return 130.0 * dot(m, g); } void main (void) { vec2 positionVec4 = texcoords2; positionVec4.x += frameCount/(brick_num/speedFactor); float n = 1.; if(valueNoise) { // Value Noise n = noise(positionVec4*500.0)+0.2; } else if(gradientNoise) { // Gradient Noise n = noise2(positionVec4*200.0)+0.2; } else if(simplexNoise) { // Simplex Noise n = snoise(positionVec4*200.0)+0.2; } // forma de ladrillos vec2 st = gl_FragCoord.xy/u_resolution.xy; st *= brick_num; st.x += frameCount*speedFactor; float offset = step(1., mod(st.y,2.0)); float limitY = step(.8, mod(st.y,1.)); float limitX = step(1.8, mod(st.x+offset,2.0)); if(limitY==1.||limitX==1.){ gl_FragColor = vec4(0.9*(n+.3),0.79*(n+.3),0.69*(n+.3),1.0); }else{ gl_FragColor = vec4(.79*n,.25*n,.32*n,1.0); } vec3 nor = normalize(normal3); vec3 l = normalize(light_dir); vec3 e = normalize(eye); float intensity = max(dot(nor,l), 0.0); gl_FragColor = vec4(intensity, intensity, intensity, 1) * gl_FragColor; } Se tiene que, en primer lugar, hay una función para generar números pseudoaleatorios usando funciones sinusoidales, tal como se enunció en el marco teórico; la siguiente función aplica el algoritmo de \u0026ldquo;Value Noise\u0026rdquo;. Después hay otro generador de números aleatorios distinto y, tras él, se implementa el algorimto de \u0026ldquo;Gradient Noise\u0026rdquo; (en los comentarios del código se acreditan las fuentes de dónde se obtuvieron estos). Acto seguido, se implementan un par de funciones necesarias para el algoritmo \u0026ldquo;simplex noise\u0026rdquo;. En el main se define la textura de ladrillos a partir del ruido escogido, la velocidad y cantidad de ladrillos dadas por los sliders.\nAhora, en el archivo de JavaScript, se destaca la creación de un frame buffer (pg) aplicar el shader. Luego se crean todos los elementos html y, en el draw, se renderiza la textura sobre la forma seleccionada. Por último, en la función mouseMoved se lleva a cabo el establecimiento de una luz puntual que se refleja también en la textura. El código completo se encuentra a continuación:\nCódigo completo let pg; let proceduralShader; let bricksize, solidSpped, noiseSelection; let valueNoise = false; let gradientNoise = false; let simplexNoise = false; let shapeSelection; let selectedShape = \u0026#39;Sphere\u0026#39;; function preload() { proceduralShader = loadShader(\u0026#39;/showcase/sketches/frags/procedural.vert\u0026#39;, \u0026#39;/showcase/sketches/frags/procedural.frag\u0026#39;); } function setup() { createCanvas(500, 500, WEBGL); pg = createGraphics(500, 500, WEBGL); pg.textureMode(NORMAL); pg.noStroke(); pg.shader(proceduralShader); // emitResolution, see: https://github.com/VisualComputing/p5.treegl#macros pg.emitResolution(proceduralShader); pg.quad(-1, -1, 1, -1, 1, 1, -1, 1); bricksize = createSlider(10, 100, 10, 10); bricksize.position(30, 30); solidSpped = createSlider(0.05, 0.4, 0.05, 0.05); solidSpped.position(190, 30); noiseSelection = createSelect(); noiseSelection.position(360, 30); noiseSelection.option(\u0026#39;None\u0026#39;); noiseSelection.option(\u0026#39;Value Noise\u0026#39;); noiseSelection.option(\u0026#39;Gradient Noise\u0026#39;); noiseSelection.option(\u0026#39;Simplex Noise\u0026#39;); noiseSelection.changed(() =\u0026gt; { let val = noiseSelection.value(); valueNoise = gradientNoise = simplexNoise = false; if(val === \u0026#39;Value Noise\u0026#39;){ valueNoise = true; } else if(val === \u0026#39;Gradient Noise\u0026#39;){ gradientNoise = true; } else if(val === \u0026#39;Simplex Noise\u0026#39;){ simplexNoise = true; } }); shapeSelection = createSelect(); shapeSelection.position(220, height - 30); shapeSelection.option(\u0026#39;Sphere\u0026#39;); shapeSelection.option(\u0026#39;Cylinder\u0026#39;); shapeSelection.option(\u0026#39;Box\u0026#39;); shapeSelection.option(\u0026#39;Cone\u0026#39;); shapeSelection.changed(() =\u0026gt; { selectedShape = shapeSelection.value(); }); // Set texture proceduralShader.setUniform(\u0026#39;brick_num\u0026#39;, 10.0); proceduralShader.setUniform(\u0026#39;speedFactor\u0026#39;, 0.02); proceduralShader.setUniform(\u0026#39;valueNoise\u0026#39;, false); proceduralShader.setUniform(\u0026#39;gradientNoise\u0026#39;, false); proceduralShader.setUniform(\u0026#39;simplexNoise\u0026#39;, false); texture(pg); noStroke(); } function draw() { proceduralShader.setUniform(\u0026#39;frameCount\u0026#39;, frameCount); proceduralShader.setUniform(\u0026#39;brick_num\u0026#39;, bricksize.value()); proceduralShader.setUniform(\u0026#39;speedFactor\u0026#39;, solidSpped.value()); proceduralShader.setUniform(\u0026#39;valueNoise\u0026#39;, valueNoise); proceduralShader.setUniform(\u0026#39;gradientNoise\u0026#39;, gradientNoise); proceduralShader.setUniform(\u0026#39;simplexNoise\u0026#39;, simplexNoise); pg.quad(-1, -1, 1, -1, 1, 1, -1, 1); orbitControl(); if(selectedShape === \u0026#39;Sphere\u0026#39;){ background(0); sphere(150); } else if(selectedShape === \u0026#39;Cylinder\u0026#39;){ background(0); cylinder(150, 200); } else if(selectedShape === \u0026#39;Box\u0026#39;){ background(0); box(200); } else if(selectedShape === \u0026#39;Cone\u0026#39;){ background(0); cone(150,250); } } function mouseMoved() { let lightLoc = treeLocation(createVector(-(mouseX - width / 2), -(mouseY - height / 2), 1.5), { from: \u0026#39;SCREEN\u0026#39;, to: \u0026#39;CLIP\u0026#39;}); proceduralShader.setUniform(\u0026#39;light_pos\u0026#39;, [lightLoc.x, lightLoc.y, lightLoc.z, 1.] ); pg.quad(-1, -1, 1, -1, 1, 1, -1, 1); } Conclusiones y Trabajo futuro\u003e Conclusiones y Trabajo futuro # Esta técnica para generar texturas en el shader es muy útil dado que es muy eficiente al aprovechar las capacidades de las GPU y permite una mayor flexibilidad al momento de generar escenarios en 3D. No obstante, para darle más naturalidad a la textura resulta necesario el uso de funciones de ruido. Adicionalmente, estas texturas necesitan de diseños que permitan percibir profundidad entre otros rasgos para no verse solo como una imagen pintada en el objeto, para lo cual ya se utilizan modelos de iluminación y de texture mapping combinados con las técnicas de procedural texturing. Pueden llegar a apreciarse patrones de moiré en algunas de las figuras si se disminuye lo suficiente el tamaño de los ladrillos, lo cual permite ver la relación que guarda el enmascaramiento visual con el procedural texturing. Como trabajo futuro se podrían implementar shaders para aplicar múltiples texturas (no solamente los ladrillos), incluyendo también el Perlin Noise y la iluminación que se trabajaron aquí. Adicionalmente, se podría ahondar en el uso de keyframes para elaborar animaciones a partir de objetos diseñados con la técnica de texturizado procedural. Referencias\u003e Referencias # [1] J. P. Charalambos, \u0026ldquo;Shaders\u0026rdquo;. Visual Computing, Apr. 2023. https://visualcomputing.github.io/docs/shaders/\u003e [1] J. P. Charalambos, \u0026ldquo;Shaders\u0026rdquo;. Visual Computing, Apr. 2023. https://visualcomputing.github.io/docs/shaders/ # [2] J. P. Charalambos, \u0026ldquo;Procedural Texturing\u0026rdquo;. Visual Computing, Apr. 2023. https://visualcomputing.github.io/docs/shaders/procedural_texturing/\u003e [2] J. P. Charalambos, \u0026ldquo;Procedural Texturing\u0026rdquo;. Visual Computing, Apr. 2023. https://visualcomputing.github.io/docs/shaders/procedural_texturing/ # [3] GILET, Guillaume, et al. Local random-phase noise for procedural texturing. ACM Transactions on Graphics (ToG), 2014, vol. 33, no 6, p. 1-11.\u003e [3] GILET, Guillaume, et al. Local random-phase noise for procedural texturing. ACM Transactions on Graphics (ToG), 2014, vol. 33, no 6, p. 1-11. # [4] Shadertoy. Gradient Noise. https://www.shadertoy.com/view/XdXGW8\u003e [4] Shadertoy. Gradient Noise. https://www.shadertoy.com/view/XdXGW8 # [5] Ian McEwan, Ashima Arts. GLSL 2D simplex noise function. https://github.com/ashima/webgl-noise\u003e [5] Ian McEwan, Ashima Arts. GLSL 2D simplex noise function. https://github.com/ashima/webgl-noise # [6] R. Touti, \u0026ldquo;Perlin Noise Algorithm\u0026rdquo;, [Online]. Available: https://rtouti.github.io/graphics/perlin-noise-algorithm.\u003e [6] R. Touti, \u0026ldquo;Perlin Noise Algorithm\u0026rdquo;, [Online]. Available: https://rtouti.github.io/graphics/perlin-noise-algorithm. # ","date":"1 January 0001","permalink":"/showcase/second/proceduraltexturing/","section":"Seconds","summary":"Procedural Texturing\u003e Procedural Texturing # Introducción\u003e Introducción # En esta sección se aborda el problema de procedural texturing, que consiste en una técnica para generar texturas de forma automatizada mediante algoritmos y reglas matemáticas, en lugar de utilizar imágenes o fotografías preexistentes como base.","title":""},{"content":"Rendering\u003e Rendering # Introducción\u003e Introducción # Para esta sección se llevó a cabo un estudio de diversos algoritmos de determinación de superficies ocultas, profundizando en el algoritmo del ordenamiento de pintado y de cómo este se puede aplicar en un sketch 3D.\nMarco Teórico\u003e Marco Teórico # Los algoritmos de determinación de superficies ocultas (Hidden Surface Determination Algorithms en inglés) son técnicas utilizadas en gráficos por computadora para determinar qué superficies de una escena 3D son visibles y cuáles están ocultas detrás de otras superficies. Estos algoritmos son fundamentales para generar imágenes realistas al renderizar escenas complejas.\nCuando se representan objetos 3D en una escena, es posible que algunas partes de los objetos estén ocultas a la vista debido a la intersección y superposición de los objetos. Los algoritmos de determinación de superficies ocultas resuelven este problema al identificar y eliminar las superficies que no son visibles desde el punto de vista del observador.\nExisten diferentes enfoques y algoritmos utilizados para la determinación de superficies ocultas, algunos de los cuales incluyen:\nAlgoritmo del Z-buffer (Z-buffer algorithm): Este es uno de los métodos más comunes y sencillos. Se utiliza un búfer de profundidad (Z-buffer) para almacenar la información de la profundidad de cada píxel en la escena. Durante el proceso de renderizado, se compara la profundidad de cada objeto y se mantiene la información del objeto más cercano en el búfer de profundidad. Esto permite renderizar los objetos más cercanos correctamente y descartar los objetos ocultos.\nAlgoritmo de eliminación de superficies ocultas (Back-face culling): Este algoritmo se basa en la eliminación de las superficies que están de espaldas al observador. Utiliza la orientación de las normales de las caras de los objetos para determinar si están orientadas hacia el observador o hacia el interior del objeto. Las superficies orientadas hacia el observador se consideran visibles y se renderizan, mientras que las superficies orientadas hacia el interior se descartan.\nBinary Space Partitioning (BSP): Es un algoritmo utilizado para dividir y organizar eficientemente el espacio tridimensional en un árbol binario. El algoritmo parte de un espacio inicial y lo divide en dos subespacios mediante un plano. Este proceso de subdivisión se repite recursivamente hasta que cada región del espacio contiene un conjunto de polígonos no superpuestos. El árbol binario resultante permite una representación jerárquica del espacio y se utiliza para acelerar las operaciones de determinación de superficies ocultas y renderizado en tiempo real.\nWarnock Algorithm: Este algoritmo, también conocido como el algoritmo de subdivisión recursiva de Warnock, se utiliza para renderizar escenas 2D y determinar la visibilidad de las regiones de una imagen. El algoritmo divide la imagen en regiones más pequeñas y decide si cada región está completamente dentro, completamente fuera o intersecta un objeto en la escena. La subdivisión recursiva continúa hasta que todas las regiones estén completamente dentro o fuera de los objetos. El algoritmo de Warnock se utiliza principalmente en renderizado de imágenes y ha sido utilizado históricamente en gráficos por computadora.\nRay Casting: Es una técnica utilizada para renderizar gráficos en 3D y simular la interacción de los rayos de luz con los objetos de una escena. El algoritmo lanza rayos desde una fuente de luz o desde la cámara a través de cada píxel de la imagen. Estos rayos se intersectan con los objetos de la escena, y se calcula la interacción de los rayos con los materiales de los objetos para determinar el color y la apariencia de cada píxel en la imagen final. El ray casting se utiliza para generar imágenes realistas y permite efectos como sombras, reflexiones y refracciones.\nAlgoritmo de corte de líneas (Line Clipping): Este algoritmo se utiliza para recortar líneas que están parcial o totalmente fuera de la ventana de visualización. Ayuda a eliminar segmentos de línea que no son visibles y solo muestra las partes visibles dentro del área de visualización.\nAlgoritmo de eliminación de superficies ocultas por área de recorte (Area Clipping): Este algoritmo se basa en recortar las regiones de las superficies que están completamente fuera de la ventana de visualización. Permite eliminar completamente las superficies ocultas antes de pasar a los pasos de renderizado, lo que ahorra tiempo de procesamiento.\nAlgoritmo del plano de separación (Plane-Sweep): Este algoritmo se utiliza para determinar qué polígonos se intersectan en una escena 3D. Se basa en mover un plano virtual a través de la escena y rastrear las intersecciones de los polígonos con el plano en un orden determinado. Permite identificar y eliminar superficies ocultas mediante el análisis de las intersecciones de los polígonos.\nAlgoritmo de detección de fronteras (Boundary Detection): Este algoritmo se utiliza para identificar las fronteras visibles entre los objetos de una escena. Examina las intersecciones de las aristas de los polígonos para determinar qué segmentos de línea son visibles y, por lo tanto, representan las fronteras entre objetos.\nAlgoritmo de eliminación de superficies ocultas basado en rasterización (Rasterization-based Hidden Surface Removal): Este enfoque se utiliza en el proceso de rasterización y renderizado en tiempo real. Se basa en el orden de dibujado de los polígonos en función de su distancia a la cámara. Los polígonos se dibujan en orden desde los más cercanos a los más lejanos, y se descartan aquellos que están completamente ocultos por polígonos previamente dibujados.\nAlgoritmo del ordenamiento de pintado (Painter\u0026rsquo;s algorithm): Es un método utilizado en gráficos por computadora para renderizar objetos tridimensionales en una escena 3D. Su objetivo principal es determinar el orden en el que se deben dibujar los objetos en función de su distancia desde la cámara y su posición relativa en la escena.\nEl algoritmo se basa en el principio de que los objetos más alejados de la cámara deben dibujarse primero, seguidos por los objetos que se encuentran más cerca. Esto se debe a que los objetos más cercanos pueden ocultar total o parcialmente los objetos más alejados. Al seguir este orden, se asegura que los objetos visibles se dibujen correctamente y que los objetos ocultos queden detrás de ellos.\nEl proceso del algoritmo del ordenamiento de pintado se puede resumir en los siguientes pasos:\nDeterminación de la distancia: Se calcula la distancia desde la cámara a cada objeto de la escena. Esto se puede lograr utilizando técnicas como el cálculo de la distancia euclidiana desde la cámara hasta el centro del objeto o utilizando información de profundidad almacenada en un búfer de profundidad (Z-buffer).\nOrdenamiento: Los objetos se ordenan en función de su distancia, de manera que los objetos más lejanos se dibujan primero y los objetos más cercanos se dibujan al final.\nRenderizado: Los objetos se dibujan en el orden determinado por el paso anterior. Cada objeto se renderiza, teniendo en cuenta el efecto de transparencia o superposición que pueda tener con otros objetos ya dibujados.\nEl principal desafío del algoritmo del ordenamiento de pintado es lidiar con los casos de intersección entre objetos. Cuando dos objetos se intersecan entre sí, puede haber partes de un objeto que estén ocultas por otro objeto. En estos casos, se pueden aplicar técnicas adicionales, como el recorte de polígonos (clipping) o el uso de información de profundidad precisa, para determinar qué partes de los objetos deben dibujarse y qué partes deben ser ocultas.\nEs importante destacar que el algoritmo del ordenamiento de pintado puede tener limitaciones en escenas con intersecciones complejas o cuando los objetos no tienen una representación convexa. En tales casos, pueden ser necesarios algoritmos más avanzados, como los algoritmos de partición espacial (como octree o BVH) o los algoritmos de eliminación de superficies ocultas basados en rasterización, para lograr una representación precisa de la escena en 3D.\nCódigo y Resultados\u003e Código y Resultados # Se optó por profundizar en el último algoritmo debido a su facilidad de abstracción. Por tanto, a continuación se deja el código de la replicación que hace Dave Pagurek [2] de portada de un álbum de la banda \u0026ldquo;Muse\u0026rdquo; en una escena 3D. Para ver la ejecución, copie y pegue el código dentro del widget. Luego oprima el botón \u0026ldquo;play\u0026rdquo;.\n/* A 3D remake of the album art for Muse\u0026#39;s Origin of Symmetry Original album art by William Eagar This sketch is by Dave :) I manually sort the tuning forks by Z so that I can draw them one at a time using the 2D canvas\u0026#39;s blur filter to imitate depth of field. */ const maxBlur = 15 const shadowLength = 200 const handleHeight = 200 const forkWidth = 80 const forkHeight = 95 const thickness = 8 let webglCanvas let forks = [] function setup() { createCanvas(400, 400) pixelDensity(1) webglCanvas = createGraphics(width + 2 * maxBlur, height + 2 * maxBlur, WEBGL) webglCanvas.setAttributes({ antialias: true }) webglCanvas.pixelDensity(pixelDensity()) genForks() } function genForks() { forks = [] for (let i = 0; i \u0026lt; 10; i++) { forks.push({ x: random(-width, width), z: random(-width, width), rotation: random(TWO_PI), }) } } function mousePressed() { genForks() } let lastScene = 0 function draw() { const scene = floor(millis() / 10000) if (scene !== lastScene) { genForks() lastScene = scene } background(\u0026#39;#f5bf42\u0026#39;) fill(255) noStroke() drawingContext.filter = `blur(${(2 * pixelDensity()).toFixed(2)}px)` rect(-50, height * 0.45, width + 100, height) imageMode(CENTER) translate(width / 2, height / 2) const sceneRotation = millis() / 20000 const transform = new DOMMatrix() transform.rotateAxisAngleSelf(0, 1, 0, sceneRotation / PI * 180) for (const fork of forks) { const transformed = new DOMPoint(fork.x, 0, fork.z).matrixTransform(transform) fork.cameraDepth = transformed.z } forks.sort((a, b) =\u0026gt; a.cameraDepth - b.cameraDepth) // Shadows webglCanvas.clear() webglCanvas.reset() webglCanvas._renderer.GL.disable(webglCanvas._renderer.GL.DEPTH_TEST) for (const { x, z } of forks) { webglCanvas.push() webglCanvas.translate(0, 0, -400) webglCanvas.rotateY(sceneRotation) webglCanvas.translate(x, height * 0.3, z) webglCanvas.noStroke() webglCanvas.beginShape(TRIANGLE_STRIP) webglCanvas.fill(0, 0, 0, 255) webglCanvas.vertex(thickness / 2, 0, thickness / 2) webglCanvas.vertex(thickness / 2, 0, -thickness / 2) webglCanvas.fill(0, 0, 0, 0) webglCanvas.vertex(-shadowLength, 0, thickness / 2) webglCanvas.vertex(-shadowLength, 0, -thickness / 2) webglCanvas.endShape() webglCanvas.pop() } image(webglCanvas, 0, 0) webglCanvas._renderer.GL.enable(webglCanvas._renderer.GL.DEPTH_TEST) for (const { x, z, cameraDepth, rotation } of forks) { webglCanvas.clear() webglCanvas.reset() webglCanvas.push() webglCanvas.translate(0, 0, -400) webglCanvas.rotateY(sceneRotation) webglCanvas.translate(x, height * 0.3, z) webglCanvas.rotateY(rotation) webglCanvas.fill(255) webglCanvas.stroke(0) webglCanvas.strokeWeight(2) webglCanvas.push() webglCanvas.translate(0, -handleHeight / 2, 0) webglCanvas.box(thickness, handleHeight, thickness) webglCanvas.pop() webglCanvas.push() webglCanvas.translate(0, -handleHeight - thickness / 2, 0) webglCanvas.box(forkWidth, thickness, thickness) webglCanvas.pop() for (const side of [-1, 1]) { webglCanvas.push() webglCanvas.translate( side * (forkWidth - thickness) / 2, -handleHeight - thickness - forkHeight / 2, 0 ) webglCanvas.box(thickness, forkHeight, thickness) webglCanvas.pop() } webglCanvas.pop() const blur = Math.min( maxBlur, Math.abs(cameraDepth / 250) ) * pixelDensity() drawingContext.filter = `blur(${blur.toFixed(2)}px)` image(webglCanvas, 0, 0) } } Dado que el ejercicio consistía, para esta sección, en estudiar uno de los algoritmos, no se desarrolló una aplicación, sino que se lleva a cabo un análisis de un código desarrollado por un tercero. Este código es un sketch en p5.js que recrea la portada del álbum \u0026ldquo;Origin of Symmetry\u0026rdquo; de la banda Muse en 3D. El objetivo principal es dibujar una serie de diapasones sintonizadores de forma tridimensional y aplicar efectos visuales como sombras y desenfoque para simular profundidad y campo de visión.\nEl sketch comienza con la inicialización de las variables y la configuración del lienzo de dibujo. Luego, se define la función genForks() que se utiliza para generar la posición y la rotación aleatoria de los diapasones.\nLa función setup() establece el lienzo principal y crea un lienzo adicional en 3D llamado webglCanvas. Se configura el antialiasing y la densidad de píxeles para el lienzo en 3D.\nEl evento mousePressed() llama a la función genForks() cuando se hace clic en el lienzo, lo que genera una nueva disposición aleatoria de los diapasones.\nLa función draw() se ejecuta continuamente y se encarga de renderizar la escena en cada fotograma. Comienza estableciendo el color de fondo y aplicando un desenfoque a un rectángulo para simular el efecto de desenfoque de campo.\nA continuación, se realiza el renderizado de los diapasones. Se define una matriz de transformación que aplica una rotación basada en el tiempo para la escena en su conjunto. Luego, se calcula la profundidad de la cámara para cada diapasón y se ordenan según su profundidad en relación con la cámara. Después, se crea un lienzo adicional en 3D llamado webglCanvas para renderizar las sombras de los diapasones. Se dibujan las sombras utilizando formas primitivas y se aplica una transformación para ajustar su posición y rotación en la escena.\nFinalmente, se renderizan los diapasones principales. Se aplica una serie de transformaciones para posicionar y rotar cada diapasón en la escena. Se dibujan las partes individuales de los diapasones utilizando primitivas como cajas y se aplica un efecto de desenfoque en función de la profundidad de la cámara para simular el enfoque y la sensación de profundidad.\nLo más destacable del código, respecto al tema de estudio de algoritmos de determinación de superficies ocultas, es que cumple con los pasos del algoritmo del ordenamiento de pintado. A saber:\nDeterminación de la distancia: En el bucle for (const fork of forks), se calcula la posición de cada diapasón en relación con la cámara y se almacena en la propiedad cameraDepth del objeto fork. Esto se logra mediante la transformación de la posición del diapasón utilizando una matriz de transformación en el espacio tridimensional.\nOrdenamiento: Después de calcular las distancias, se realiza el ordenamiento de los diapasones en función de su propiedad cameraDepth utilizando el método sort() en la siguiente línea:\nforks.sort((a, b) =\u0026gt; a.cameraDepth - b.cameraDepth)\nEsto garantiza que los diapasones se dibujen en orden ascendente de profundidad de la cámara, desde los más alejados hasta los más cercanos.\nRenderizado: El renderizado de los diapasones se realiza dentro del bucle for (const { x, z, cameraDepth, rotation } of forks). Cada diapasón se dibuja en función de su posición, rotación y profundidad de la cámara. Se utilizan diversas funciones para dibujar los componentes de los diapasones, como cajas (box()) y formas (beginShape() y endShape()). La función image() se utiliza para renderizar los diapasones en el lienzo principal.\nEn resumen, el código refleja los pasos del algoritmo de determinación de superficies ocultas al calcular las distancias desde la cámara, ordenar los objetos en función de esas distancias y realizar el renderizado en el orden adecuado.\nConclusiones y Trabajo futuro\u003e Conclusiones y Trabajo futuro # En general, aunque el desarrollo de sketches en 3D no se enfque específicamente en la determinación de superficies ocultas, estos siguen una estructura que reflejan los pasos clave de un algoritmo determinado de forma implícita. Por ejemplo, aquí se pudo evidenciar la determinación de la distancia, el ordenamiento y el renderizado, lo cual permite deducir que se emplea un algoritmo de ordenamiento de pintado para el desarrollo de la escena. Esta labor puede llevarse a cabo con otros sketches para determinar cómo están implícitos otros algoritmos de determinación de superficies ocultas en ellos. En compañía con los algoritmos, es importante que se apliquen efectos visuales como sombras y desenfoque para simular la profundidad y el campo de visión en la escena. También se puede hacer uso de claves monoculares y otras estrategias para lograr dicho propósito. Como trabajo futuro se podría profundizar en los demás algoritmos, evaluar distintos sketches y realizar una comparativa en cuanto al consumo de recursos computacionales, fiabilidad de la escena, entre otros. Incluso, sería interesante ver cuál de estos algoritmos es el más utilizado y con qué otros efectos visuales se suele acompañar. Otra propuesta de trabajo futuro consiste en desarrollar un sketch 3d que use el algoritmo de ordenamiento de pintado, para luego volverlo a desarrollar empleando un algoritmo distinto con el fin de ver qué cosas cambiarían a nivel de código. Referencias\u003e Referencias # [1] J. P. Charalambos, \u0026ldquo;Rendering\u0026rdquo;. Visual Computing, May. 2023. https://visualcomputing.github.io/docs/rendering/\u003e [1] J. P. Charalambos, \u0026ldquo;Rendering\u0026rdquo;. Visual Computing, May. 2023. https://visualcomputing.github.io/docs/rendering/ # [2] D. Pagurek, \u0026ldquo;Depth of field in p5.js\u0026rdquo;. Blog. Oct. 2021. https://www.davepagurek.com/blog/depth-of-field/\u003e [2] D. Pagurek, \u0026ldquo;Depth of field in p5.js\u0026rdquo;. Blog. Oct. 2021. https://www.davepagurek.com/blog/depth-of-field/ # [3] SUTHERLAND, Ivan E.; SPROULL, Robert F.; SCHUMACKER, Robert A. A characterization of ten hidden-surface algorithms. ACM Computing Surveys (CSUR), 1974, vol. 6, no 1, p. 1-55.\u003e [3] SUTHERLAND, Ivan E.; SPROULL, Robert F.; SCHUMACKER, Robert A. A characterization of ten hidden-surface algorithms. ACM Computing Surveys (CSUR), 1974, vol. 6, no 1, p. 1-55. # [4] DE BERG, Mark, et al. Binary space partitions: The painter’s algorithm. Computational Geometry: Algorithms and Applications, 2008, p. 259-281.\u003e [4] DE BERG, Mark, et al. Binary space partitions: The painter’s algorithm. Computational Geometry: Algorithms and Applications, 2008, p. 259-281. # [5] NISHA, Nisha. Visible Surface Detection Algorithms: A Review. International Journal of Advanced Engineering Research and Science, 2017, vol. 4, no 2, p. 237055.\u003e [5] NISHA, Nisha. Visible Surface Detection Algorithms: A Review. International Journal of Advanced Engineering Research and Science, 2017, vol. 4, no 2, p. 237055. # ","date":"1 January 0001","permalink":"/showcase/second/rendering/","section":"Seconds","summary":"Rendering\u003e Rendering # Introducción\u003e Introducción # Para esta sección se llevó a cabo un estudio de diversos algoritmos de determinación de superficies ocultas, profundizando en el algoritmo del ordenamiento de pintado y de cómo este se puede aplicar en un sketch 3D.","title":""},{"content":"Texturing\u003e Texturing # Introducción\u003e Introducción # En esta sección se abordará el problema de texture sampling, también conocido como mapeo de texturas o filtrado de texturas, el cual es una técnica utilizada en gráficos por computadora para aplicar texturas a superficies u objetos 3D.\nMarco Teórico\u003e Marco Teórico # El filtrado o suavizado de texturas abarca los métodos empleados para reconstruir texturas mediante la modificación de los valores de los pixeles de estas de acuerdo a un criterio. Por su parte, el tinte de texturas consiste en operar los valores de las componentes RGB de los pixeles con información interpolada en estos, como la posición, la luz, y otro tipo de datos que se puedan asociar a los vértices de los triángulos que conforman el objeto en cuestión.\nEn este ejercicio se realizan dos implementaciones:\nColoring Brightness Tools: Las herramientas de ajuste de brillo para colorear, como luma, HSV, HSL y Average, se utilizan para modificar el brillo o la luminosidad de un color o una imagen. Estas herramientas permiten ajustar la intensidad luminosa de los colores de una manera específica.\nLuma es una medida de la luminosidad de un color. En el contexto del ajuste de brillo, la luma se refiere a modificar la cantidad de luz en una imagen manteniendo la saturación de los colores. Ajustar la luma puede oscurecer o aclarar una imagen sin alterar los tonos de color.\nEl modelo de color HSV divide los colores en tres componentes: matiz (hue), saturación (saturation) y valor (value). La herramienta de brillo HSV se enfoca en el componente de valor, que controla la luminosidad de un color. Ajustar el valor puede hacer que un color sea más brillante o más oscuro sin afectar su tono o saturación.\nSimilar al modelo HSV, el modelo de color HSL también utiliza el matiz (hue) y la saturación (saturation) como componentes, pero reemplaza el valor con luminosidad (lightness). La herramienta de brillo HSL permite ajustar la luminosidad de un color en una escala de más oscuro a más claro, manteniendo el tono y la saturación.\nLa herramienta \u0026ldquo;Average\u0026rdquo; es una técnica básica para ajustar el brillo en la que se calcula el valor promedio de los componentes de color de un píxel o una selección de píxeles. Al promediar los valores de los componentes, se obtiene un nuevo color que puede ser más brillante o más oscuro que el original, dependiendo de los valores iniciales.\nEstas herramientas de ajuste de brillo son comunes en aplicaciones de edición de imágenes y software de diseño gráfico. Permiten controlar el aspecto visual de una imagen o un color mediante el ajuste de su luminosidad, sin modificar necesariamente otros aspectos como el tono o la saturación.\nTexture Tinting: También conocido como tinte de textura, es una técnica utilizada en gráficos por computadora para modificar el color de una textura aplicada a una superficie o un objeto. Consiste en cambiar el tono o matiz de una textura manteniendo su estructura y detalles originales.\nCuando se aplica una textura a una superficie, es posible aplicar un tinte o coloración a esa textura para alterar su apariencia. Esto se logra multiplicando o mezclando los valores de color de los texels de la textura con un color específico.\nEl tinte de textura puede ser utilizado para varios propósitos, como ajustar la atmósfera o el ambiente de una escena, resaltar o cambiar el tono de ciertos elementos de una textura, o crear variaciones de color en una superficie para darle más realismo o estilizarla.\nEn la implementación práctica, el tinte de textura se puede lograr mediante operaciones de mezcla o multiplicación de colores en el espacio de color utilizado en la escena, como RGB (rojo, verde, azul) u otros espacios de color.\nEs importante tener en cuenta que el tinte de textura no altera la estructura de la textura ni los patrones o detalles originales, sino que solo modifica el color en función del tinte aplicado.\nCódigo y Resultados\u003e Código y Resultados # A continuación se presenta el ejercicio desarrollado, en el cual se carga una imagen (o video) y, sobre esta, se pueden aplicar las herramientas mencionadas anteriormente.\nPuede dar clic sobre el selector de la parte superior izquierda para cambiar entre luma, HSV, HSL y Average; junto a este encuentra un checkbox para alternar entre imagen o video. Por su parte, con el selector de la parte superior derecha es posible seleccionar si aplicar el tinte o no; adicionalmente, al lado se cuenta con la opción de elegir el color a interporlar.\nA continuación se muestra el shader implementado para esta aplicación:\ntexturing.frag precision mediump float; uniform int coloringBrightness; uniform bool tinting; uniform vec4 color; uniform sampler2D texture; varying vec2 texcoords2; float luma(vec3 texel) { return 0.299 * texel.r + 0.587 * texel.g + 0.114 * texel.b; } float hsv(vec3 texel) { return max(max(texel.r, texel.b), max(texel.r,texel.g)); } float hsl(vec3 texel){ float maxColor = max(max(texel.r, texel.g), texel.b); float minColor = min(min(texel.r, texel.g), texel.b); return (maxColor + minColor)/2.0; } float average(vec3 texel) { return (texel.r + texel.g + texel.b)/3.0; } vec4 tint(vec4 texel, bool blend){ if (blend) { return texel * color; } else { return texel; } } void main() { vec4 texel = texture2D(texture, texcoords2); if (coloringBrightness == 1) { gl_FragColor = tint(vec4((vec3(luma(texel.rgb))), 1.0), tinting); } else if (coloringBrightness == 2) { gl_FragColor = tint(vec4((vec3(hsv(texel.rgb))), 1.0), tinting); } else if (coloringBrightness == 3) { gl_FragColor = tint(vec4((vec3(hsl(texel.rgb))), 1.0), tinting); } else if (coloringBrightness == 4) { gl_FragColor = tint(vec4((vec3(average(texel.rgb))), 1.0), tinting); } else { gl_FragColor = tint(texel, tinting); } } Es evidente que para el HSV se toma el mayor valor de los canales de color, para el HSL se promedia entre el máximo y el mínimo, y para \u0026ldquo;average\u0026rdquo; se calcula el promedio de los componentes r, g, b. En cuanto al tinte de la textura, se aplica un \u0026ldquo;color blending\u0026rdquo; sencillo, realizando una multiplicación entre el color seleccionado y el texel. Para mayor detalle sobre esta mezcla de colores, se puede remitir a la sección anterior.\nEl código en JavaScript, por su parte, también es bastante intuitivo. Primero se carga la imagen. Luego, se instancian los objetos html, entre ellos los selectores para determinar la aplicación de las herramientas implementadas en el shader, un selector de color y un checkbox para alternar entre imagen y video. Después, tras indicar los valores uniformes, se traza la textura utilizando la función vertex, donde las coordenadas le indican que debe ocupar todo el canvas. A continuación se deja el archivo en su totalidad:\nCódigo completo let coloringShader; let img, vid, video_on; let coloringBrightness; let blendingSelect, picker, selectedColor; function preload() { coloringShader = readShader(\u0026#39;/showcase/sketches/frags/texturing.frag\u0026#39;, { varyings: Tree.texcoords2 }); img = loadImage(\u0026#39;showcase/assets/fire_breathing.jpg\u0026#39;); vid = createVideo([\u0026#39;showcase/assets/wagon.webm\u0026#39;]); vid.hide(); src = img } function setup() { createCanvas(700, 500, WEBGL); noStroke(); textureMode(NORMAL); shader(coloringShader); video_on = createCheckbox(\u0026#39;video\u0026#39;, false); video_on.style(\u0026#39;color\u0026#39;, \u0026#39;white\u0026#39;); video_on.changed(() =\u0026gt; { src = video_on.checked() ? vid : img; video_on.checked() ? vid.loop() : vid.pause(); }); video_on.position(120, 30); coloringBrightness = createSelect(); coloringBrightness.position(30, 30); coloringBrightness.option(\u0026#39;None\u0026#39;); coloringBrightness.option(\u0026#39;Luma\u0026#39;); coloringBrightness.option(\u0026#39;HSV\u0026#39;); coloringBrightness.option(\u0026#39;HSL\u0026#39;); coloringBrightness.option(\u0026#39;Average\u0026#39;); picker = createColorPicker(color(\u0026#39;#010104\u0026#39;)); picker.position(width - 60, 30); blendingSelect = createSelect(); blendingSelect.position(width - 140, 30); blendingSelect.option(\u0026#39;No Tint\u0026#39;); blendingSelect.option(\u0026#39;Tint\u0026#39;); coloringBrightness.changed(() =\u0026gt; { let val = coloringBrightness.value(); if (val === \u0026#39;Luma\u0026#39;) { coloringShader.setUniform(\u0026#39;coloringBrightness\u0026#39;, 1); } else if (val === \u0026#39;HSV\u0026#39;) { coloringShader.setUniform(\u0026#39;coloringBrightness\u0026#39;, 2); } else if (val === \u0026#39;HSL\u0026#39;) { coloringShader.setUniform(\u0026#39;coloringBrightness\u0026#39;, 3); } else if (val === \u0026#39;Average\u0026#39;) { coloringShader.setUniform(\u0026#39;coloringBrightness\u0026#39;, 4); } else { coloringShader.setUniform(\u0026#39;coloringBrightness\u0026#39;, 0); } }); blendingSelect.changed(() =\u0026gt; { if (blendingSelect.value() === \u0026#39;Tint\u0026#39;) { coloringShader.setUniform(\u0026#39;tinting\u0026#39;, true); } else { coloringShader.setUniform(\u0026#39;tinting\u0026#39;, false); } }); coloringShader.setUniform(\u0026#39;coloringBrightness\u0026#39;, 0); coloringShader.setUniform(\u0026#39;tinting\u0026#39;, false); } function draw() { selectedColor = picker.color(); background(0); coloringShader.setUniform(\u0026#39;texture\u0026#39;, src); coloringShader.setUniform(\u0026#39;color\u0026#39;, [red(selectedColor), green(selectedColor), blue(selectedColor), 1.0]); beginShape(); // format is: vertex(x, y, z, u, v) vertex(-1, -1, 0, 0, 1); vertex(1, -1, 0, 1, 1); vertex(1, 1, 0, 1, 0); vertex(-1, 1, 0, 0, 0); endShape(); } Conclusiones y Trabajo futuro\u003e Conclusiones y Trabajo futuro # Se logra evidenciar que, mediante el uso de shaders, se reduce enormemente el consumo de recursos computacionales en las tareas relacionadas con el uso de texturas, pues la aplicación de las herramientas de coloración y el tinte sobre el video se logran de manera casi inmediata, algo que, bajo un enfoque que no emplea shaders, requeriría aplicar las operaciones pixel a pixel, fotograma por fotograma, causando latencia y otros problemas; en este caso, eso no sucede. Como trabajo futuro, se podrían implementar los otros tipos de mezcla para realizar el texture tinting, pues sólo se hace mediante \u0026ldquo;MULTIPLY\u0026rdquo; para esta aplicación. También, sería interesante aplicar las herramientas de coloración y tinte en texturas sobre objetos 3D. Referencias\u003e Referencias # [1] J. P. Charalambos, \u0026ldquo;Shaders: Texturing\u0026rdquo;. Visual Computing, Apr. 2023. https://visualcomputing.github.io/docs/shaders/texturing/\u003e [1] J. P. Charalambos, \u0026ldquo;Shaders: Texturing\u0026rdquo;. Visual Computing, Apr. 2023. https://visualcomputing.github.io/docs/shaders/texturing/ # [2] FITRIYAH, Hurriyatul; WIHANDIKA, Randy Cahya. An analysis of rgb, hue and grayscale under various illuminations. En 2018 International Conference on Sustainable Information Engineering and Technology (SIET). IEEE, 2018. p. 38-41.\u003e [2] FITRIYAH, Hurriyatul; WIHANDIKA, Randy Cahya. An analysis of rgb, hue and grayscale under various illuminations. En 2018 International Conference on Sustainable Information Engineering and Technology (SIET). IEEE, 2018. p. 38-41. # [3] Michael W. Schwarz, William B. Cowan, and John C. Beatty (April 1987). \u0026ldquo;An experimental comparison of RGB, YIQ, LAB, HSV, and opponent color models.\u0026rdquo; ACM Transactions on Graphics 6(2): 123–158.\u003e [3] Michael W. Schwarz, William B. Cowan, and John C. Beatty (April 1987). \u0026ldquo;An experimental comparison of RGB, YIQ, LAB, HSV, and opponent color models.\u0026rdquo; ACM Transactions on Graphics 6(2): 123–158. # ","date":"1 January 0001","permalink":"/showcase/second/texturing/","section":"Seconds","summary":"Texturing\u003e Texturing # Introducción\u003e Introducción # En esta sección se abordará el problema de texture sampling, también conocido como mapeo de texturas o filtrado de texturas, el cual es una técnica utilizada en gráficos por computadora para aplicar texturas a superficies u objetos 3D.","title":""},{"content":"Juan José Peña Becerra\u003e Juan José Peña Becerra # Estudiante de Ingenieria de Sistemas y Computación de la UNAL, apasionado por el desarrollo de aplicaciones audiovisuales. Me gusta la interactividad en los medios, la experimentación y la creación de contenido para nuevas y mejores experiencias. Un poco adicto al tooling y la optimización de los sistemas en todas las fases. Apoyo y utilizo preferiblemente herramientas open source.\nMis intereses:\u003e Mis intereses: # Desarrollo de videojuegos. Animación y computación visual. Inteligencia arificial. Idiomas. ","date":"1 January 0001","permalink":"/showcase/team/jjpb/","section":"Teams","summary":"Juan José Peña Becerra\u003e Juan José Peña Becerra # Estudiante de Ingenieria de Sistemas y Computación de la UNAL, apasionado por el desarrollo de aplicaciones audiovisuales.","title":""},{"content":"Juan Pablo Bustamante Moreno\u003e Juan Pablo Bustamante Moreno # Desde el primer semestre de 2019 me encuentro estudiando Ingeniería de Sistemas y Computación en la Universidad Nacional de Colombia. La disciplina y la proactividad caracterizan mi trabajo académico. A nivel personal, soy apasionado por el consumo y la creación de contenido de ficción, tanto a nivel literario como cinematográfico: en particular, siento afinidad por las obras de terror, misterio, sátira y surrealismo.\nMis intereses:\u003e Mis intereses: # Desarrollo FrontEnd. Análisis de lenguajes de programación. Inteligencia artificial. Ciberseguridad. Arquitectura de Software. ","date":"1 January 0001","permalink":"/showcase/team/jpbm/","section":"Teams","summary":"Juan Pablo Bustamante Moreno\u003e Juan Pablo Bustamante Moreno # Desde el primer semestre de 2019 me encuentro estudiando Ingeniería de Sistemas y Computación en la Universidad Nacional de Colombia.","title":""},{"content":"Nicolás Romero Niño\u003e Nicolás Romero Niño # Hola, mi nombre es Nicolás Romero Niño. Soy estudiante de ingeniería de sistemas y computación de la Universidad Nacional de Colombia desde el 2019. Durante estos años en la universidad he sido una persona muy responsable y trabajadora, lo cual me ha permitido avanzar sin mayores inconvenientes en la carrera. A nivel personal, soy una persona social que disfruta mucho del tiempo compartido con la familia y amigos, además de disfrutar de actividades físicas como bailar o hacer deporte.\nMis intereses:\u003e Mis intereses: # Desarrollo web. Inteligencia artificial. Videojuegos. Bailar. Fútbol y otros deportes. Salir a comer. ","date":"1 January 0001","permalink":"/showcase/team/nrn/","section":"Teams","summary":"Nicolás Romero Niño\u003e Nicolás Romero Niño # Hola, mi nombre es Nicolás Romero Niño.","title":""},{"content":"","date":"1 January 0001","permalink":"/showcase/authors/","section":"Authors","summary":"","title":"Authors"},{"content":"","date":"1 January 0001","permalink":"/showcase/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":"1 January 0001","permalink":"/showcase/first/","section":"Firsts","summary":"","title":"Firsts"},{"content":"","date":"1 January 0001","permalink":"/showcase/second/","section":"Seconds","summary":"","title":"Seconds"},{"content":"","date":"1 January 0001","permalink":"/showcase/series/","section":"Series","summary":"","title":"Series"},{"content":"","date":"1 January 0001","permalink":"/showcase/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"","date":"1 January 0001","permalink":"/showcase/team/","section":"Teams","summary":"","title":"Teams"}]