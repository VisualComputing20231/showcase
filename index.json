[{"content":"En esta página web se alojan los reportes académicos de los ejercicios desarrollados por el equipo para el curso (semestre 2023-I).\n→ Video correspondiente a la primera entrega ←\n","date":"1 January 0001","permalink":"/showcase/","section":"","summary":"En esta página web se alojan los reportes académicos de los ejercicios desarrollados por el equipo para el curso (semestre 2023-I).","title":""},{"content":"Coloring\u003e Coloring # Introducción\u003e Introducción # En esta sección se abordará el problema de \"remappear\" o reasignar colores a una imagen para facilitar su visualizacion por parte de personas con visibilidad reducida. Color Blindness\u003e Color Blindness # Marco Teórico\u003e Marco Teórico # Cerca de 1 de cada 20 personas son daltónicas, lo cual significa que tienen problemas para distinguir algunos colores [1].\nEl ojo humano tiene dos tipos de receptores: conos y bastones. Los bastones detectan la luz y la oscuridad, mientras que los conos detectan los colores correspondientes a las longitudes de onda de la luz roja, verde y azul. El daltonismo está relacionado con el tipo de cono que tiene problemas [2]:\nProtanopia: los conos de onda larga (rojos) no funcionan o están ausentes. Protanomalía: los conos de onda larga funcionan parcialmente. Deuteranopia: los conos de onda media (verdes) no funcionan o están ausentes. Deuteranomalía: los conos de onda media funcionan parcialmente. Tritanopia: los conos de onda corta (azules) no funcionan o están ausentes. Tritanomalía: los conos de onda corta funcionan parcialmente. Acromatopsia: ningún cono o solo un tipo de cono funciona. Técnicamente, es posible que una persona tenga más de tres tipos de conos, lo que significa que todos somos daltónicos en cierto sentido.\nPara personas con daltonismo de tipo protanopia es difícil distinguir entre el rojo y el verde. Esto se debe a que el sistema visual de estas personas no puede distinguir entre estos colores. Podemos simular esto reemplazando colores en una imagen para crear líneas de confusión y, así, verla tal como la vería alguien con daltonismo.\nPara ello, se diseñó una aplicacion que permite modificar los colores de las imágenes de un video por medio de RGB para que, primeramente, se imite la dificultad para distinguir colores de una imagen y, posteriormente, esta pueda modificarse para facilitar el uso a personas con discapacidad visual.\nInstrucciones\u003e Instrucciones # Haga click o arrastre un video para que la aplicación lo cargue. Puede utilizar la página [3] para descargar un archivo de prueba: se recomienda emplear un mp4 con dimensiones 360 x 240, dado que esta configuración se ajusta mejor a las dimensiones del canvas. Si lo desea, puede acceder al ejemplo en dicho formato específico siguiendo este enlace.\nEl video iniciará automáticamente sin ningún filtro. Luego, podrá añadair hasta cuatro copias del video con diferentes filtros para comparar.\nManipule los canales con las teclas \u0026lsquo;r\u0026rsquo;, \u0026lsquo;g\u0026rsquo;, \u0026lsquo;b\u0026rsquo; para ver cómo se ven (aproximadamente) los colores para personas con daltonismo.\nMantenga presionado la tecla SHIFT y mueva el rodillo del ratón (o deslice dos dedos en su touchpad) para aumentar o disminuir el valor del canal seleccionado.\nOprima la tecla \u0026rsquo;s\u0026rsquo; para guardar un pantallazo de los filtros activos en formato jpg.\nCódigo y Resultados\u003e Código y Resultados # Este ejercicio se desarrolla haciendo uso de la librería p5.js, la cual permite la creación de aplicaciones web interactivas y de código abierto. Se utiliza, principalmente, la funcionalidad de tomar frame por frame de un video y manipularlos para crear una nueva imagen con el método onSeek que se ejeuta en cada frame y hace una pausa determinada.\nLa principal funcionalidad es manipular los píxeles de la imagen actual y reemplazarlos por los píxeles de la imagen duplicada. Para ello, se utiliza el método loadPixels() que carga los píxeles de la imagen actual y el método updatePixels() que actualiza los píxeles de la imagen actual con los píxeles cargados.\nSe puede volver a cargar un nuevo video arrastrándolo o haciendo click en la imagen. Además, está disponible el botón de reiniciar el video desde el principio.\nSe dispone de comandos por teclado para guardar un pantallazo de todos los filtros activos, así como las teclas para cambiar los valores de los canales RGB.\nCódigo completo let video; let time = 0; let loaded = false; let selected = false; let finished = false; let reseted = false; let filters = [false, false, false, false]; let button1, button2, button3, button4; let rgb = 0; let r1, g1, b1, r2, g2, b2, r3, g3, b3; function setup() { let c = createCanvas(400, 400); // Twice the width of the video c.drop(processFile); c.mouseClicked(openFile); button1 = createButton(\u0026#34;Copy original image here\u0026#34;); button1.style(\u0026#34;background-color\u0026#34;, \u0026#34;rgba(0,0,0,0)\u0026#34;); button1.style(\u0026#34;color\u0026#34;, \u0026#34;#006699\u0026#34;); button1.style(\u0026#34;border\u0026#34;, \u0026#34;none\u0026#34;); button1.style(\u0026#34;font-size\u0026#34;, \u0026#34;20px\u0026#34;); button1.style(\u0026#34;font-weight\u0026#34;, \u0026#34;bold\u0026#34;); button2 = createButton(\u0026#34;Copy original image here\u0026#34;); button2.style(\u0026#34;background-color\u0026#34;, \u0026#34;rgba(0,0,0,0)\u0026#34;); button2.style(\u0026#34;color\u0026#34;, \u0026#34;#006699\u0026#34;); button2.style(\u0026#34;border\u0026#34;, \u0026#34;none\u0026#34;); button2.style(\u0026#34;font-size\u0026#34;, \u0026#34;20px\u0026#34;); button2.style(\u0026#34;font-weight\u0026#34;, \u0026#34;bold\u0026#34;); button3 = createButton(\u0026#34;Copy original image here\u0026#34;); button3.style(\u0026#34;background-color\u0026#34;, \u0026#34;rgba(0,0,0,0)\u0026#34;); button3.style(\u0026#34;color\u0026#34;, \u0026#34;#006699\u0026#34;); button3.style(\u0026#34;border\u0026#34;, \u0026#34;none\u0026#34;); button3.style(\u0026#34;font-size\u0026#34;, \u0026#34;20px\u0026#34;); button3.style(\u0026#34;font-weight\u0026#34;, \u0026#34;bold\u0026#34;); button4 = createButton(\u0026#34;RESET\u0026#34;); button4.style(\u0026#34;background-color\u0026#34;, \u0026#34;rgba(0,0,0,0)\u0026#34;); button4.style(\u0026#34;color\u0026#34;, \u0026#34;#F00\u0026#34;); button4.style(\u0026#34;font-size\u0026#34;, \u0026#34;10px\u0026#34;); button4.style(\u0026#34;font-weight\u0026#34;, \u0026#34;bold\u0026#34;); button1.hide(); button2.hide(); button3.hide(); button4.hide(); (r1 = 1), (g1 = 1), (b1 = 1), (r2 = 1), (g2 = 1), (b2 = 1), (r3 = 1), (g3 = 1), (b3 = 1); } function processFile(file) { selected = true; noLoop(); // noCanvas(); if (file.type === \u0026#34;video\u0026#34;) { video = createVideo(file.data, () =\u0026gt; { loaded = true; video.volume(0); video.hide(); const drawNextFrame = () =\u0026gt; { // Only draw the image to the screen when the video // seek has completed const onSeek = () =\u0026gt; { draw(); // All the drawing code goes here video.elt.removeEventListener(\u0026#34;seeked\u0026#34;, onSeek); // Wait a 100 milisecond and draw the next frame setTimeout(drawNextFrame, 100); // TODO: Can be adjusted with keypress }; video.elt.addEventListener(\u0026#34;seeked\u0026#34;, onSeek); // Start seeking ahead video.time(time); // Seek ahead to the new time if (reseted) { reseted = false; time = 0; } time += 1 / 60; }; drawNextFrame(); video.onended(() =\u0026gt; { // When the video ends finished = true; }); }); } } function openFile() { let inputbtn = createFileInput(processFile); inputbtn.remove(); inputbtn.elt.click(); //Simulate Click } function printCurrentFrame() { fill(\u0026#34;#F00\u0026#34;); noStroke(); textAlign(LEFT, TOP); textSize(width / 40); text(\u0026#34;Frame: \u0026#34; + round(time * 60), width / 8, 5); } function copyImage(im, x, y, r, g, b) { let img = createImage(im.width, im.height); img.copy(im, 0, 0, im.width, im.height, 0, 0, im.width, im.height); img.loadPixels(); let d = pixelDensity(); let fullImage = 4 * (width * d) * (height * d); for (let i = 0; i \u0026lt; fullImage; i += 4) { img.pixels[i] = img.pixels[i] * r; img.pixels[i + 1] = img.pixels[i + 1] * g; img.pixels[i + 2] = img.pixels[i + 2] * b; img.pixels[i + 3] = img.pixels[i + 3]; } img.updatePixels(); image(img, im.width * x, im.height * y); } function manageButtons() { if (!filters[0]) { button1.show(); button1.position((width * 3) / 4 - (button1.width * 3) / 4, height / 4); button1.style(\u0026#34;font-size\u0026#34;, width / 30 + \u0026#34;px\u0026#34;); button1.style( \u0026#34;color\u0026#34;, random([\u0026#34;#00669933\u0026#34;, \u0026#34;#00669933\u0026#34;, \u0026#34;#00669933\u0026#34;, \u0026#34;#00669933\u0026#34;, \u0026#34;#00669930\u0026#34;]) ); button1.mousePressed(() =\u0026gt; { filters[0] = true; button1.hide(); }); } if (!filters[1]) { button2.show(); button2.position(width / 4 - (button2.width * 3) / 4, (height * 3) / 4); button1.style(\u0026#34;font-size\u0026#34;, width / 30 + \u0026#34;px\u0026#34;); button2.style( \u0026#34;color\u0026#34;, random([\u0026#34;#00669933\u0026#34;, \u0026#34;#00669933\u0026#34;, \u0026#34;#00669933\u0026#34;, \u0026#34;#00669933\u0026#34;, \u0026#34;#00669930\u0026#34;]) ); button2.mousePressed(() =\u0026gt; { filters[1] = true; button2.hide(); }); } if (!filters[2]) { button3.show(); button3.position( (width * 3) / 4 - (button3.width * 3) / 4, (height * 3) / 4 ); button1.style(\u0026#34;font-size\u0026#34;, width / 30 + \u0026#34;px\u0026#34;); button3.style( \u0026#34;color\u0026#34;, random([\u0026#34;#00669933\u0026#34;, \u0026#34;#00669933\u0026#34;, \u0026#34;#00669933\u0026#34;, \u0026#34;#00669933\u0026#34;, \u0026#34;#00669930\u0026#34;]) ); button3.mousePressed(() =\u0026gt; { filters[2] = true; button3.hide(); }); } button4.show(); button4.position(5, 5); button4.style(\u0026#34;font-size\u0026#34;, width / 50 + \u0026#34;px\u0026#34;); button4.style(\u0026#34;color\u0026#34;, random([\u0026#34;#F00\u0026#34;, \u0026#34;#FFF\u0026#34;])); button4.mousePressed(() =\u0026gt; { (r1 = 1), (g1 = 1), (b1 = 1), (r2 = 1), (g2 = 1), (b2 = 1), (r3 = 1), (g3 = 1), (b3 = 1); reseted = true; }); } function draw() { background(20); if (!selected) { let time = millis(); fill(0, 102, 153); textAlign(CENTER, CENTER); textStyle(BOLD); textSize((width / 500) * sin(time / 250) + width / 20); text(\u0026#34;SELECT or DROP video files HERE...\u0026#34;, width / 2, height / 2); } if (loaded \u0026amp;\u0026amp; !finished) { // Draw the original video to the screen resizeCanvas(video.width * 2, video.height * 2); image(video, 0, 0); printCurrentFrame(); manageButtons(); if (filters[0]) { copyImage(video, 1, 0, r1, g1, b1); } if (filters[1]) { copyImage(video, 0, 1, r2, g2, b2); } if (filters[2]) { copyImage(video, 1, 1, r3, g3, b3); } if (keyIsDown(82)) { fill(\u0026#34;#F00\u0026#34;); textAlign(CENTER, CENTER); textStyle(BOLD); text(\u0026#34;R\u0026#34;, video.width, 15); } if (keyIsDown(71)) { fill(\u0026#34;#0F0\u0026#34;); textAlign(CENTER, CENTER); textStyle(BOLD); text(\u0026#34;G\u0026#34;, video.width, 15); } if (keyIsDown(66)) { fill(\u0026#34;#00F\u0026#34;); textAlign(CENTER, CENTER); textStyle(BOLD); text(\u0026#34;B\u0026#34;, video.width, 15); } } } function keyPressed() { if (key == \u0026#34;s\u0026#34;) { saveCanvas(\u0026#34;myCanvas\u0026#34;, \u0026#34;jpg\u0026#34;); } if (key == \u0026#34;r\u0026#34;) { rgb = 0; } if (key == \u0026#34;g\u0026#34;) { rgb = 1; } if (key == \u0026#34;b\u0026#34;) { rgb = 2; } } function mouseWheel(event) { print(event.delta); textAlign(CENTER, CENTER); textStyle(BOLD); if (keyIsDown(SHIFT)) { if (mouseX \u0026gt; video.width \u0026amp;\u0026amp; mouseY \u0026lt; video.height \u0026amp;\u0026amp; filters[0]) { if (rgb == 0) { fill(\u0026#34;#F00\u0026#34;); r1 -= event.delta / 256; if (r1 \u0026lt; 0) { r1 = 0; } else if (r1 \u0026gt; 5) { r1 = 5; } text(\u0026#34;R: \u0026#34; + round(r1, 1), (video.width * 3) / 2, 15); } if (rgb == 1) { fill(\u0026#34;#0F0\u0026#34;); g1 -= event.delta / 256; if (g1 \u0026lt; 0) { g1 = 0; } else if (g1 \u0026gt; 5) { g1 = 5; } text(\u0026#34;G: \u0026#34; + round(g1, 1), (video.width * 3) / 2, 15); } if (rgb == 2) { fill(\u0026#34;#00F\u0026#34;); b1 -= event.delta / 100; if (b1 \u0026lt; 0) { b1 = 0; } else if (b1 \u0026gt; 5) { b1 = 5; } text(\u0026#34;B: \u0026#34; + round(b1, 1), (video.width * 3) / 2, 15); } } if (mouseX \u0026lt; video.width \u0026amp;\u0026amp; mouseY \u0026gt; video.height \u0026amp;\u0026amp; filters[1]) { if (rgb == 0) { fill(\u0026#34;#F00\u0026#34;); r2 -= event.delta / 256; if (r2 \u0026lt; 0) { r2 = 0; } else if (r2 \u0026gt; 5) { r2 = 5; } text(\u0026#34;R: \u0026#34; + round(r2, 1), video.width / 2, video.height + 15); } if (rgb == 1) { fill(\u0026#34;#0F0\u0026#34;); g2 -= event.delta / 256; if (g2 \u0026lt; 0) { g2 = 0; } else if (g2 \u0026gt; 5) { g2 = 5; } text(\u0026#34;G: \u0026#34; + round(g2, 1), video.width / 2, video.height + 15); } if (rgb == 2) { fill(\u0026#34;#00F\u0026#34;); b2 -= event.delta / 256; if (b2 \u0026lt; 0) { b2 = 0; } else if (b2 \u0026gt; 5) { b2 = 5; } text(\u0026#34;B: \u0026#34; + round(b2, 1), video.width / 2, video.height + 15); } } if (mouseX \u0026gt; video.width \u0026amp;\u0026amp; mouseY \u0026gt; video.height \u0026amp;\u0026amp; filters[2]) { if (rgb == 0) { fill(\u0026#34;#F00\u0026#34;); r3 -= event.delta / 256; if (r3 \u0026lt; 0) { r3 = 0; } else if (r3 \u0026gt; 5) { r3 = 5; } text(\u0026#34;R: \u0026#34; + round(r3, 1), (video.width * 3) / 2, video.height + 15); } if (rgb == 1) { fill(\u0026#34;#0F0\u0026#34;); g3 -= event.delta / 256; if (g3 \u0026lt; 0) { g3 = 0; } else if (g3 \u0026gt; 5) { g3 = 5; } text(\u0026#34;G: \u0026#34; + round(g3, 1), (video.width * 3) / 2, video.height + 15); } if (rgb == 2) { fill(\u0026#34;#00F\u0026#34;); b3 -= event.delta / 256; if (b3 \u0026lt; 0) { b3 = 0; } else if (b3 \u0026gt; 5) { b3 = 5; } text(\u0026#34;B: \u0026#34; + round(b3, 1), (video.width * 3) / 2, video.height + 15); } } } } Conclusiones y Trabajo futuro\u003e Conclusiones y Trabajo futuro # Se pueden utilizar otros espacios de color para la manipulación de colores, tales como el espacio de color XYZ o HSV y, adí, concluir cuál es más apropiado para esta tarea. Es posible modificar los valores de los filtros de manera más precisa, y también la velocidad o frames por segundo de la aplicacion. Como trabajo futuro, se podría mejorar la experiencia de usuario para que la aplicación sea más intuitiva y fácil de usar. Así mismo, se plantea reducir el tamaño de los videos para que se puedan visualizar mejor. Adicionalmente, se propone profundizar en funciones de color para la manipulacion de colores en cada pixel y, en lo posible, evitar cambios constantes. Esto implicaría acercarse a un enfoque polinomial o curvas. Es posible utilizar, a manera de filtro inverso, las imágenes que logren confundir a las personas con discapacidad visual en este programa para que sea aplicado a voluntad según se requiera. Finalmente, se plantea la opción de extender la aplicación para que pueda ser utilizada con otros tipos de entrada como imágenes, videos, etc. Referencias\u003e Referencias # [1] D. Nichols, \u0026ldquo;Coloring for Colorblindness\u0026rdquo; [Online]. Available: https://davidmathlogic.com/colorblind/#%23D81B60-%231E88E5-%23FFC107-%23004D40.\u003e [1] D. Nichols, \u0026ldquo;Coloring for Colorblindness\u0026rdquo; [Online]. Available: https://davidmathlogic.com/colorblind/#%23D81B60-%231E88E5-%23FFC107-%23004D40. # [2] ndesmic, \u0026ldquo;Exploring Color Math Through Color Blindness\u0026rdquo; [Online]. Available: https://dev.to/ndesmic/exploring-color-math-through-color-blindness-2m2h.\u003e [2] ndesmic, \u0026ldquo;Exploring Color Math Through Color Blindness\u0026rdquo; [Online]. Available: https://dev.to/ndesmic/exploring-color-math-through-color-blindness-2m2h. # [3] \u0026ldquo;Sample Videos\u0026rdquo; [Online]. Available: https://sample-videos.com.\u003e [3] \u0026ldquo;Sample Videos\u0026rdquo; [Online]. Available: https://sample-videos.com. # ","date":"1 January 0001","permalink":"/showcase/first/coloring/","section":"Firsts","summary":"Coloring\u003e Coloring # Introducción\u003e Introducción # En esta sección se abordará el problema de \"","title":""},{"content":"Depth Perception\u003e Depth Perception # Introducción\u003e Introducción # En esta sección se discute acerca de la percepción de la profundidad y su relación con las claves monoculares (\u0026quot;monocular cues\u0026quot; en inglés). Luego se procede a definir y detallar algunas de ellas.\nActo seguido, se desarrollan dos sketchs en 2D, los cuales, haciendo uso de las claves monoculares descritas en el marco teórico, permiten engañar al ojo para que se perciban como escenas en tres dimensiones.\nFinalmente, se discuten posibles aplicaciones de los conceptos aprendidos, así como el trabajo futuro que se puede llevar a cabo a partir del uso de las claves monoculares.\nMarco Teórico\u003e Marco Teórico # La percepción de la profundidad es la habilidad para percibir la distancia a los objetos en el mundo utilizando el sistema visual y la percepción visual. Es un factor importante en la percepción del mundo en tres dimensiones [1]. La percepción de la profundidad ocurre, principalmente, debido a la estereopsis, un fenómeno propio de la visión binocular. Sin embargo, también existen algunas pistas visuales que favorecen la percepción de la profundidad con un sólo ojo: estas se conocen como claves monoculares.\nA continuación se describen las claves monoculares que se aplicaron en el desarrollo del presente ejercicio:\nParalaje de movimiento: El paralaje es un fenómeno óptico en el que la posición aparente de un objeto parece cambiar cuando se observa desde diferentes ángulos o posiciones. Este efecto es causado por la diferencia en la ubicación de los ojos o la cámara utilizada para observar el objeto.\nEl paralaje es importante en muchas áreas: por ejemplo, en la astronomía, el paralaje se utiliza para medir la distancia de las estrellas y otros objetos celestes; en la fotografía, el paralaje se utiliza para ajustar la posición de la imagen en el visor de la cámara y para obtener una vista previa precisa de la imagen antes de tomarla; y en la medición, el paralaje se utiliza para determinar la profundidad y la distancia de los objetos [2].\nAhora bien, si además de la posición aparente se recibe información acerca del entorno, esto porque el observador está en movimiento, es aquí cuando se empieza a hablar de paralaje de movimiento: este efecto visual hace que los objetos cercanos parezcan moverse más rápido que los objetos más alejados, los cuales parecen estacionarios o, incluso, parecen moverse en la dirección opuesta [3].\nPerspectiva Aérea: La perspectiva aérea, también conocida como perspectiva atmosférica, se refiere al efecto visual en el que los objetos que se encuentran más lejos parecen más borrosos, descoloridos y con menos detalle que los objetos que están más cerca [4].\nEste efecto se produce debido a la interferencia de la atmósfera entre los objetos y el observador.\nPerspectiva: En este contexto, la perspectiva se refiere a la relación entre el tamaño de los objetos y su distancia percibida. Los objetos más alejados parecen más pequeños que los objetos más cercanos debido a la manera en que los rayos de luz se proyectan en el ojo del observador [5].\nEste efecto de los rayos de luz se traslada a líneas que convergen en un punto al infinito, lo cual permite que el observador reconstruya la distancia relativa entre los objetos. También existen otros tipos de perspectiva, como la mencionada anteriormente, la curvilínea y otras con más puntos de fuga. Sin embargo, para este ejercicio sólo se contempló la perspectiva lineal con un punto de fuga, pues, aunada con otras claves monoculares, ayudaba a reforzar la percepción de la profundidad.\nProfundidad a partir de la expansión óptica: La profundidad a partir del movimiento, también conocida como \u0026ldquo;depth from motion\u0026rdquo; en inglés, es una forma de percepción visual que se basa en la observación de la forma en que los objetos se mueven en relación con el observador. A partir de esta información, el cerebro puede determinar la velocidad, la dirección y la aceleración del movimiento de los objetos, lo que le permite calcular la profundidad y la distancia de los objetos en el mundo real [5].\nDentro de la profundidad a partir del movimiento, puede encontrarse una clave monocular aún más específica: la profunidad a partir de la expansión óptica. Esta pista visual es utilizada para inferir la profundidad y la distancia a partir de la información sobre cómo se expande un objeto en la retina, lo que le indica al cerebro que el objeto está más cerca en el espacio [6]. En consencuencia, este fenómeno otorga una sensación de movimiento en dirección al observador debido al cambio de tamaño.\nCódigo y Resultados\u003e Código y Resultados # De las claves monoculares que se detallaron anteriormente, se tomaron las dos primeras para desarrollar un sketch semejante a este gif, el cual es un ejemplo típico al hablar de claves monoculares. El resultado se puede ver a continuación:\nCódigo completo let val1 = []; let val2 = []; let val3 = [] function setup() { createCanvas(400, 400) frameRate(30) for(let i = 0; i \u0026lt; 8; i++){ append(val1, height - random(height/2, height)) append(val2, height - random(height/4, 4*height/5)) append(val3, height - random(20, 3*height/5)) } } function draw() { background(255) for(let i = -1; i \u0026lt;= width; i+=1){ stroke(100,150,255,85) rect(((frameCount + i) % (width+1)), val1[int(i/50)], 0, height) } for(let i = -1; i \u0026lt;= width; i++){ stroke(50,100,200,170) rect(((frameCount*4 + i) % (width+1)), val2[int(i/50)], 0, height) } for(let i = -1; i \u0026lt;= width; i++){ stroke(0,50,100,255) rect(((frameCount*8 + i) % (width+1)), val3[int(i/50)], 0, height) } } Como se puede evidenciar, en primer lugar se asignan unos valores aleatorios entre ciertos rangos para definir las alturas de los \u0026ldquo;edificios\u0026rdquo;. Luego, dentro del draw se definen 3 ciclos for, uno para los edificios delanteros, otro para los que van en medio y otro para los que van atrás. Por ejemplo, se tiene que el ciclo para el último caso mencionado es el siguiente:\nfor(let i = -1; i \u0026lt;= width; i+=1){ stroke(100,150,255,85) rect(((frameCount + i) % (width+1)), val1[int(i/50)], 0, height) } En primer lugar, se define el stroke correspondiente: en este caso es un tono de azul claro con un nivel alto de transparencia. Los edificios del medio tienen un color más oscuro y transparencia más baja, mientras que los edificios delanteros poseen el tono más opaco y sin transparencia. Esto se hace, justamente, para aplicar la clave monocular de perspectiva aérea.\nLuego, se dibujan rectángulos sin ancho (es decir, líneas, pero usa la función rect por practicidad) donde la posición en x está dada por la palabra reservada frameCount: esto se hace para dar la sensación de movimiento y, adicionalmente, se usa el operador módulo con el ancho del canvas para que se repita sucesivamente. Como altura se le da uno de los valores aleatorios asignados en el setup.\nAhora, para la otra clave monocular contenida en esta parte del código, se tiene que en el ciclo for de los edificios posicionados en medio, el valor de frameCount se cuadruplica, mientras que para los edificios posicionados al frente, este valor se multiplica por 8. De tal forma se consigue que, entre más cerca estén al observador, estos se muevan más rápido, lo cual es una aplicación directa del paralaje de movimiento.\nEn cuanto al segundo sketch, se buscó aplicar las otras dos claves monoculares descritas, es decir, la perspectiva y la profundidad a partir de la expansión óptica. Para ello, se tomó como punto de partida el siguiente código, el cual dibuja una carretera estática.\nAprovechando la perspectiva que presentaba, se buscó la manera de animarlo al agrandar el tamaño progresivamente, esto con el fin de dar la sensación de entrada a un túnel. El resultado se presenta a continuación:\nCódigo completo function setup() { createCanvas(600, 400); frameRate(100) rectMode(CENTER) } function draw() { background(80,180,80); road(frameCount % (2*height)) } function road(fc){ fill(140, 180, 240) for(let sky = -10; sky \u0026gt;= -200; sky-=10) { rect(width / 2, sky + fc / 4, 100 * fc, fc / 4) } stroke(\u0026#34;black\u0026#34;) strokeWeight(1) fill(220, 255) triangle(300, -10, 0, 400, 0, 350); fill(220, 255) triangle(300, -10, 600, 400, 600, 350); fill(220, 255) ellipse(300, -10 + fc/10, 63*fc/100); rect(width/2, -10 + fc/4, 63*fc/100, fc/4); fill(0, 255) ellipse(300, -10 + fc/10, 55*fc/100); rect(width/2, -10 + fc/4, 55*fc/100, fc/4); fill(70) triangle(300, -10, 600, 400, 0, 400); fill(155, 200) triangle(300, -10, 570, 400, 30, 400); stroke(255) fill(255) for(let i=15; i\u0026gt;0; i-=2){ if (i \u0026lt;= 1){ i+= 0.3 } quad(width/2 - fc/80, i*fc/5, width/2 + fc/80, i*fc/5, width/2 + fc/200, (i-1)*fc/5, width/2 - fc/200, (i-1)*fc/5) } noStroke() } En síntesis, se probaron varios valores por ensayo y error hasta que la animación fuese fluida. En este caso, el valor frameCount % (2*height) se seleccionó dado que permite ver la animación hasta un punto en el que la figura del túnel ocupa la mayor parte del espacio. Este mismo valor (que dentro la función se denomina como fc) es el que se emplea para ir variando el tamaño del ancho y el alto de los elementos. Por ejemplo, se tiene el siguiente ciclo:\nfor(let i=15; i\u0026gt;0; i-=2){ if (i \u0026lt;= 1){ i+= 0.3 } quad(width/2 - fc/80, i*fc/5, width/2 + fc/80, i*fc/5, width/2 + fc/200, (i-1)*fc/5, width/2 - fc/200, (i-1)*fc/5) } Esta porción de código es la que dibuja las líneas en la carretera, de forma que se ven más grandes entre más cerca están al borde inferior del canvas. En los puntos ubicados a la izquierda del paralelogramo, se resta una razón que lleva la partícula fc, mientras que en los puntos ubicados a la derecha se suma: con esto se logra que el tamaño se amplíe ya que los puntos se mueven en direcciones opuestas a la misma proporción.\nConclusiones y Trabajo futuro\u003e Conclusiones y Trabajo futuro # En síntesis, se puede evidenciar que el uso de claves monoculares puede ser muy útil a la hora de realizar animaciones en 2D, puesto que otorgan sensación de movimiento y profunidad de una manera sencilla. Es evidenciable que, cuando se aplican varias claves monoculares, el efecto de percepción de la profundidad es mayor. A veces, incluso, termina siendo inevitable que se empleen dos o más claves monoculares, tal como ocurre con el segundo sketch: para dar la sensación de que un elemento se acerca, este debe crecer proporcionalmente. Lo anterior conlleva una sensación de perspectiva, dado que si se lleva el trazo de las posiciones del objeto cuyo tamaño cambia, esto dará como resultado un par de líneas que tienden a un punto en el infinito. Como trabajo futuro, se podría hacer uso de otras claves monoculares que, al integrarlas con las presentadas, acrecenten la sensación de percepción de la profundidad. Así mismo, se podrían evaluar las diferencias de percepción entre el ojo izquierdo y el derecho en cuanto a las claves monoculares y, a partir de los resultados, buscar la mejor forma en que dichas claves pueden integrarse en un sketch 2D. Referencias\u003e Referencias # [1] J. P. Charalambos, \u0026ldquo;Depth perception\u0026rdquo;. Visual Computing, Feb. 2023. https://visualcomputing.github.io/docs/visual_illusions/depth_perception/\u003e [1] J. P. Charalambos, \u0026ldquo;Depth perception\u0026rdquo;. Visual Computing, Feb. 2023. https://visualcomputing.github.io/docs/visual_illusions/depth_perception/ # [2] A. E. Roy and D. Clarke, \u0026ldquo;Astronomy: Principles and Practice,\u0026rdquo; 4th ed., CRC Press, 2003.\u003e [2] A. E. Roy and D. Clarke, \u0026ldquo;Astronomy: Principles and Practice,\u0026rdquo; 4th ed., CRC Press, 2003. # [3] B. J. Rogers and M. E. Graham, \u0026ldquo;Motion Parallax as an Independent Cue for Depth Perception,\u0026rdquo; Perception, vol. 8, no. 2, pp. 125-34, 1979. doi: 10.1068/p080125.\u003e [3] B. J. Rogers and M. E. Graham, \u0026ldquo;Motion Parallax as an Independent Cue for Depth Perception,\u0026rdquo; Perception, vol. 8, no. 2, pp. 125-34, 1979. doi: 10.1068/p080125. # [4] J. Murray. \u0026ldquo;Some perspectives on visual depth perception,\u0026rdquo; ACM SIGGRAPH Computer Graphics, vol. 28, no. 2, pp. 155-157, 1994, doi: 10.1145/178951.178985.\u003e [4] J. Murray. \u0026ldquo;Some perspectives on visual depth perception,\u0026rdquo; ACM SIGGRAPH Computer Graphics, vol. 28, no. 2, pp. 155-157, 1994, doi: 10.1145/178951.178985. # [5] S. Schwartz, \u0026ldquo;Visual Perception: A Clinical Orientation (Fourth Edition),\u0026rdquo; McGraw-Hill Education, 1994.\u003e [5] S. Schwartz, \u0026ldquo;Visual Perception: A Clinical Orientation (Fourth Edition),\u0026rdquo; McGraw-Hill Education, 1994. # [6] M.T. Swanston and W.C. Gogel, \u0026ldquo;Perceived size and motion in depth from optical expansion,\u0026rdquo; Perception \u0026amp; Psychophysics, vol. 39, pp. 309-326, 1986. doi: 10.3758/BF03202998.\u003e [6] M.T. Swanston and W.C. Gogel, \u0026ldquo;Perceived size and motion in depth from optical expansion,\u0026rdquo; Perception \u0026amp; Psychophysics, vol. 39, pp. 309-326, 1986. doi: 10.3758/BF03202998. # ","date":"1 January 0001","permalink":"/showcase/first/depth/","section":"Firsts","summary":"Depth Perception\u003e Depth Perception # Introducción\u003e Introducción # En esta sección se discute acerca de la percepción de la profundidad y su relación con las claves monoculares (\u0026quot;monocular cues\u0026quot; en inglés).","title":""},{"content":"Visual illusions\u003e Visual illusions # Introducción\u003e Introducción # En esta sección se presentarán algunos fenómenos visuales y su respectiva implementación; además, se proporcionará una breve explicación junto con la fuente de donde se obtuvo la información. Motion Aftereffect (Waterfall Illusion)\u003e Motion Aftereffect (Waterfall Illusion) # Marco Teórico\u003e Marco Teórico # El fenómeno de la cascada es una ilusión óptica que se produce cuando se observa una cascada o un río en movimiento y luego se mira hacia una superficie estática. La superficie estática parece moverse en la dirección opuesta al flujo original. Este efecto se debe a la adaptación del sistema visual a un estímulo en movimiento que, al desaparecer, produce un efecto residual de movimiento en la dirección opuesta. Este efecto es conocido como \u0026ldquo;inducción de movimiento\u0026rdquo;.\nEste fenómeno ha sido observado desde la antigüedad. Lucrecio, poeta romano del siglo I a.C., describió la ilusión de que las piernas de un caballo estacionado en un río parecían moverse en la dirección opuesta al flujo del agua. El filósofo Aristóteles también hizo referencia a la percepción errónea del movimiento en el agua, aunque en su caso se tratara de la percepción de que objetos estáticos parecían moverse.\nInvestigaciones posteriores han demostrado que el fenómeno de la cascada está mediado por procesos neuronales complejos en el cerebro que procesan y adaptan la percepción visual del movimiento. Estudios modernos han identificado las áreas cerebrales que son responsables de esta adaptación y han ayudado a explicar los mecanismos subyacentes a la ilusión de la cascada.\nEn resumen, la ilusión de la cascada es un fenómeno óptico que se produce cuando se observa una superficie estática después de haber mirado un objeto en movimiento. Este efecto se debe a la adaptación del sistema visual a estímulos en movimiento que persisten durante unos momentos después de que el estímulo ha desaparecido. Este fenómeno ha sido estudiado desde la antigüedad y ha sido objeto de investigación científica moderna, lo que ha ayudado a comprender mejor los mecanismos neuronales que subyacen a la percepción visual del movimiento.\nInstrucciones\u003e Instrucciones # Fíjate en la cruz central durante el movimiento y mira el ciclo al menos tres veces. Observa el efecto de la imagen posterior al movimiento en la figura en reposo (el Buda de Kamakura). La \u0026ldquo;deformación\u0026rdquo; causada por el efecto de la imagen posterior al movimiento se aplica a cualquier cosa que observes. También puedes intentar cubrir un ojo, adaptarte durante aproximadamente tres ciclos y luego probar con el otro ojo.\nEsto se explica, a menudo, en términos de \u0026ldquo;fatiga\u0026rdquo; de la clase de neuronas que codifican una dirección de movimiento. Sin embargo, es más preciso interpretar esto en términos de adaptación o \u0026ldquo;control de ganancia\u0026rdquo;. Estos detectores de movimiento no se encuentran en la retina, sino en el cerebro [1]. Para obtener una explicación más detallada y una demostración interesante del \u0026ldquo;efecto cascada\u0026rdquo;, consulte la página de George Mather.\nWaterfall Illusion Código y Resultados\u003e Código y Resultados # En este ejercicio se desarrolla el efecto visual de la cascada. Para lograr la sensación de movimiento, se dibujan múltiples áreas a distintos radios, determinados por la cantidad de anillos (rings). Luego, se almacenan en un array ringpos[] en orden del más grande al más pequeño y, por último, se les asigna un color blanco o negro alternadamente. También hay que notar que la función está restringida por el módulo (%) de un radio establecido, por tanto, una vez que el radio de alguna área llega al límite (si se expande), este se devolverá al radio mínimo.\nfor (let i = 0; i \u0026lt; rings; i++) { cursize = (frameCount * speed + (size / rings) * i) % size; if (cursize == 0) { inverted = !inverted; exterior = !exterior; } if (reversed) cursize = size - cursize; ringpos[i] = cursize; } En este caso no se dibujan círculos completos, sino más bien arcos con un ángulo definido por la cantidad de ángulos angles y los radios previamente calculados. Luego, se rellenan las áreas de estos arcos cambiando su radio en cada frame. De esta manera, se logra el efecto de movimiento y el estilo cuadriculado de cada anillo.\nfor (let i = 0; i \u0026lt; rings; i++) { inverted = !inverted; for (let j = 0; j \u0026lt; angles; j++) { noStroke(); push(); if (j % 2 == 0) { if (inverted) fill(255); else fill(0); } else { if (inverted) fill(0); else fill(255); } arc( 200, 200, ringpos[i], ringpos[i], (2 * PI * j) / angles, (2 * PI * (j + 1)) / angles ); pop(); } } Según el frame rate, se actualiza el radio de cada arco presente, de manera que se expande o se colapsa según lo que determine la variable reversed, que cambiará su valor de verdad presionando el respectivo botón. Para lograr que el efecto se muestre correctamente, la duración del movimiento es mucho mayor a la duración de la imagen mostrada (cerca de 3 veces más largo).\nPor último, se dibuja la imagen original en el centro de la pantalla. Esta imagen se carga previamente en la función preload() y se almacena en la variable originalImage. Esta imagen se muestra o no dependiendo del valor de la variable showImage, que se cambia cada cierto tiempo dependiendo de la variable delayTime y el frame actual.\nif (frameCount % delayTime == 0) showImage = !showImage; if (showImage) { delayTime = 200; image(originalImage, 0, 0, 400, 400); } else { noFill(); stroke(0); erase(0,255); strokeWeight(100); circle(200,200, 500); noErase(); delayTime = 600; } El código completo se encuentra en el siguiente desplegable:\nCódigo completo let fr = 30; let img = null; let delayTime = 200; let size = 560; let angles = 12; let speed = 4; let anchor = 100; let rings = 14; let inverted = false; let exterior = false; let reversed = false; let showImage = false; let button; function preload() { originalImage = loadImage(\u0026#34;../assets/buddha.jpg\u0026#34;); }; function setup() { var canvas = createCanvas(400, 400); canvas.parent(\u0026#34;waterfall-illusion\u0026#34;); frameRate(fr); button = createButton(\u0026#39;Collapse\u0026#39;); button.parent(\u0026#34;waterfall-illusion\u0026#34;); button.position(0, 0, \u0026#39;sticky\u0026#39;); button.mousePressed(() =\u0026gt; { button.elt.innerText = reversed ? \u0026#39;Collapse\u0026#39; : \u0026#39;Expand\u0026#39;; reversed = !reversed} ); }; function draw() { background(128); var cursize; noStroke(); for (let j = 0; j \u0026lt; angles; j++) { push(); if (j % 2 == 0) { if (exterior) fill(255); else fill(0); } else { if (exterior) fill(0); else fill(255); } arc( 200, 200, size, size, (2 * PI * j) / angles, (2 * PI * (j + 1)) / angles ); pop(); } let ringpos = []; for (let i = 0; i \u0026lt; rings; i++) { cursize = (frameCount * speed + (size / rings) * i) % size; if (cursize == 0) { inverted = !inverted; exterior = !exterior; } if (reversed) cursize = size - cursize; ringpos[i] = cursize; } ringpos.sort(function (a, b) { return b - a; }); for (let i = 0; i \u0026lt; rings; i++) { inverted = !inverted; for (let j = 0; j \u0026lt; angles; j++) { noStroke(); push(); if (j % 2 == 0) { if (inverted) fill(255); else fill(0); } else { if (inverted) fill(0); else fill(255); } arc( 200, 200, ringpos[i], ringpos[i], (2 * PI * j) / angles, (2 * PI * (j + 1)) / angles ); pop(); } } fill(0, 0, 255); circle(200, 200, 30); fill(255, 0, 0); rect(199, 192, 2, 16); rect(192, 199, 16, 2); if (frameCount % delayTime == 0) showImage = !showImage; if (showImage) { delayTime = 200; image(originalImage, 0, 0, 400, 400); } else { noFill(); stroke(0); erase(0,255); strokeWeight(100); circle(200,200, 500); noErase(); delayTime = 600; } }; Conclusiones y Trabajo futuro\u003e Conclusiones y Trabajo futuro # Es posible mejorar el programa para que sea modificable en términos de velocidad, cantidad de anillos y/o ángulos, tamaño de la imagen, etc. Como trabajo futuro, se pueden investigar e implementar más fenómenos visuales. Referencias\u003e Referencias # [1] M. Bach, \u0026ldquo;Motion Aftereffect (Waterfall Illusion)\u0026rdquo; [Online]. Available: https://michaelbach.de/ot/mot-adapt/index.html.\u003e [1] M. Bach, \u0026ldquo;Motion Aftereffect (Waterfall Illusion)\u0026rdquo; [Online]. Available: https://michaelbach.de/ot/mot-adapt/index.html. # ","date":"1 January 0001","permalink":"/showcase/first/illusions/","section":"Firsts","summary":"Visual illusions\u003e Visual illusions # Introducción\u003e Introducción # En esta sección se presentarán algunos fenómenos visuales y su respectiva implementación; además, se proporcionará una breve explicación junto con la fuente de donde se obtuvo la información.","title":""},{"content":"Temporal Coherence\u003e Temporal Coherence # Introducción\u003e Introducción # En esta sección se discute, en primer lugar, acerca del fenómeno de coherencia temporal y la estrecha relación que guarda con las animaciones y, en particular, con el uso de fotogramas clave (keyframes).\nDespués, a manera de ejercicio práctico, se desarrolla una animación por keyframes empleando la librería nub en el lenguaje Processing. Para ello, se realiza un estudio detallado de esta, con el fin de comprender su funcionamiento y generar, así, un producto donde el posicionamiento de sus fotogramas guarde una intención determinada: es decir, donde este no se lleve a cabo de manera aleatoria.\nFinalmente, se discuten posibles aplicaciones de los conceptos aprendidos, así como el trabajo futuro que se puede llevar a cabo tomando el ejercicio desarrollado como punto de partida.\nMarco Teórico\u003e Marco Teórico # La coherencia temporal es un fenómeno visual presente en toda la naturaleza: de acuerdo con la teoría, un color percibido de un punto en una región de interés tiende a variar más según el tiempo transcurrido entre dos momentos dados [1].\nPor otra parte, en animación y gráficos por computadora, se tiene que los keyframes son un conjunto de fotogramas que representan los momentos clave de una animación. Los keyframes son los fotogramas en los que se definen los cambios en la posición, la rotación, la escala, la opacidad y otros atributos de los objetos en la animación. En general, suelen utilizarse algoritmos de interpolación para crear fotogramas intermedios que conecten los keyframes [2].\nCon base en lo anterior, los keyframes son esenciales para la coherencia temporal de la animación, pues se relacionan directamente con su capacidad para mantener una secuencia de fotogramas que sea suave y coherente a lo largo del tiempo. Si los keyframes están mal ubicados o son inconsistentes, la animación puede tener errores de coherencia temporal, como saltos o sacudidas entre fotogramas.\nUna vez que es claro el concepto, entra a colación el uso de la libería nub. Esta es de código abierto y fue cuenta con marcos de interacción, visualización, animación, y admite técnicas avanzadas de renderizado (tanto en como fuera de pantalla): por ejemplo, el recorte de frustum de visualización [3], que se refiere a la visualización de una escena desde diferentes ángulos empleando una cámara vortual, tal como ocurre en el presente ejercicio.\nA grandes rasgos, para desarrollar una animación empleando esta librería se debe instaciar la escena y los nodos. Luego de las configuraciones iniciales, el proceso consiste en ir variando la posición, la orientación, la escala y demás atributos entre cada keyframe: esto es lo que permite que los nodos se muevan dentro de la escena. Al ojo de la escena también se le pueden aplicar las mismas transformaciones, lo cual da la sensación de movimiento de la cámara al desplegar su animación por fotogramas clave.\nLa posición se establece mediante vectrores de tres dimensiones, los cuales dan las coordenadas para los ejes x, y y z en el plano.\nLa rotación se establece mediante cuaterniones: estos elementos matemáticos, que fueron descubiertos por William Rowan Hamilton en 1843, constan de cuatro componentes: una parte real y tres partes imaginarias [4]. Se pueden escribir como q = w + xi + yj + zk, donde w, x, y y z son números reales, e i, j y k son unidades imaginarias que satisfacen las siguientes relaciones:\ni² = j² = k² = ijk = -1\nSe tiene que, a diferencia de los números complejos, los cuaterniones poseen tres partes imaginarias Esto permite una representación más completa de las rotaciones tridimensionales que los números complejos no pueden proporcionar, ya que sólo poseen dos partes imaginarias.\nCódigo y Resultados\u003e Código y Resultados # A continuación, se describe el ejercicio desarrollado y se destacan las piezas de código más relevantes, las cuales demuestran los aportes efectuados sobre el ejemplo disponible. Para esta sección en particular, se adjunta un vídeo de demostración, dado que este ejercicio fue hecho en Processing y no en P5js.\nComo se puede evidenciar, el ejercicio desarrollado consiste en una animación que muestra un bosquejo de la rotación de la Tierra, al igual que tanto la rotación como la traslación de su satélite (la Luna). Haciendo uso del mouse, se pueden alterar los keyframes de la Luna lo que, en consecuencia, altera la rotación natural que trae por defecto; también se puede mover el ángulo de la cámara.\nPara la construcción de los objetos (Tierra y Luna), se crearon los nodos como esferas y se texturizaron a partir de imágenes en línea como se evidencia a continuación:\nearth = loadImage(\u0026#34;https://b3d.interplanety.org/wp-content/upload_content/2016/08/01-3.jpg\u0026#34;); moon = loadImage(\u0026#34;https://svs.gsfc.nasa.gov/vis/a000000/a004700/a004720/lroc_color_poles_1k.jpg\u0026#34;); (...) pshape = createShape(SPHERE, 80); pshape2 = createShape(SPHERE, 20); pshape.setTexture(earth); pshape2.setTexture(moon); Para definir los movimientos de rotación y traslación se definió el siguiente ciclo for:\nshape2.setPosition(120,0,0); int x = 40; int y = 80; for (int i = 0; i \u0026lt; 8; i++) { shape.addKeyFrame(Node.AXES | Node.SHAPE, 4000); shape.rotate(new Quaternion(0,1,0,0)); shape2.addKeyFrame(Node.AXES | Node.SHAPE | Node.HUD, 2000); shape2.translate(x*(i \u0026lt; 4 ? -1 : 1), 0, y*(i \u0026gt; 1 \u0026amp;\u0026amp; i \u0026lt; 6 ? -1 : 1)); shape2.rotate(new Quaternion(0,1,0,0)); if (i % 2 == 0){ int tmp = x; x = y; y = tmp; } } Aquí se evidencia cómo se instancia un cuaternión cuyo único valor distinto de cero es el segundo a la hora de rotar la Tierra: con ello se consigue que rote únicamente alrededor del eje y. En cuanto a la Luna, aparte de la rotación (que sigue la misma lógica), se lleva a cabo una traslación definida por condiciones.\nPara lograr lo anterior, se estudiaron, mediante ensayo y error, configuraciones matemáticas que dieran la sensación de movimiento circular (pues en realidad sólo se desplaza en línea recta). Entonces, se definieron 8 posiciones (adelante, atrás, izquierda, derecha y las 4 diagonales). La Luna arranca en la posición (120,0,0), es decir, a la derecha de la Tierra, y se desplaza en relación 80 y 40 en los ejes x y y para ocupar cada una de las posiciones subsecuentes.\nEs así que se inicializan dos variables con dichos valores y, cada vez que el contador del ciclo es par, estos valores se intercambian. La primera coordenada siempre será negativa cuando i \u0026lt; 4, porque será cuando la Luna se desplace hacia la izquierda; después de ello será positiva porque se desplazará a la derecha. El mismo razonamiento aplica cuando se desplaza hacia atrás y hacia adelante: el primer caso se dará desde la segunda iteración hasta la sexta y el otro se dará en los escenarios opuestos.\nPor otra parte, algunas teclas permiten alterar los keyframes del ojo de la escena. Dicha configuración está dada de la siguiente manera:\nVista frontal (FRONT): tecla \u0026ldquo;F\u0026rdquo;. Vista posterior (BACK): tecla \u0026ldquo;B\u0026rdquo;. Vista lateral izquierda (LEFT): tecla \u0026ldquo;L\u0026rdquo;. Vista lateral derecha (RIGHT): tecla \u0026ldquo;R\u0026rdquo;. Vista superior (UP): tecla \u0026ldquo;U\u0026rdquo;. Vista inferior (DOWN): tecla \u0026ldquo;D\u0026rdquo;. Esto se consigue a partir de la función keyPressed(), la cual se detalla a continuación:\nvoid keyPressed() { if (key == \u0026#39; \u0026#39;) { shape2.toggleHint(Node.KEYFRAMES); } else { scene.eye().removeKeyFrames(); scene.eye().addKeyFrame(Node.CAMERA | Node.BULLSEYE, 1000); if (key == \u0026#39;u\u0026#39;) { scene.eye().setPosition(0,-300,0); scene.eye().setOrientation(1,0,0,1); } if (key == \u0026#39;l\u0026#39;) { scene.eye().setPosition(-300,0,0); scene.eye().setOrientation(0,-1,0,1); } if (key == \u0026#39;r\u0026#39;) { scene.eye().setPosition(300,0,0); scene.eye().setOrientation(0,1,0,1); } if (key == \u0026#39;d\u0026#39;) { scene.eye().setPosition(0,300,0); scene.eye().setOrientation(-1,0,0,1); } if (key == \u0026#39;b\u0026#39;) { scene.eye().setPosition(0,0,-300); scene.eye().setOrientation(0,1,0,0); } if (key == \u0026#39;f\u0026#39;) { scene.eye().setPosition(0,0,300); scene.eye().setOrientation(0,0,0,0); } scene.eye().addKeyFrame(Node.CAMERA | Node.BULLSEYE, 1000); scene.eye().animate(); } } Se puede ver que la cámara toma una distancia de 300 respecto al centro para poder apreciar toda la animación: este valor se ubica en la posición correspondiente a la vista que se desea (por ejemplo, para ver desde arriba, se ubica su negativo en la posición y). Para la rotación se indica sobre qué eje se desea el movimiento de la cámara y los demás ejes se dejan en cero; en cuanto al cuatro valor, cuando este equivale a 1, el giro resulta ser de 90°, mientras que si equivale a 0, el giro resulta ser de 180°.\nEl código completo se encuentra en el siguiente desplegable y, de manera posteior, se encuentran dos botones en caso de que se desee descargar el programa como un archivo ejecutable, dependiendo del sistema operativo, para interactuar con este desde la máquina local.\nCódigo completo import nub.primitives.*; import nub.core.*; import nub.processing.*; Scene scene; Node shape; Node shape2; String renderer = P3D; float speed = 2; PImage night = loadImage(\u0026#34;https://images.hdqwalls.com/download/dark-starry-sky-stars-4k-9m-2560x1700.jpg\u0026#34;); void setup() { size(1000, 800, renderer); scene = new Scene(this, 150); PShape pshape; PShape pshape2; PImage earth; PImage moon; earth = loadImage(\u0026#34;https://b3d.interplanety.org/wp-content/upload_content/2016/08/01-3.jpg\u0026#34;); moon = loadImage(\u0026#34;https://svs.gsfc.nasa.gov/vis/a000000/a004700/a004720/lroc_color_poles_1k.jpg\u0026#34;); night.resize(1000,800); noStroke(); pshape = createShape(SPHERE, 80); pshape2 = createShape(SPHERE, 20); pshape.setTexture(earth); pshape2.setTexture(moon); shape = new Node(pshape); shape2 = new Node(pshape2); shape.setMinMaxScalingFilter(0.5, 1.0); shape2.setMinMaxScalingFilter(0.5, 1.0); scene.eye().disableHint(Node.KEYFRAMES); shape.disableHint(Node.KEYFRAMES); shape2.enableHint(Node.KEYFRAMES, Node.AXES, 1, color(255, 255, 255), 2); shape.setAnimationRecurrence(true); shape2.setAnimationRecurrence(true); shape2.setPosition(120,0,0); int x = 40; int y = 80; for (int i = 0; i \u0026lt; 8; i++) { shape.addKeyFrame(Node.AXES | Node.SHAPE, 4000); shape.rotate(new Quaternion(0,1,0,0)); shape2.addKeyFrame(Node.AXES | Node.SHAPE | Node.HUD, 2000); shape2.translate(x*(i \u0026lt; 4 ? -1 : 1), 0, y*(i \u0026gt; 1 \u0026amp;\u0026amp; i \u0026lt; 6 ? -1 : 1)); shape2.rotate(new Quaternion(0,1,0,0)); if (i % 2 == 0){ int tmp = x; x = y; y = tmp; } } shape2.addKeyFrame(Node.AXES | Node.SHAPE | Node.HUD, 2000); shape.resetScalingFilter(); shape2.resetScalingFilter(); shape.animate(); shape2.animate(); } void draw() { background(night); scene.render(); } void keyPressed() { if (key == \u0026#39; \u0026#39;) { shape2.toggleHint(Node.KEYFRAMES); } else { scene.eye().removeKeyFrames(); scene.eye().addKeyFrame(Node.CAMERA | Node.BULLSEYE, 1000); if (key == \u0026#39;u\u0026#39;) { scene.eye().setPosition(0,-300,0); scene.eye().setOrientation(1,0,0,1); } if (key == \u0026#39;l\u0026#39;) { scene.eye().setPosition(-300,0,0); scene.eye().setOrientation(0,-1,0,1); } if (key == \u0026#39;r\u0026#39;) { scene.eye().setPosition(300,0,0); scene.eye().setOrientation(0,1,0,1); } if (key == \u0026#39;d\u0026#39;) { scene.eye().setPosition(0,300,0); scene.eye().setOrientation(-1,0,0,1); } if (key == \u0026#39;b\u0026#39;) { scene.eye().setPosition(0,0,-300); scene.eye().setOrientation(0,1,0,0); } if (key == \u0026#39;f\u0026#39;) { scene.eye().setPosition(0,0,300); scene.eye().setOrientation(0,0,0,0); } scene.eye().addKeyFrame(Node.CAMERA | Node.BULLSEYE, 1000); scene.eye().animate(); } } void mouseDragged() { if (mouseButton == LEFT) scene.spin(); else if (mouseButton == RIGHT) scene.shift(); } void mouseMoved() { scene.updateTag(); } Conclusiones y Trabajo futuro\u003e Conclusiones y Trabajo futuro # El uso de la libería nub permite una comprensión más fácil y práctica del concepto de los cuaterniones, pues se logró evidenciar que las tres primeras posiciones indicaban sobre cuál de los 3 ejes (x, y o z) se hacía la rotación, mientras que el último valor permitía variar el ángulo. Aunque es posible generar una animación aleatorizando las posiciones, rotaciones y escalas de cada keyframe, se pudo comprobar que, con un estudio minucioso del manejo de vectores y cuaterniones, el programador puede definir con facilidad algunas transformaciones determinadas para que la animación tenga un propósito o cuente una historia. Dentro de las posibilidades de trabajo futuro, una de ellas consistiría en aprovechar los hallazgos encontrados para hacer modelos de interés científico, como lo pueden ser el sistema solar, los átomos, las sociedades artificiales, entre muchos otros. Sin embargo, para lograrlo habría que buscar una mejor manera de definir las trayectorias circulares o elípticas en reemplazo a las traslaciones en línea recta. Otra posible aplicación a futuro podría ser desarrollar una librería semejante para P5js, dado que el lenguaje Processing no cuenta con un editor web oficial (sólo se consiguen algunos que, incluso, advierten que ya no es tan utilizado y donde el manejo de liberías está lleno de dificultades). Esto permitiría aprovechar nociones matemáticas y de coherencia temporal para crear animaciones con un acabado profesional en javascript. Referencias\u003e Referencias # [1] J. P. Charalambos, \u0026ldquo;Temporal Coherence,\u0026rdquo; Visual Computing, Feb. 2023. https://visualcomputing.github.io/docs/visual_illusions/temporal_coherence/\u003e [1] J. P. Charalambos, \u0026ldquo;Temporal Coherence,\u0026rdquo; Visual Computing, Feb. 2023. https://visualcomputing.github.io/docs/visual_illusions/temporal_coherence/ # [2] R. Parent, \u0026ldquo;Computer Animation: Algorithms and Techniques,\u0026rdquo; 2nd ed. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc., 2012.\u003e [2] R. Parent, \u0026ldquo;Computer Animation: Algorithms and Techniques,\u0026rdquo; 2nd ed. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc., 2012. # [3] J. P. Charalambos, \u0026ldquo;nub: A library for processing large datasets in parallel,\u0026rdquo; Github, 2021. https://github.com/VisualComputing/nub\u003e [3] J. P. Charalambos, \u0026ldquo;nub: A library for processing large datasets in parallel,\u0026rdquo; Github, 2021. https://github.com/VisualComputing/nub # [4] K. Shoemake, \u0026ldquo;Animating rotation with quaternion curves,\u0026rdquo; in ACM SIGGRAPH Computer Graphics, vol. 19, no. 3, pp. 245-254, Jul. 1985, doi: 10.1145/325165.325242.\u003e [4] K. Shoemake, \u0026ldquo;Animating rotation with quaternion curves,\u0026rdquo; in ACM SIGGRAPH Computer Graphics, vol. 19, no. 3, pp. 245-254, Jul. 1985, doi: 10.1145/325165.325242. # ","date":"1 January 0001","permalink":"/showcase/first/keyframes/","section":"Firsts","summary":"Temporal Coherence\u003e Temporal Coherence # Introducción\u003e Introducción # En esta sección se discute, en primer lugar, acerca del fenómeno de coherencia temporal y la estrecha relación que guarda con las animaciones y, en particular, con el uso de fotogramas clave (keyframes).","title":""},{"content":"Visual Masking I\u003e Visual Masking I # Introducción\u003e Introducción # En esta sección se discute acerca del enmascaramiento visual, haciendo énfasis en los kinegramas y los patrones de moiré, que son fenómenos visuales estrechamente relacionados con esta acción.\nActo seguido, se desarrollan varios ejercicios relacionados con tales conceptos. En particular, se llevó a cabo un programa que genera un kinegrama a partir de un gif, así como un patrón de moiré en dos dimensiones y otro en tres dimensiones.\nPara terminar, se discuten posibles aplicaciones de los conceptos aprendidos, así como el trabajo futuro que se puede desarrollar mediante este tipo de patrones y animaciones.\nMarco Teórico\u003e Marco Teórico # El enmascaramiento visual es un fenómeno perceptual en el que la visibilidad de un estímulo se ve afectada por la presentación simultánea de otro estímulo: el que afecta la visibilidad del otro se conoce como enmascarador o máscara y puede ser presentado antes, después, o al mismo tiempo que el estímulo objetivo. Este fenómeno puede ocurrir tanto en la visión consciente como en la no consciente, y puede tener implicaciones en la percepción visual, la atención y la memoria [1].\nA continuación, se exponen los ejemplos de enmascaramiento visual que competen a esta sección:\nPatrones de moiré: Los patrones de moiré son un ejemplo de visual masking en el que se produce una interferencia entre dos patrones de líneas superpuestas con una ligera diferencia en su orientación o frecuencia espacial. El resultado es una percepción errónea de los patrones de líneas, con la aparición de un nuevo patrón que puede parecer más denso, más oscuro o con un patrón diferente al original. Este fenómeno puede tener aplicaciones en la detección de irregularidades en superficies o la verificación de seguridad en tarjetas de crédito o documentos de identidad [2].\nKinegramas: Los kinegramas son una técnica de impresión en la que se superponen patrones de líneas de diferentes tamaños y orientaciones para crear una imagen con efectos de movimiento y cambios de color. Se utilizan, comúnmente, como medida de seguridad en billetes, pasaportes y otros documentos oficiales, pues están hechos de patrones de líneas finas y detallados que, cuando se ven desde diferentes ángulos o se mueven, crean un efecto de profundidad y movimiento que es difícil de reproducir o falsificar [3].\nEn general, el proceso de creación de un kinegrama está compuesto por los siguientes pasos elementales [4]:\nSeleccionar un conjunto de imágenes que tengan el mismo tamaño y que representen una secuencia de movimiento suave o una transformación gradual, como una rotación o un cambio de forma. Superponer estas imágenes con un patrón de rejilla o de líneas finas. Ajustar la frecuencia y la velocidad del movimiento o la transformación para crear una animación fluida. Código y Resultados:\u003e Código y Resultados: # Inicialmente, se realizaron dos implementaciones de patrones de moiré: la primera de ellas se trata de dos conjuntos de círculos concentricos, donde el segundo grupo se desplaza horizontalmente. Es posible mover el slider para aumentar o disminuir la velocidad de tal desplazamiento.\nCódigo completo let direction = true; function setup() { createCanvas(650, 500); ellipseMode(RADIUS); slider = createSlider(10, 100, 50); slider.position(width/3, (9 * height) / 10); slider.style(\u0026#39;width\u0026#39;, \u0026#39;250px\u0026#39;); } function draw() { background(255); frameRate(slider.value()); if (frameCount % width == 0) direction = !direction; for (let i = 8; i \u0026lt;= height/2 - 40; i+=8){ noFill() strokeWeight(2) ellipse(width/2, height/2 - 40, i); if (direction) ellipse(frameCount % width, height/2 - 40, i); else ellipse(width - (frameCount % width), height/2 - 40, i); } } De este código se destaca la manera en que se trazan los círculos en movimiento:\nif (direction) ellipse(frameCount % width, height/2 - 40, i); else ellipse(width - (frameCount % width), height/2 - 40, i); Aquí, direction es un valor booleano que cambia cada vez que frameCount % width == 0: así se consigue que cambie el sentido de izquierda a derecha y viceversa, dado que cambia la coordenada en x mientras que lo demás permanece igual.\nUna vez se ha interiorizado el concepto mediante esta implementación sencilla, se procede con una más compleja, pues se trata de una animación en tres dimensiones que emplea color. De manera análoga, es posible cambiar la velocidad de rotación de cada patrón mediante el slider cuyo color coincide con las líneas (solo que, en este caso, deslizar hacia la izquierda incrementa la velocidad, mientras deslizar a la derecha hace que disminuya). Por otra parte, el ángulo de la cámara es mutable presionando las teclas de \u0026ldquo;flecha derecha\u0026rdquo; y \u0026ldquo;flecha izquierda\u0026rdquo; para tener la vista frontal o posterior, respectivamente.\nCódigo completo function setup() { createCanvas(700, 700, WEBGL); slider1 = createSlider(5000, 20000, 10000); slider1.position(width/3, 30); slider1.style(\u0026#39;width\u0026#39;, \u0026#39;250px\u0026#39;); slider1.style(\u0026#39;accent-color\u0026#39;, \u0026#39;red\u0026#39;); slider2 = createSlider(5000, 20000, 10000); slider2.position(width/12, height-50); slider2.style(\u0026#39;width\u0026#39;, \u0026#39;250px\u0026#39;); slider2.style(\u0026#39;accent-color\u0026#39;, \u0026#39;green\u0026#39;); slider3 = createSlider(5000, 20000, 10000); slider3.position(7*width/12, height-50); slider3.style(\u0026#39;width\u0026#39;, \u0026#39;250px\u0026#39;); slider3.style(\u0026#39;accent-color\u0026#39;, \u0026#39;blue\u0026#39;); cam = createCamera(); cam.setPosition(0, 0, 600); } function draw() { background(255); strokeWeight(3); rotateX(-millis()/slider1.value()); lines(\u0026#39;red\u0026#39;); rotateX(millis()/slider1.value()); rotateY(-millis()/slider2.value()); lines(\u0026#39;green\u0026#39;); rotateY(millis()/slider2.value()); rotateZ(-millis()/slider3.value()); lines(\u0026#39;blue\u0026#39;); if (keyIsDown(LEFT_ARROW)) { cam.setPosition(0, 0, -600); cam.lookAt(0,0,0); } if (keyIsDown(RIGHT_ARROW)) { cam.setPosition(0, 0, 600); cam.lookAt(0,0,0); } } function lines(color){ stroke(color) for (let i = 160 - width/2; i \u0026lt;= width/2 - 160; i += 5){ line(i, 160 - height/2, i, height/2 - 160) } } En este programa se destaca la manera en que se logran las rotaciones: rotateX(-millis()/slider1.value()); lines(\u0026#39;red\u0026#39;); rotateX(millis()/slider1.value()); rotateY(-millis()/slider2.value()); lines(\u0026#39;green\u0026#39;); rotateY(millis()/slider2.value()); rotateZ(-millis()/slider3.value()); lines(\u0026#39;blue\u0026#39;); Dado que las funciones rotateX, rotateY y rotateZ afectan a todos los elementos que se dibujen después de haberlas mencionado, fue necesario volver a aplicar rotateX con la misma magnitud pero el sentido opuesto para que el segundo patrón rotara únicamente en el eje y. Del mismo modo se volvió a aplicar rotateY al dibujar el tercer patrón para que este rotara tan solo en el eje z. Así se consiguió que cada conjunto de líneas girara sobre un eje distinto.\nPara el próximo ejercicio, partiendo de los pasos descritos en el marco teórico para crear un kiengrama, se implementó un programa que, tomando un archivo de tipo gif, extrajera sus primeros 20 fotogramas y construyera tal animación. Con el botón de la parte inferior es posible mostrar y ocultar la máscara, mientras que el slider permite controlar su velocidad.\nCódigo completo let keyframes = []; let kinegram; let overlay; let animate = false; let gif; let newGif; let gifLoading = false; let uploaded = false; function preload(){ gif = loadImage(\u0026#34;../assets/example.gif\u0026#34;) } function setup() { createCanvas(600, 600); imageMode(CENTER); textAlign(CENTER); if (!uploaded){ newGif = createFileInput(handleFile); newGif.position(width / 3, height / 8); slider = createSlider(5, 25, 5); slider.position((3 * width) / 5, (5 * height) / 6); slider.style(\u0026#39;width\u0026#39;, \u0026#39;100px\u0026#39;); } gif.resize(0,300) for (let i = 0; i \u0026lt; 20; i++) { gif.setFrame(i) keyframes[i] = createImage(gif.width, gif.height); gif.loadPixels(); keyframes[i].loadPixels(); for (let j = 0; j \u0026lt; gif.width; j++) { for (let k = 0; k \u0026lt; gif.height; k++) { keyframes[i].set(j, k, gif.get(j, k)); } } keyframes[i].updatePixels(); } kinegram = createImage(keyframes[0].width, keyframes[0].height); kinegram.loadPixels(); overlay = createImage(keyframes[0].width, keyframes[0].height); overlay.loadPixels(); for (let x = 0; x \u0026lt; overlay.width; x++) { for (let y = 0; y \u0026lt; overlay.height; y++) { let index = int(x/0.25) % keyframes.length; if (x \u0026lt; kinegram.width) { kinegram.set(x, y, keyframes[index].get(x, y)); } if (index \u0026gt; 0) { overlay.set(x, y, color(0, 255)); } else { overlay.set(x, y, color(0, 0)); } } } kinegram.updatePixels(); overlay.updatePixels(); button = createButton(\u0026#39;Toggle Overlay\u0026#39;); button.position(width / 4, (5 * height) / 6); button.mousePressed(() =\u0026gt; {animate = !animate}); } function draw() { background(255); if(!gifLoading){ image(kinegram, width / 2, height / 2); if (animate){ frameRate(slider.value()); image(overlay, (frameCount * 4) % width, height / 2); } } } function handleFile(file) { gifLoading = true; gif = createImg(file.data, \u0026#34;\u0026#34;); gif.hide(); let gifSrc = gif.attribute(\u0026#34;src\u0026#34;); gif = loadImage(gifSrc, gifLoaded); } function gifLoaded() { gifLoading = false; uploaded = true; setup(); } Por otra parte, con el botón de la parte superior es posible cargar cualquier gif desde la máquina local. Es importante tener en cuenta que su tamaño será reajustado a las proporciones del canvas. A continuación se deja una carpeta con archivos de prueba que se pueden cargar:\nAquí hay varias secciones del código que se destacan: en primera medida, se tiene el ciclo empleado para extraer los fotogramas del gif:\nfor (let i = 0; i \u0026lt; 20; i++) { gif.setFrame(i) keyframes[i] = createImage(gif.width, gif.height); gif.loadPixels(); keyframes[i].loadPixels(); for (let j = 0; j \u0026lt; gif.width; j++) { for (let k = 0; k \u0026lt; gif.height; k++) { keyframes[i].set(j, k, gif.get(j, k)); } } keyframes[i].updatePixels(); } Como se puede observar, se utiliza la función setFrame para obtener el fotograma y, luego, se cargan sus pixeles en otra imagen, la cual se almacena en un arreglo. Con los ciclos anidades se itera a través de los pixeles para copiarlos en la nueva imagen; finalmente, los pixeles se actualizan.\nEl kinegrama y la máscara se crean usando un ciclo parecido al anterior:\nfor (let x = 0; x \u0026lt; overlay.width; x++) { for (let y = 0; y \u0026lt; overlay.height; y++) { let index = int(x/0.25) % keyframes.length; if (x \u0026lt; kinegram.width) { kinegram.set(x, y, keyframes[index].get(x, y)); } if (index \u0026gt; 0) { overlay.set(x, y, color(0, 255)); } else { overlay.set(x, y, color(0, 0)); } } } La lógica es que se toma una columna de cada fotograma y sus pixeles se cargan en la nueva imagen (el kinegrama): esto se hace de manera iterativa, por ello se utiliza la función módulo respecto a la longitud del arreglo keyframes. Para la máscara, cada vez que este módulo sea igual a cero (cuando termina una \u0026ldquo;iteración\u0026rdquo;) se pinta de color negro y, en caso contrario, se deja transparente. La fracción respecto a x en el cálculo del índice fue un valor obtenido mediante ensayo y error que permitiese ver una animación fluida. Esta misma razón se tiene en cuenta al animar la máscara mediante el código image(overlay, (frameCount * 4) % width, height / 2), pues dividir entre 0.25 es equivalente a multiplicar por 4.\nPor último, se destacan las funciones para cargar gifs desde la máquina local:\nfunction handleFile(file) { gifLoading = true; gif = createImg(file.data, \u0026#34;\u0026#34;); gif.hide(); let gifSrc = gif.attribute(\u0026#34;src\u0026#34;); gif = loadImage(gifSrc, gifLoaded); } function gifLoaded() { gifLoading = false; uploaded = true; setup(); } Como se observa, cada vez que se carga un archivo, la bandera gifLoading se deja como verdadera, y se vuelve a poner como falsa en la función gifLoaded, es decir, cuando ya ha finalizado la carga. Se crea una nueva imagen en la variable gif (que luego debe ocultarse para evitar que aparezca bajo el canvas) y se carga la ruta del archivo con loadImage, cuyo callback es la función que indica que ya se cargó el archivo: dentro de esta última vuelve a llamarse el setup, pero se marca la bandera uploaded como verdadera para que se refresquen únicamente las imágenes (pero no los sliders, por ejemplo).\nConclusiones y Trabajo futuro\u003e Conclusiones y Trabajo futuro # Una particularidad muy interesante de los patrones de moiré que se observó en la segunda implementación son las deformaciones que se generan cuando el plano está en tres dimensiones: mientras que la superposición de patrones de líneas rectas en dos dimensiones genera efectos visuales rectilíneos, en este ejercicio la superposición de líneas rectas desemboca en la visualización de líneas curvas, dado que cada patrón gira sobre un eje distinto: x,y o z. Adicionalmente, estas líneas curvas se aprecian en tonalidades de amarillo, magenta y cian, puesto que las líneas rectas se trazaron en rojo, verde y azul (RGB). Mientras que elaborar un kinegrama manualmente puede ser una tarea compleja y que requiere conocimientos técnicos en programas de edición de imágenes, en p5js resulta convertirse en una tarea sencilla gracias a las múltiples funcionalidades que este lenguaje posee. Además, la propuesta realizada de convertir un gif a kinegrama permite comprender aún mejor su funcionamiento, dado que se ve la transición de animación a imagen estática y máscara, que vuelven a dar como resultado la misma animación cuando la máscara se pone en movimiento. No obstante, esta implementación también cuenta con algunos puntos negativos: por un lado, requiere ser muy precisa, porque calquier variación en el cálculo del índice podría estropear la armonía visual de la animación resultante (haciendo que no luzca ni remotamente parecido al gif original); por otra parte, consume muchos recursos tanto de tiempo, pues se aprecia que la página carga un poco más lento de lo usual, como de memoria, lo cual es evidente considerando toda la cantidad de imágenes y archivos que se están manipulando. Como trabajo futuro, se podría aplicar la implementación de patrones de moiré en tres dimensiones para construir, de manera análoga, kinegramas en tres dimensiones: esto podría conllevar un estudio a profunidad de cómo generar la máscara visual y, así mismo, de cómo debería ser su rotación para ofrecer una animación visualmente armoniosa. Referencias\u003e Referencias # [1] B. G. Breitmeyer, “Visual Masking: An Integrative Approach,” Oxford University Press, Oxford, 1984.\u003e [1] B. G. Breitmeyer, “Visual Masking: An Integrative Approach,” Oxford University Press, Oxford, 1984. # [2] L. Spillmann, \u0026ldquo;The Perception of Movement and Depth in Moiré Patterns,\u0026rdquo; Perception, vol. 22, no. 3, pp. 287-308, 1993, doi: 10.1068/p220287.\u003e [2] L. Spillmann, \u0026ldquo;The Perception of Movement and Depth in Moiré Patterns,\u0026rdquo; Perception, vol. 22, no. 3, pp. 287-308, 1993, doi: 10.1068/p220287. # [3] R. L. Van Renesse, \u0026ldquo;A review of holograms and other microstructures as security features,\u0026rdquo; Springer Series in Optical Sciences, vol. 78, 2007. https://www.dslreports.com/r0/download/2346751~41cd69e70ba7cfa509b37dddbba63faa/vanrenesse.pdf\u003e [3] R. L. Van Renesse, \u0026ldquo;A review of holograms and other microstructures as security features,\u0026rdquo; Springer Series in Optical Sciences, vol. 78, 2007. https://www.dslreports.com/r0/download/2346751~41cd69e70ba7cfa509b37dddbba63faa/vanrenesse.pdf # [4] O. Georgiou and M. Georgiou, \u0026ldquo;ZEBRA | COMPUTING MOIRE ANIMATIONS,\u0026rdquo; Sustainable Computational Workflows, pp. 49-56, 2018. http://papers.cumincad.org/data/works/att/ecaaderis2018_120.pdf\u003e [4] O. Georgiou and M. Georgiou, \u0026ldquo;ZEBRA | COMPUTING MOIRE ANIMATIONS,\u0026rdquo; Sustainable Computational Workflows, pp. 49-56, 2018. http://papers.cumincad.org/data/works/att/ecaaderis2018_120.pdf # ","date":"1 January 0001","permalink":"/showcase/first/masking1/","section":"Firsts","summary":"Visual Masking I\u003e Visual Masking I # Introducción\u003e Introducción # En esta sección se discute acerca del enmascaramiento visual, haciendo énfasis en los kinegramas y los patrones de moiré, que son fenómenos visuales estrechamente relacionados con esta acción.","title":""},{"content":"Visual Masking II\u003e Visual Masking II # Introducción\u003e Introducción # En esta sección se continúa con el enmascaramiento visual, pero esta vez el foco está en las máscaras de convolución (kernels de imágenes), así como en otras cualidades del procesamiento digital de imágenes: los histogramas y la luminosidad.\nActo seguido, se desarrolla una aplicación de procesamiento de imágenes que abarca algunos de los kernels más comunes: identity, edge detection, sharpen, emboss, gaussian blur, unsharp masking. Dentro del mismo programa se lleva a cabo la generación de un histograma con los valores de luminosidad del modo de color HSL: dicha luminosidad también es mutable en la imagen resultante de la convolución, alterando el histograma trazado en el proceso.\nPara terminar, se discuten posibles aplicaciones de los conceptos aprendidos, así como el trabajo futuro que se puede desarrollar mediante visual masking en general.\nMarco Teórico\u003e Marco Teórico # En la sección anterior se definía el enmascaramiento visual; por consiguiente, se pasa directamente a la conceptualización propia de esta sección.\nMáscaras de convolución: Para entender el concepto, en primer lugar es importante comprender qué es una convolución. De acuerdo con [1], esta se puede describir, intuitivamente, como una función que es la integral o suma de dos funciones componentes y que mide la cantidad de superposición a medida que una se desplaza sobre la otra. Formalmente, una convolución está definida de la siguiente manera [2]:\n\\[ g(x,y) = \\omega * f(x,y) = \\sum_{dx=-a}^{a}\\sum_{dy=-b}^{b} \\omega(dx,dy) f(x-dx,y-dy) \\] Ahora, una máscara de convolución es una matriz o kernel numérico utilizado en procesamiento de imágenes y visión por computadora para realizar operaciones de convolución en una imagen. Estas matrices, típicamente son de tamaño 3 x 3, aunque también se pueden emplear matrices de tamaño 2 x 2 y 5 x 5, entre otras. En líneas generales, en procedimiento consiste en que, a partir de una imagen, se van tomando bloques que posean el mismo tamaño de la matriz. Luego, aplicando la operación descrita (convolución) en función de los valores de los píxeles originales y los coeficientes de la máscara, se calcula el valor de cada píxel de la imagen resultante [1]. Esta tendrá ciertas características dependiendo del kernel que haya sido aplicado. Algunos de los más comunes (y los cuales fueron implementados en este ejercicio) se listan a continuación [2] y [3]:\nIdentity: \\[ \\begin{bmatrix} 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \\\\ \\end{bmatrix} \\] Edge detection: \\[ \\begin{bmatrix} -1 \u0026amp; -1 \u0026amp; -1 \\\\ -1 \u0026amp; 8 \u0026amp; -1 \\\\ -1 \u0026amp; -1 \u0026amp; -1 \\\\ \\end{bmatrix} \\] Sharpen: \\[ \\begin{bmatrix} 0 \u0026amp; -1 \u0026amp; 0 \\\\ -1 \u0026amp; 5 \u0026amp; -1 \\\\ 0 \u0026amp; -1 \u0026amp; 0 \\\\ \\end{bmatrix} \\] Emboss: \\[ \\begin{bmatrix} -2 \u0026amp; -1 \u0026amp; 0 \\\\ -1 \u0026amp; 1 \u0026amp; 1 \\\\ 0 \u0026amp; 1 \u0026amp; 2 \\\\ \\end{bmatrix} \\] Gaussian Blur (5x5): \\[ \\frac{1}{256} \\begin{bmatrix} 1 \u0026amp; 4 \u0026amp; 6 \u0026amp; 4 \u0026amp; 1 \\\\ 4 \u0026amp; 16 \u0026amp; 24 \u0026amp; 16 \u0026amp; 4 \\\\ 6 \u0026amp; 24 \u0026amp; 36 \u0026amp; 24 \u0026amp; 6 \\\\ 4 \u0026amp; 16 \u0026amp; 24 \u0026amp; 16 \u0026amp; 4 \\\\ 1 \u0026amp; 4 \u0026amp; 6 \u0026amp; 4 \u0026amp; 1 \\\\ \\end{bmatrix} \\] Unsharp Masking (5x5): \\[ \\frac{-1}{256} \\begin{bmatrix} 1 \u0026amp; 4 \u0026amp; 6 \u0026amp; 4 \u0026amp; 1 \\\\ 4 \u0026amp; 16 \u0026amp; 24 \u0026amp; 16 \u0026amp; 4 \\\\ 6 \u0026amp; 24 \u0026amp; -476 \u0026amp; 24 \u0026amp; 6 \\\\ 4 \u0026amp; 16 \u0026amp; 24 \u0026amp; 16 \u0026amp; 4 \\\\ 1 \u0026amp; 4 \u0026amp; 6 \u0026amp; 4 \u0026amp; 1 \\\\ \\end{bmatrix} \\] Luminosidad e histograma de imagen: En este ámbito, un histograma es una representación gráfica de la distribución de los valores de los píxeles en una imagen: el eje x representa los valores de intensidad de los píxeles, mientras que el eje y representa la cantidad de píxeles en la imagen que tienen cada valor de intensidad [4].\nLa intensidad es un valor presente en el modelo de color HSI, también conocido como HSL: sus siglas se refieren a Hue (matiz), que corresponde al tono que distingue a un color de otro; Saturation (saturación), que se relaciona con la pureza del color, es decir, la cantidad de gris en relación con el matiz; y Lightness (luminosidad), que es la cantidad de luz blanca que se mezcla con un color, dando una medida de la claridad u oscuridad del mismo con valores que van desde 0 (negro) hasta 100 (blanco) [4].\nCódigo y Resultados:\u003e Código y Resultados: # Código completo let file; let newFile let maskedFile; let lightFile let histogram; let fileLoading = false; let uploaded = false; function preload() { file = loadImage(\u0026#34;../../assets/shrek.png\u0026#34;); } function setup() { createCanvas(600, 1000); colorMode(RGB, 255); textSize(20); textAlign(CENTER); textStyle(BOLD); file.resize(250,250); maskedFile = createImage(file.width, file.height); lightFile = createImage(file.width, file.height); if (!uploaded){ newFile = createFileInput(handleFile); newFile.position(width/2 - 65, 25); slider = createSlider(-100, 100, 0); slider.position(325, 380); slider.style(\u0026#39;width\u0026#39;, \u0026#39;250px\u0026#39;); menu = createSelect(); menu.position(25, 380); menu.style(\u0026#39;width\u0026#39;, \u0026#39;250px\u0026#39;); menu.option(\u0026#34;Identity\u0026#34;); menu.option(\u0026#34;Edge Detection\u0026#34;); menu.option(\u0026#34;Sharpen\u0026#34;); menu.option(\u0026#34;Emboss\u0026#34;); menu.option(\u0026#34;Gaussian Blur 5x5\u0026#34;); menu.option(\u0026#34;Unsharp Masking 5x5\u0026#34;); resetHistogram(); file.loadPixels(); maskedFile.loadPixels(); for (let j = 0; j \u0026lt; file.width; j++) { for (let k = 0; k \u0026lt; file.height; k++) { let pixel = file.get(j, k); maskedFile.set(j, k, pixel); lightFile.set(j, k, pixel); histogram[int(lightness(pixel))]++; } } scaleHistogram(); maskedFile.updatePixels(); lightFile.updatePixels(); } else { masking(); } slider.input(changeLightness); menu.input(masking); } function draw() { background(0); noStroke(); fill(\u0026#34;white\u0026#34;) text(\u0026#34;Lightness:\u0026#34;, 3*width/4, 360); text(\u0026#34;Kernel:\u0026#34;, width/4, 360); rect(0, 420, width, height-420) fill(\u0026#34;black\u0026#34;) text(\u0026#34;Histograma de la imagen resultante:\u0026#34;, width/2, 450); if(!fileLoading){ image(file, 25, 70); image(lightFile, 325, 70); stroke(\u0026#34;purple\u0026#34;); for (let w = 0; w \u0026lt;= 100; w++) { let ab = int(map(w, 0, 100, 25, width - 25)); line(ab, height - histogram[w] - 30, ab, height - 30); } } } function changeLightness(){ resetHistogram(); for (let z = 0; z \u0026lt; maskedFile.width; z++) { for (let k = 0; k \u0026lt; maskedFile.height; k++) { let pixel = maskedFile.get(z, k); let l = constrain(int(lightness(pixel) + slider.value()), 0, 100) let newColor = \u0026#34;hsl(\u0026#34; + int(hue(pixel)) + \u0026#34;, \u0026#34; + saturation(pixel) + \u0026#34;%, \u0026#34; + l + \u0026#34;%)\u0026#34; lightFile.set(z, k, color(newColor)); histogram[l]++; } } lightFile.updatePixels(); scaleHistogram(); } function changeKernel(M){ resetHistogram(); let rangeX = int(M.length/2) let rangeY = int(M[0].length/2) for (let x = 0; x \u0026lt; maskedFile.width; x++) { for (let y = 0; y \u0026lt; maskedFile.height; y++) { let convolution = [0,0,0,255] for (let i = -rangeX; i \u0026lt;= rangeX; i++) { for (let j = -rangeY; j \u0026lt;= rangeY; j++) { let pixel = file.get(x + i, y + j); let val = M[i + rangeX][j + rangeY] convolution[0] += pixel[0] * val; convolution[1] += pixel[1] * val; convolution[2] += pixel[2] * val; } } histogram[int(lightness(convolution))]++; maskedFile.set(x, y, convolution) } } maskedFile.updatePixels(); scaleHistogram(); changeLightness(); } function masking() { if (menu.value() == \u0026#34;Identity\u0026#34;){ matrix = [ [ 0, 0, 0 ], [ 0, 1, 0 ], [ 0, 0, 0 ] ]; } else if (menu.value() == \u0026#34;Edge Detection\u0026#34;){ matrix = [ [ -1, -1, -1 ], [ -1, 8, -1 ], [ -1, -1, -1 ] ]; } else if (menu.value() == \u0026#34;Sharpen\u0026#34;){ matrix = [ [ 0, -1, 0 ], [ -1, 5, -1 ], [ 0, -1, 0 ] ]; } else if (menu.value() == \u0026#34;Emboss\u0026#34;){ matrix = [ [ -2, -1, 0 ], [ -1, 1, 1 ], [ 0, 1, 2 ] ]; } else if (menu.value() == \u0026#34;Gaussian Blur 5x5\u0026#34;){ matrix = [ [ 1/256, 4/256, 6/256, 4/256, 1/256 ], [ 4/256, 16/256, 24/256, 16/256, 4/256 ], [ 6/256, 24/256, 36/256, 24/256, 6/256 ], [ 4/256, 16/256, 24/256, 16/256, 4/256 ], [ 1/256, 4/256, 6/256, 4/256, 1/256 ] ]; } else if (menu.value() == \u0026#34;Unsharp Masking 5x5\u0026#34;){ matrix = [ [ -1/256, -4/256, -6/256, -4/256, -1/256 ], [ -4/256, -16/256, -24/256, -16/256, -4/256 ], [ -6/256, -24/256, 476/256, -24/256, -6/256 ], [ -4/256, -16/256, -24/256, -16/256, -4/256 ], [ -1/256, -4/256, -6/256, -4/256, -1/256 ] ]; } changeKernel(matrix); } function resetHistogram(){ histogram = []; for (let i = 0; i \u0026lt;= 100; i++){ append(histogram, 0); } } function scaleHistogram(){ for (let i = 0; i \u0026lt;= 100; i++){ histogram[i] = int(map(histogram[i], min(histogram), max(histogram), 0, 500, true)); } } function handleFile(img) { fileLoading = true; file = createImg(img.data, \u0026#34;\u0026#34;); file.hide(); let fileSrc = file.attribute(\u0026#34;src\u0026#34;); file = loadImage(fileSrc, fileLoaded); } function fileLoaded() { fileLoading = false; uploaded = true; setup(); } El programa desarrollado es extenso y posee varias funcionalidades: en primer lugar, el botón de la parte superior sirve para cargar imágenes desde la máquina local. Es importante que la imagen sea cuadrada desde un inicio para evitar deformaciones cuando se reajuste su tamaño. A continuación se deja una carpeta con archivos de prueba que se pueden cargar para probar el funcionamiento:\nEn segundo lugar, a la izquierda aparece la imagen original, mientras que a la derecha aparece la imagen transformada. Debajo de ellas hay un selector y un slider: con el primero es posible elegir el kernel a aplicar y con el segundo es posible cambiar la luminosidad.\nFinalmente, se tiene el histograma que genera la imagen resultante, es decir, después de sufrir la convolución y el cambio en la luminosidad: se hizo de esta forma porque así es posible ver las variaciones de este atributo para cada caso.\nEn cuanto al código, se tienen las mismas funciones para la carga de archivos que en la sección anterior sobre visual masking: allí están detalladas por si se desea conocer su funcionamiento. Como tal, en esta aplicación hay dos funciones que se destacan: la del cambio de luminosidad y la del cambio de kernel.\nfunction changeLightness(){ resetHistogram(); for (let z = 0; z \u0026lt; maskedFile.width; z++) { for (let k = 0; k \u0026lt; maskedFile.height; k++) { let pixel = maskedFile.get(z, k); let l = constrain(int(lightness(pixel) + slider.value()), 0, 100) let newColor = \u0026#34;hsl(\u0026#34; + int(hue(pixel)) + \u0026#34;, \u0026#34; + saturation(pixel) + \u0026#34;%, \u0026#34; + l + \u0026#34;%)\u0026#34; lightFile.set(z, k, color(newColor)); histogram[l]++; } } lightFile.updatePixels(); scaleHistogram(); } Esta función del cambio de luminosidad, primero que todo, resetea el histograma (pues está mostrando la distribución de la luminosidad y esta va a cambiar en cuanto se deslice el slider): esto equivale a poner todos sus valores en cero. Acto seguido, itera sobre cada pixel de la imagen emnascarada (es decir, la que ya pasó por la convolución) y obtiene su nuevo valor de luminosidad, limitándolo en el rango de cero a cien.\nPara actualizar la imagen, se crea un nuevo pixel utilizando el modo de color HSL, a diferencia del canvas, que utiliza RGB: aquí se recibe el matiz y la saturación sin variaciones, pero en la luminosidad sí se pasa el valor alterado. Por último, se incrementa el valor del histograma en la posición l, esto es porque el eje x representa los valores del cero al cien que puede tomar la luminosidad, dado que se va a mostrar la distribución que tiene en la imagen. Al final se actualizan los pixeles del archivo lightFile (el que finalmente se muestra al usuario) y se escala al histograma para que este se muestre de manera apropiada en el canvas.\nfunction changeKernel(M){ resetHistogram(); let rangeX = int(M.length/2) let rangeY = int(M[0].length/2) for (let x = 0; x \u0026lt; maskedFile.width; x++) { for (let y = 0; y \u0026lt; maskedFile.height; y++) { let convolution = [0,0,0,255] for (let i = -rangeX; i \u0026lt;= rangeX; i++) { for (let j = -rangeY; j \u0026lt;= rangeY; j++) { let pixel = file.get(x + i, y + j); let val = M[i + rangeX][j + rangeY] convolution[0] += pixel[0] * val; convolution[1] += pixel[1] * val; convolution[2] += pixel[2] * val; } } histogram[int(lightness(convolution))]++; maskedFile.set(x, y, convolution) } } maskedFile.updatePixels(); scaleHistogram(); changeLightness(); } Ahora, en la función de cambiar el kernel también se reinicia el histograma. Se calculan las variables rangeX y rangeY para saber qué tan atrás se debe ir en los pixeles que están al borde de la imagen. Luego están cuatro ciclos anidados: con los dos primeros se itera sobre los pixeles de la imagen original, mientras que con los dos internos se itera en los bloques del mismo tamaño de la matriz del kernel para realizar la operación de convolución. Este procedimiento se aplica sobre cada valor R, G y B del píxel, dando como resultado un pixel nuevo, llamado convolution, el cual se pone en el archivo maskedFile.\nPor su parte, para el histograma se calcula la luminosidad del pixel nuevo y se incrementa el valor en dicha posición del arreglo. Al final se actualizan los pixeles, se escala el histograma y se actualiza la luminosidad, pues es el archivo lightFile y no maskedFile el que se está renderizando junto a la imagen original. Esta función se llama dentro de la función masking, la cual contiene las matrices expresadas en el marco teórico y le pasa una de ellas como parámetro a la función changeKernel a partir de lo que se ingresó en el selector.\nConclusiones y Trabajo futuro\u003e Conclusiones y Trabajo futuro # Se puede evidenciar que el uso de máscaras de convolución es muy útil en el procesamiento de imágenes digitales, pues dependiendo del kernel aplicado es posible obtener información relevante como los bordes de la imagen o versiones más nítidas y/o borrrosas de la misma. Esto es algo que también se puede lograr mediante la aplicación de métodos numéricos, pero resulta ser mucho más sencillo (y práctico) con las técnicas desarrolladas en esta sección. El hecho de que el histograma de la imagen cambie en tiempo real permite reconocer la importancia de la luminosidad, pues su aumento o su decremento altera por completo la distribución del gráfico. Una posible aplicación, que podría considerarse también como trabajo futuro, consistiría en tener sliders que permitan variar las tonalidades R, G y B de manera individual, con el fin de evaluar qué impacto tienen en el cáculo y la distribución de la luminosidad en la imagen. Entre otros trabajos futuros a desarrollar, se considera la posibilidad de aplicar máscaras de convolución a animaciones y vídeos: esto podría ser una herramienta ampliamente útil, pero que podría consumir muchos recursos computacionales, puesto que, finalmente, se terminaría aplicando la operación en cada pixel de cada uno de los fotogramas. En síntesis, el enmascaramiento visual ofrece un abanico de posibilidades, tanto en lo que respecta a patrones y animaciones de moiré como a kernels de imágenes: los conceptos aprendidos pueden aplicarse para evaluar la atención visual de un producto multimedia, mejorar su accesibilidad y/o armonía, detectar ciertos patrones, entre muchos otros. Así mismo, gracias a esto también es posible comprender cómo funcionan los filtros que de edición de fotografías en los celulares y las aplicaciones móviles, e incluso algunas de las herramientas que pueden llegar a emplear las inteligencias artificiales relacionadas con el procesamiento de imágenes digitales. Referencias\u003e Referencias # [1] S. Kim and R. Casper, \u0026ldquo;Applications of convolution in image processing with MATLAB,\u0026rdquo; University of Washington, pp. 1-20, 2013. http://kiwi.bridgeport.edu/cpeg585/ConvolutionFiltersInMatlab.pdf\u003e [1] S. Kim and R. Casper, \u0026ldquo;Applications of convolution in image processing with MATLAB,\u0026rdquo; University of Washington, pp. 1-20, 2013. http://kiwi.bridgeport.edu/cpeg585/ConvolutionFiltersInMatlab.pdf # [2] J. P. Charalambos, \u0026ldquo;Visual masking\u0026rdquo;. Visual Computing, Feb. 2023. https://visualcomputing.github.io/docs/visual_illusions/masking/\u003e [2] J. P. Charalambos, \u0026ldquo;Visual masking\u0026rdquo;. Visual Computing, Feb. 2023. https://visualcomputing.github.io/docs/visual_illusions/masking/ # [3] S. Raveendran, P. J. Edavoor, N. K. Yernad Balachandra and V. Moodabettu Harishchandra, \u0026ldquo;Design and implementation of image kernels using reversible logic gates,\u0026rdquo; IET Image Processing, vol. 14, no 16, pp. 4110-4121, 2020, doi: 10.1049/iet-ipr.2019.1681.\u003e [3] S. Raveendran, P. J. Edavoor, N. K. Yernad Balachandra and V. Moodabettu Harishchandra, \u0026ldquo;Design and implementation of image kernels using reversible logic gates,\u0026rdquo; IET Image Processing, vol. 14, no 16, pp. 4110-4121, 2020, doi: 10.1049/iet-ipr.2019.1681. # [4] R. C. Gonzalez and R. E. Woods, \u0026ldquo;Digital Image Processing,\u0026rdquo; 4th ed., Pearson, 2018.\u003e [4] R. C. Gonzalez and R. E. Woods, \u0026ldquo;Digital Image Processing,\u0026rdquo; 4th ed., Pearson, 2018. # ","date":"1 January 0001","permalink":"/showcase/first/masking2/","section":"Firsts","summary":"Visual Masking II\u003e Visual Masking II # Introducción\u003e Introducción # En esta sección se continúa con el enmascaramiento visual, pero esta vez el foco está en las máscaras de convolución (kernels de imágenes), así como en otras cualidades del procesamiento digital de imágenes: los histogramas y la luminosidad.","title":""},{"content":"Spatial Coherence\u003e Spatial Coherence # Introducción\u003e Introducción # Para este ejercicio se puso en práctica el fenómeno visual de \u0026ldquo;Spatial Coherence\u0026rdquo; con el objetivo de desarrollar un código capaz de pixelar un video.\nDurante este ejercicio se buscó ir un poco más allá de lo que se pedía como tarea inicial: por consiguiente, se llevó a cabo el pixelado de un video en vivo y se agregaron diversas funciones para que el usuario pudiese interectuar con el programa. Todo esto, por supuesto, hecho a través de la implementación de un código en JavaScript, haciendo uso de la libreria P5js.\nFinalmente, se discuten posibles aplicaciones de los conceptos aprendidos, así como el trabajo futuro que se puede llevar a cabo tomando el ejercicio desarrollado como punto de partida.\nMarco Teórico\u003e Marco Teórico # La coherencia espacial es un término que se utiliza en la óptica para describir la capacidad de una onda electromagnética para mantener una relación de fase estable en diferentes puntos del espacio. La coherencia espacial se refiere a la propiedad de la luz que hace que se comporte como una onda: esto significa que tiene la capacidad de interferir constructiva o destructivamente consigo misma en diferentes puntos del espacio [1].\nLa coherencia espacial se utiliza en muchas aplicaciones ópticas, como la holografía, la tomografía óptica de coherencia y la microscopía de campo cercano [7]. En estas aplicaciones, la coherencia espacial es esencial para la formación de imágenes y para obtener información detallada sobre la estructura de los objetos que se están observando.\nLa coherencia espacial se puede cuantificar utilizando varias medidas, tales como la función de correlación espacial, el ancho de banda de coherencia y la longitud de coherencia: la función de correlación espacial es una medida de la correlación entre dos puntos de la onda en diferentes posiciones, el ancho de banda de coherencia es una medida de la gama de longitudes de onda que contribuyen a la coherencia de la onda y la longitud de coherencia es una medida de la distancia sobre la cual la onda mantiene una relación de fase estable [1].\nEn resumen, la coherencia espacial es una propiedad fundamental de las ondas electromagnéticas que permite que se comporten como ondas y que puedan interferir constructiva o destructivamente en diferentes puntos del espacio. La coherencia espacial se utiliza en muchas aplicaciones ópticas para la formación de imágenes y para obtener información detallada sobre la estructura de los objetos que se están observando.\nCódigo y Resultados\u003e Código y Resultados # A continuación, se describe el ejercicio desarrollado y se destacan las piezas de código más relevantes, las cuales demuestran los aportes efectuados sobre el ejemplo disponible. Cabe aclarar que para el correcto funcionamiento del código se debe autorizar a la página para que esta use la webcam.\nComo se puede evidenciar, el ejercicio desarrollado consiste en capturar la imagen que proviene de la cámara del computador y modificar los pixeles provinientes de cada frame: es posible variar su tamaño al desplazar el mouse horizontalemnte, así como su forma y su color mediante los botones de la sección inferior.\nPara lograrlo, se toma cada fotograma como una matriz compuesta de pequeñas submatrices, las cuales se manipulan para lograr cada efecto visual propuesto. Primero, se emplea una función interna de P5 llamada LoadPixels [9], la cual facilita el trabajo y permite usar cada frame como una matriz. Después, se crea una variable que permite manejar el valor de cada pixel a partir del tamaño de la pantalla, la posición del mouse sobre el eje x y otros valores predeterminados. Una vez definido el tamaño de los pixeles, estos se agrupan en pequeñas submatrices de 4 x 4: es aquí cuando se empieza a jugar con el tamaño de los pixeles. Por un lado, en caso de querer disminuir el número de pixeles en la transmisión, a cada pequeña submatriz se le asigna un color siguiendo el principio de la coherencia espacial [10]. Una vez que todos los pixeles de esa matriz tienen el mismo color, esta pasa a ser considerada como un único pixel, el cual ahora forma parte de una matriz más grande. Este proceso se puede repetir hasta llegar al mínimo de pixeles permitidos por frame.\nCabe aclarar que cada pixel debe ser escalado para que se ajuste al tamaño de la pantalla. Cuando se modifica la forma de los pixeles, dicha función se ejecuta, precisamente, sobre aquellos que están escalados. Entonces, al igual como se manipulan las submatrices para modificar el tamaño de los pixeles de la imagen (y su color por spatial coherence), tanto la forma como la paleta de colores se alteran siguiendo la misma lógica y empleando más funciones especiales de P5js [2] [3] [4] [5] [6] [7].\nEn la siguiente porción de código se resalta cómo se generan los valores para cada pixel y, además, cómo se van manipulando el tamaño, la forma o el color dependiendo de la posición del mouse y la opción que escoja el usuario.\nvideo.loadPixels(); if (isProcessingEnabled) { pixelSize = int(map(mouseX, 0, windowWidth, minSize, maxSize)); for (let x = 0; x \u0026lt; video.width; x += pixelSize) { for (let y = 0; y \u0026lt; video.height; y += pixelSize) { let index = (y * video.width + x) * 4; let r = video.pixels[index + 0]; let g = video.pixels[index + 1]; let b = video.pixels[index + 2]; let newColor = getColor(r, g, b); // Scale to fit windowWidth: let scaledX = floor(x * windowVideoRatio); let scaledY = floor(y * windowVideoRatio); let scaledPixelSize = ceil(pixelSize * windowVideoRatio); fill(newColor); if(pixelMode == 0){ stroke(\u0026#34;black\u0026#34;); rect(scaledX, scaledY, scaledPixelSize, scaledPixelSize); }else if(pixelMode == 1){ ellipse(scaledX, scaledY, scaledPixelSize, scaledPixelSize); }else if(pixelMode == 2){ triangle(scaledX, scaledY, scaledX + (pixelSize / 2), scaledY + pixelSize, scaledX + pixelSize, scaledY) } else if(pixelMode == 3){ noStroke() rect(scaledX, scaledY, scaledPixelSize, scaledPixelSize); } } } } else { image(video, 0,0, videoXResolution * windowVideoRatio, videoYResolution * windowVideoRatio); } El código completo se encuentra en el siguiente desplegable:\nCódigo completo const videoXResolution = 640; const videoYResolution = 480; let video; let pixelSize = 10; const minSize = 5; const maxSize = 50; let windowVideoRatio; let colorButton; let videoModeButton; let isProcessingEnabled = true; let colorModeIndex = 0; let lastColorModeIndex = 4; let pixelMode = 0 let lastPixelMode = 3 function setup() { createCanvas(800, 550); video = createCapture(VIDEO); video.size(videoXResolution, videoYResolution); video.hide(); windowVideoRatio = windowWidth / video.width; colorButton = createButton(\u0026#34;Toggle color modes\u0026#34;); colorButton.size(200, 50); colorButton.position(230, windowHeight - 60); colorButton.mousePressed(changeColorMode); colorButton = createButton(\u0026#34;Toggle processing\u0026#34;); colorButton.size(200, 50); colorButton.position(10, windowHeight - 60); colorButton.mousePressed(changeProcessing); colorButton = createButton(\u0026#34;Change pixels\u0026#34;); colorButton.size(200, 50); colorButton.position(450, windowHeight - 60); colorButton.mousePressed(changePixels); } function draw() { background(0); video.loadPixels(); if (isProcessingEnabled) { pixelSize = int(map(mouseX, 0, windowWidth, minSize, maxSize)); for (let x = 0; x \u0026lt; video.width; x += pixelSize) { for (let y = 0; y \u0026lt; video.height; y += pixelSize) { let index = (y * video.width + x) * 4; let r = video.pixels[index + 0]; let g = video.pixels[index + 1]; let b = video.pixels[index + 2]; let newColor = getColor(r, g, b); // Scale to fit windowWidth: let scaledX = floor(x * windowVideoRatio); let scaledY = floor(y * windowVideoRatio); let scaledPixelSize = ceil(pixelSize * windowVideoRatio); fill(newColor); if(pixelMode == 0){ stroke(\u0026#34;black\u0026#34;); rect(scaledX, scaledY, scaledPixelSize, scaledPixelSize); }else if(pixelMode == 1){ ellipse(scaledX, scaledY, scaledPixelSize, scaledPixelSize); }else if(pixelMode == 2){ triangle(scaledX, scaledY, scaledX + (pixelSize / 2), scaledY + pixelSize, scaledX + pixelSize, scaledY) } else if(pixelMode == 3){ noStroke() rect(scaledX, scaledY, scaledPixelSize, scaledPixelSize); } } } } else { image(video, 0,0, videoXResolution * windowVideoRatio, videoYResolution * windowVideoRatio); } textSize(16); fill(255); text(`Pixel size: ${pixelSize} - Frame rate: ${int(frameRate())}`, 10, windowHeight - 80); } function changeColorMode() { if (colorModeIndex \u0026lt; lastColorModeIndex) { colorModeIndex++; } else { colorModeIndex = 0; } } function changeProcessing() { isProcessingEnabled = !isProcessingEnabled; } function changePixels() { if (pixelMode \u0026lt; lastPixelMode) { pixelMode++; } else { pixelMode = 0; } } function getColor(r, g, b) { let newColor; switch (colorModeIndex) { case 0: newColor = color(r, g, b); break; case 1: newColor = getGrayScaleColor(r, g, b); break; case 2: newColor = getGameboyColor(r, g, b); break; case 3: newColor = getFunkyFutureColor(r, g, b); break; case 4: newColor = getFairyDustColor(r, g, b); break; } return newColor; } function getGrayScaleColor(r, g, b) { return 0.2126 * r + 0.7152 * g + 0.0722 * b; } function getGameboyColor(r, g, b) { let grayScale = getGrayScaleColor(r, g, b); let colorPalette = [\u0026#34;#332c50\u0026#34;, \u0026#34;#46878f\u0026#34;, \u0026#34;#94e344\u0026#34;, \u0026#34;#e2f3e4\u0026#34;]; let index = floor(map(grayScale, 0, 255, 0, 4)); return colorPalette[index]; } function getFunkyFutureColor(r, g, b) { let colorPalette = [ \u0026#34;#2b0f54\u0026#34;, \u0026#34;#ab1f65\u0026#34;, \u0026#34;#ff4f69\u0026#34;, \u0026#34;#fff7f8\u0026#34;, \u0026#34;#ff8142\u0026#34;, \u0026#34;#ffda45\u0026#34;, \u0026#34;#3368dc\u0026#34;, \u0026#34;#49e7ec\u0026#34;, ]; return getNearestColorInPalette(colorPalette, r, g, b); } function getFairyDustColor(r, g, b) { let colorPalette = [ \u0026#34;#f0dab1\u0026#34;, \u0026#34;#e39aac\u0026#34;, \u0026#34;#c45d9f\u0026#34;, \u0026#34;#634b7d\u0026#34;, \u0026#34;#6461c2\u0026#34;, \u0026#34;#2ba9b4\u0026#34;, \u0026#34;#93d4b5\u0026#34;, \u0026#34;#f0f6e8\u0026#34;, ]; return getNearestColorInPalette(colorPalette, r, g, b); } function getNearestColorInPalette(colorPalette, r, g, b) { let nearestColorIndex = 0; let nearestColorDistance = 255; for (let c = 0; c \u0026lt; colorPalette.length; c++) { let indexColor = color(colorPalette[c]); let indexRed = red(indexColor); let indexGreen = green(indexColor); let indexBlue = blue(indexColor); let colorDist = dist(r, g, b, indexRed, indexGreen, indexBlue); if (colorDist \u0026lt; nearestColorDistance) { nearestColorDistance = colorDist; nearestColorIndex = c; } } return colorPalette[nearestColorIndex]; } Conclusiones y Trabajo futuro\u003e Conclusiones y Trabajo futuro # El uso de la libería P5js otorga muchas facilidades a la hora de realizar el ejercicio de pixelar una imagen (y, en consencuencia, los frames de un video). Mediante las diversas funciones internas que brinda es posible solventar muchas dificultades que se presentan, lo que permite ahorrar tiempo y esfuerzo. Como el programa está hecho para modificar cada pixel de cada frame de un video en vivo, hay muchos valores que se deben calcular por segundo. En consecuencia, se debe llevar a cabo un gran procesamiento interno y se puede evidenciar que, a medida que aumenta el número de pixeles en el video, este va disminuyendo los frames por segundo que pude procesar. Sí a esto se le suman todas las funciones extra que el usuario puede realizar, esto se termina convirtiendo en una ardua tarea para la máquina, así que es razonable que los frames procesados por segundo caigan tanto en los escenarios más complejos. Para un trabajo futuro, se podría implementar una función que permita una mejora en la tasa de procesamiento para cada pixel y en la tasa de refresco de frames en pantalla. Con esto, se podría lograr una mayor fluidez en la transmisión cuando el número de pixeles aumente. Otro trabajo futuro consistiría en implementar diversas formas de manipular las submatrices de pixeles (por ejemplo utilizando promedio de color en lugar de coherencia espacial) y, de tal forma, comparar cuál código tiene un mejor desempeño a la hora de generar frames por segundo, así como una mejor calidad visual. Referencias\u003e Referencias # [1] J. P. Charalambos. \u0026ldquo;Spatial Coherence\u0026rdquo;. Visual Computing. 2023. https://visualcomputing.github.io/docs/visual_illusions/spatial_coherence/\u003e [1] J. P. Charalambos. \u0026ldquo;Spatial Coherence\u0026rdquo;. Visual Computing. 2023. https://visualcomputing.github.io/docs/visual_illusions/spatial_coherence/ # [2] \u0026ldquo;Kirokaze (Game Boy)\u0026rdquo;, Lospec, [En línea]. Available: https://lospec.com/palette-list/kirokaze-gameboy.\u003e [2] \u0026ldquo;Kirokaze (Game Boy)\u0026rdquo;, Lospec, [En línea]. Available: https://lospec.com/palette-list/kirokaze-gameboy. # [3] \u0026ldquo;Funky Future 8\u0026rdquo;, Lospec, [En línea]. Available: https://lospec.com/palette-list/funkyfuture-8.\u003e [3] \u0026ldquo;Funky Future 8\u0026rdquo;, Lospec, [En línea]. Available: https://lospec.com/palette-list/funkyfuture-8. # [4] \u0026ldquo;Fairydust 8\u0026rdquo;, Lospec, [En línea]. Available: https://lospec.com/palette-list/fairydust-8.\u003e [4] \u0026ldquo;Fairydust 8\u0026rdquo;, Lospec, [En línea]. Available: https://lospec.com/palette-list/fairydust-8. # [5] E. Glytsis, \u0026ldquo;Spatial Coherence,\u0026rdquo; [En línea]. Available: http://users.ntua.gr/eglytsis/OptEng/Coherence_p.pdf.\u003e [5] E. Glytsis, \u0026ldquo;Spatial Coherence,\u0026rdquo; [En línea]. Available: http://users.ntua.gr/eglytsis/OptEng/Coherence_p.pdf. # [6] S. Johnson, \u0026ldquo;Stephen Johnson on Digital Photography,\u0026rdquo; O\u0026rsquo;Reilly, 2006, ISBN 0-596-52370-X.\u003e [6] S. Johnson, \u0026ldquo;Stephen Johnson on Digital Photography,\u0026rdquo; O\u0026rsquo;Reilly, 2006, ISBN 0-596-52370-X. # [7] C. Poynton, \u0026ldquo;Digital Video and HD: Algorithms and Interfaces,\u0026rdquo; 2nd ed., Morgan Kaufmann, 2012, pp. 31-35, 65-68, 333, 337, ISBN 978-0-12-391926-7. [Online]. Available: https://www.sciencedirect.com/book/9780123919267/digital-video-and-hd. [Accessed: Mar. 31, 2022].\u003e [7] C. Poynton, \u0026ldquo;Digital Video and HD: Algorithms and Interfaces,\u0026rdquo; 2nd ed., Morgan Kaufmann, 2012, pp. 31-35, 65-68, 333, 337, ISBN 978-0-12-391926-7. [Online]. Available: https://www.sciencedirect.com/book/9780123919267/digital-video-and-hd. [Accessed: Mar. 31, 2022]. # [8] Daniel Shiffman, \u0026ldquo;The Nature of Code - Simulating Natural Systems with Processing,\u0026rdquo; YouTube, 2012. [En línea]. Available: https://www.youtube.com/watch?v=KfLqRuFjK5g. [Accedido: Mar. 31, 2022].\u003e [8] Daniel Shiffman, \u0026ldquo;The Nature of Code - Simulating Natural Systems with Processing,\u0026rdquo; YouTube, 2012. [En línea]. Available: https://www.youtube.com/watch?v=KfLqRuFjK5g. [Accedido: Mar. 31, 2022]. # [9] The Coding Train, \u0026ldquo;What is p5.js?,\u0026rdquo; YouTube, 2017. [En línea]. Available: https://www.youtube.com/watch?v=M3wTNVICUTg\u0026t=2s. [Accedido: Mar. 31, 2022].\u003e [9] The Coding Train, \u0026ldquo;What is p5.js?,\u0026rdquo; YouTube, 2017. [En línea]. Available: https://www.youtube.com/watch?v=M3wTNVICUTg\u0026t=2s. [Accedido: Mar. 31, 2022]. # [10] The Coding Train, \u0026ldquo;Introduction to p5.js - Variables and Drawing - p5.js Tutorial,\u0026rdquo; YouTube, 2015. [En línea]. Available: https://www.youtube.com/watch?v=VYg-YdGpW1o. [Accedido: Mar. 31, 2022].\u003e [10] The Coding Train, \u0026ldquo;Introduction to p5.js - Variables and Drawing - p5.js Tutorial,\u0026rdquo; YouTube, 2015. [En línea]. Available: https://www.youtube.com/watch?v=VYg-YdGpW1o. [Accedido: Mar. 31, 2022]. # ","date":"1 January 0001","permalink":"/showcase/first/pixelator/","section":"Firsts","summary":"Spatial Coherence\u003e Spatial Coherence # Introducción\u003e Introducción # Para este ejercicio se puso en práctica el fenómeno visual de \u0026ldquo;Spatial Coherence\u0026rdquo; con el objetivo de desarrollar un código capaz de pixelar un video.","title":""},{"content":"Mach Bands\u003e Mach Bands # Introducción\u003e Introducción # Para este ejercicio se definen los efectos visuales de \u0026ldquo;Mach bands\u0026rdquo; y Perlin noise, así como la relación que guardan al momento de generar un terreno automatizado y continuo.\nPosteriormente, como ejercicio práctico, se realizó una animación a través de la implementación de un código en JavaScript, haciendo uso de la libreria P5.js: esta consiste en generar terreno de manera aleatoria por medio de los dos efectos visuales mencionados en el párrafo anterior.\nFinalmente, se discuten posibles aplicaciones de los conceptos aprendidos, así como el trabajo futuro que se puede llevar a cabo tomando el ejercicio desarrollado como punto de partida.\nMarco Teórico\u003e Marco Teórico # Las bandas de Mach (Mach bands, en inglés) son un fenómeno óptico que se produce cuando hay un cambio gradual en la intensidad de la luz en una imagen. El ojo humano tiende a percibir estas transiciones como bordes o líneas más definidas de lo que realmente son [1]; esto se debe a la forma en que el cerebro procesa la información visual.\nPor otro lado, la generación de terreno con Perlin noise es un método para crear terrenos y superficies naturales de forma procedural. Este método utiliza una función de ruido desarrollada por Ken Perlin, en la década de 1980, que genera valores de ruido aleatorios pero coherentes y suaves [6].\nLa relación entre los Mach bands y la generación de terreno con Perlin noise es que se pueden utilizar las bandas de Mach para mejorar la apariencia visual de los terrenos generados con este método. Al aplicar una función de suavizado a los valores de ruido generados por Perlin noise, se pueden crear transiciones suaves entre las diferentes alturas del terreno [4]. Sin embargo, estas transiciones suaves pueden producir una apariencia visual demasiado uniforme o artificial. Por consiguiente, al aplicar una función que aumente los valores de intensidad cerca de las transiciones, se pueden crear efectos visuales similares a los de las bandas de Mach, que ayudan a definir mejor los bordes y las transiciones del terreno [5].\nEn resumen, las bandas de Mach se utilizan en la generación de terreno con Perlin noise como una técnica para mejorar la apariencia visual de los terrenos, creando efectos visuales que ayudan a definir mejor los bordes y las transiciones.\nCódigo y Resultados\u003e Código y Resultados # A continuación, se describe el ejercicio desarrollado y se destacan las piezas de código más relevantes, las cuales demuestran los aportes efectuados sobre el ejemplo disponible.\nComo se puede evidenciar, el ejercicio desarrollado consiste en una animación que genera de manera aleatoria y continua un terreno, al cual se le pueden modificar los valores de altitud máxima que puede llegar a tomar cada punto. Entrando un poco más en detalle sobre cómo se realizó el ejercicio, es importante aclarar que para generar el terreno se utilizó una grilla en 3 dimensiones (x, y, z) con valores predeterminados para los puntos x y y.\nEn primer lugar, se realiza una rotación sobre el eje x y se le asigna un valor aleatorio a la coordenada z entre un rango preestablecido. Acá es donde entra el método de Perlin Noise: de manera simplificada, con este método es poisble asegurar que cada valor generado \u0026ldquo;aleatoriamente\u0026rdquo; para la coordenada z, de cada punto de la grilla, únicamnete diverja dentro de un rango preestablecido y, a su vez, que este rango sea dependiente de los valores en z que se encuntren alrededor del punto que se está generando [7].\nEn la siguiente porción de codigo se resalta cómo se generan los valores en z para cada par de coordenadas (x, y) de la grilla y, también, cómo se asegura que estos valores sean acordes al método de Perlin noise.\nfunction draw() { fly -= 0.1 var yoff = fly for(var j=0; j\u0026lt;rows; j++){ var xoff = 0 for(var i=0; i\u0026lt;cols; i++){ terrein[i][j] = map(noise(xoff,yoff), 0, 1, -100, slider.value()) xoff += 0.1 } yoff += 0.1 } background(10); stroke(225) translate(0, 0) rotateX(PI/3) translate(-w/2, -h/2) for(var j=0; j\u0026lt;rows-1; j++){ beginShape(TRIANGLE_STRIP) for(var i=0; i\u0026lt;cols; i++){ vertex(i*scl, j*scl, terrein[i][j]) vertex(i*scl, (j+1)*scl, terrein[i][j+1]) colors = colorChange(terrein[i][j]) fill(colors) } endShape() } } El código completo se encuentra en el siguiente desplegable:\nCódigo completo let cols, rows; let scl = 20 let w = 1200 let h = 900 let terrein var fly = 0 let colors function setup() { createCanvas(800, 650, WEBGL); cols = w/scl rows = h/scl terrein = Array(cols) for(var x = 0; x\u0026lt;terrein.length; x++){ terrein[x] = new Array(rows) } slider = createSlider(0, 300, 100); slider.position(width/3, 30); slider.style(\u0026#39;width\u0026#39;, \u0026#39;300px\u0026#39;); } function draw() { fly -= 0.1 var yoff = fly for(var j=0; j\u0026lt;rows; j++){ var xoff = 0 for(var i=0; i\u0026lt;cols; i++){ terrein[i][j] = map(noise(xoff,yoff), 0, 1, -100, slider.value()) xoff += 0.1 } yoff += 0.1 } background(10); stroke(225) translate(0, 0) rotateX(PI/3) translate(-w/2, -h/2) for(var j=0; j\u0026lt;rows-1; j++){ beginShape(TRIANGLE_STRIP) for(var i=0; i\u0026lt;cols; i++){ vertex(i*scl, j*scl, terrein[i][j]) vertex(i*scl, (j+1)*scl, terrein[i][j+1]) colors = colorChange(terrein[i][j]) fill(colors) } endShape() } } function colorChange(num){ if(num\u0026gt;=120){ return(255) }else if(num\u0026gt;=100){ return(200) }else if(num\u0026gt;=40){ return([128,64 ,0]) } else if(num\u0026gt;=10){ return([0,150,0]) } else{ return([0,0,150]) } } Conclusiones y Trabajo futuro\u003e Conclusiones y Trabajo futuro # El uso de la libería P5js permite una aplicación más fácil y práctica de los conceptos de Periln Noise y Mach bands, pues estos ya están integrados dentro de la libreria, además de brindar una amplia documentación y guía de cómo usarlos. Así mismo, el uso de la librería P5js facilita considerablemente el trabajo y manejo de objetos en tres dimensiones: brinda una amplia variedad de herramientas para la manipulación de dichos elementos y, además, resulta ser precisa para el desarrollo de esta actividad, pues permite realizar distintas acciones para cada caso, como la manipulacion de los ejes x, y o z, entre otras. Para un trabajo futuro, se podría implementar una función que le permita al usuario girar la vista en modo 360° o que le permita jugar con los ejes de la grilla, de modo que pueda visualizar la generación automática del terreno desde diferentes perspectivas. Así mismo, se podria integrar como trabajo fúturo la incorporación de una función que le permita al usuario jugar con los valores preestablecidos para el método de Pearling Noise en tiempo de ejecución, es decir, aquellos valores que se generan \u0026ldquo;aleatoriamente\u0026rdquo; en z para cada par de coordenadas x, y. Referencias\u003e Referencias # [1] F. Ratliff, \u0026ldquo;Mach bands: quantitative studies on neural networks in the retina\u0026rdquo;, Holden-Day, 1965, ISBN 9780816270453.\u003e [1] F. Ratliff, \u0026ldquo;Mach bands: quantitative studies on neural networks in the retina\u0026rdquo;, Holden-Day, 1965, ISBN 9780816270453. # [2] G. von Békésy, \u0026ldquo;Mach Band Type Lateral Inhibition in Different Sense Organs\u0026rdquo;, PDF, 1967.\u003e [2] G. von Békésy, \u0026ldquo;Mach Band Type Lateral Inhibition in Different Sense Organs\u0026rdquo;, PDF, 1967. # [3] F.A.A. Kingdom, \u0026ldquo;Mach bands explained by response normalization\u0026rdquo;, Frontiers in Human Neuroscience, vol. 8, pp. 843, Nov. 2014, doi: 10.3389/fnhum.2014.00843, ISSN 1662-5161, PMC 4219435, PMID 25408643.\u003e [3] F.A.A. Kingdom, \u0026ldquo;Mach bands explained by response normalization\u0026rdquo;, Frontiers in Human Neuroscience, vol. 8, pp. 843, Nov. 2014, doi: 10.3389/fnhum.2014.00843, ISSN 1662-5161, PMC 4219435, PMID 25408643. # [4] P. Ambalathankandy et al, \u0026ldquo;Radiography Contrast Enhancement: Smoothed LHE Filter a Practical Solution for Digital X-Rays with Mach Band\u0026rdquo;, Digital Image Computing: Techniques and Applications, 2019.\u003e [4] P. Ambalathankandy et al, \u0026ldquo;Radiography Contrast Enhancement: Smoothed LHE Filter a Practical Solution for Digital X-Rays with Mach Band\u0026rdquo;, Digital Image Computing: Techniques and Applications, 2019. # [5] C.J. Nielsen, \u0026ldquo;Effect of Scenario and Experience on Interpretation of Mach Bands\u0026rdquo;, Journal of Endodontics, vol. 27, no. 11, pp. 687-691.\u003e [5] C.J. Nielsen, \u0026ldquo;Effect of Scenario and Experience on Interpretation of Mach Bands\u0026rdquo;, Journal of Endodontics, vol. 27, no. 11, pp. 687-691. # [6] R. Touti, \u0026ldquo;Perlin Noise Algorithm\u0026rdquo;, [Online]. Available: https://rtouti.github.io/graphics/perlin-noise-algorithm.\u003e [6] R. Touti, \u0026ldquo;Perlin Noise Algorithm\u0026rdquo;, [Online]. Available: https://rtouti.github.io/graphics/perlin-noise-algorithm. # [7] D. Shiffman, \u0026ldquo;What is a Pixel?\u0026rdquo; [Online]. Available: https://www.youtube.com/watch?v=IKB1hWWedMk\u0026t=1s. [Accessed: Mar. 31, 2022].\u003e [7] D. Shiffman, \u0026ldquo;What is a Pixel?\u0026rdquo; [Online]. Available: https://www.youtube.com/watch?v=IKB1hWWedMk\u0026t=1s. [Accessed: Mar. 31, 2022]. # ","date":"1 January 0001","permalink":"/showcase/first/terrain/","section":"Firsts","summary":"Mach Bands\u003e Mach Bands # Introducción\u003e Introducción # Para este ejercicio se definen los efectos visuales de \u0026ldquo;Mach bands\u0026rdquo; y Perlin noise, así como la relación que guardan al momento de generar un terreno automatizado y continuo.","title":""},{"content":"Coloring\u003e Coloring # Coloring.\n","date":"1 January 0001","permalink":"/showcase/second/coloring/","section":"Seconds","summary":"Coloring\u003e Coloring # Coloring.","title":""},{"content":"Image Processing\u003e Image Processing # ","date":"1 January 0001","permalink":"/showcase/second/imageprocessing/","section":"Seconds","summary":"Image Processing\u003e Image Processing # ","title":""},{"content":"Photomosaic\u003e Photomosaic # Photomosaic.\n","date":"1 January 0001","permalink":"/showcase/second/photomosaic/","section":"Seconds","summary":"Photomosaic\u003e Photomosaic # Photomosaic.","title":""},{"content":"Post Effects\u003e Post Effects # Post Effects.\n","date":"1 January 0001","permalink":"/showcase/second/posteffects/","section":"Seconds","summary":"Post Effects\u003e Post Effects # Post Effects.","title":""},{"content":"Procedural Texturing\u003e Procedural Texturing # Procedural Texturing.\n","date":"1 January 0001","permalink":"/showcase/second/proceduraltexturing/","section":"Seconds","summary":"Procedural Texturing\u003e Procedural Texturing # Procedural Texturing.","title":""},{"content":"Rendering\u003e Rendering # Rendering.\n","date":"1 January 0001","permalink":"/showcase/second/rendering/","section":"Seconds","summary":"Rendering\u003e Rendering # Rendering.","title":""},{"content":"Space Transformations\u003e Space Transformations # Space Transformations.\n","date":"1 January 0001","permalink":"/showcase/second/spacetransformations/","section":"Seconds","summary":"Space Transformations\u003e Space Transformations # Space Transformations.","title":""},{"content":"Spatial Coherence\u003e Spatial Coherence # Spatial Coherence.\n","date":"1 January 0001","permalink":"/showcase/second/spatialcoherence/","section":"Seconds","summary":"Spatial Coherence\u003e Spatial Coherence # Spatial Coherence.","title":""},{"content":"Texturing\u003e Texturing # Texturing.\n","date":"1 January 0001","permalink":"/showcase/second/texturing/","section":"Seconds","summary":"Texturing\u003e Texturing # Texturing.","title":""},{"content":"Juan José Peña Becerra\u003e Juan José Peña Becerra # Estudiante de Ingenieria de Sistemas y Computación de la UNAL, apasionado por el desarrollo de aplicaciones audiovisuales. Me gusta la interactividad en los medios, la experimentación y la creación de contenido para nuevas y mejores experiencias. Un poco adicto al tooling y la optimización de los sistemas en todas las fases. Apoyo y utilizo preferiblemente herramientas open source.\nMis intereses:\u003e Mis intereses: # Desarrollo de videojuegos. Animación y computación visual. Inteligencia arificial. Idiomas. ","date":"1 January 0001","permalink":"/showcase/team/jjpb/","section":"Teams","summary":"Juan José Peña Becerra\u003e Juan José Peña Becerra # Estudiante de Ingenieria de Sistemas y Computación de la UNAL, apasionado por el desarrollo de aplicaciones audiovisuales.","title":""},{"content":"Juan Pablo Bustamante Moreno\u003e Juan Pablo Bustamante Moreno # Desde el primer semestre de 2019 me encuentro estudiando Ingeniería de Sistemas y Computación en la Universidad Nacional de Colombia. La disciplina y la proactividad caracterizan mi trabajo académico. A nivel personal, soy apasionado por el consumo y la creación de contenido de ficción, tanto a nivel literario como cinematográfico: en particular, siento afinidad por las obras de terror, misterio, sátira y surrealismo.\nMis intereses:\u003e Mis intereses: # Desarrollo FrontEnd. Análisis de lenguajes de programación. Inteligencia artificial. Ciberseguridad. Arquitectura de Software. ","date":"1 January 0001","permalink":"/showcase/team/jpbm/","section":"Teams","summary":"Juan Pablo Bustamante Moreno\u003e Juan Pablo Bustamante Moreno # Desde el primer semestre de 2019 me encuentro estudiando Ingeniería de Sistemas y Computación en la Universidad Nacional de Colombia.","title":""},{"content":"Nicolás Romero Niño\u003e Nicolás Romero Niño # Hola, mi nombre es Nicolás Romero Niño. Soy estudiante de ingeniería de sistemas y computación de la Universidad Nacional de Colombia desde el 2019. Durante estos años en la universidad he sido una persona muy responsable y trabajadora, lo cual me ha permitido avanzar sin mayores inconvenientes en la carrera. A nivel personal, soy una persona social que disfruta mucho del tiempo compartido con la familia y amigos, además de disfrutar de actividades físicas como bailar o hacer deporte.\nMis intereses:\u003e Mis intereses: # Desarrollo web. Inteligencia artificial. Videojuegos. Bailar. Fútbol y otros deportes. Salir a comer. ","date":"1 January 0001","permalink":"/showcase/team/nrn/","section":"Teams","summary":"Nicolás Romero Niño\u003e Nicolás Romero Niño # Hola, mi nombre es Nicolás Romero Niño.","title":""},{"content":"","date":"1 January 0001","permalink":"/showcase/authors/","section":"Authors","summary":"","title":"Authors"},{"content":"","date":"1 January 0001","permalink":"/showcase/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":"1 January 0001","permalink":"/showcase/first/","section":"Firsts","summary":"","title":"Firsts"},{"content":"","date":"1 January 0001","permalink":"/showcase/second/","section":"Seconds","summary":"","title":"Seconds"},{"content":"","date":"1 January 0001","permalink":"/showcase/series/","section":"Series","summary":"","title":"Series"},{"content":"","date":"1 January 0001","permalink":"/showcase/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"","date":"1 January 0001","permalink":"/showcase/team/","section":"Teams","summary":"","title":"Teams"}]